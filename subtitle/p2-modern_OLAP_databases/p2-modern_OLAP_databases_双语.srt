1
00:00:14,370 --> 00:00:16,120
Sorry, for not be in pittsburgh, as I said,
对不起 我不在匹兹堡

2
00:00:16,450 --> 00:00:17,880
I had come out here to seattle. 
正如我所说 我来到了西雅图

3
00:00:18,040 --> 00:00:21,520
I'm actually in front of the courthouse right now, 
事实上 我现在就在法院前面

4
00:00:22,410 --> 00:00:25,200
because smooth shoes, 
因为光滑的鞋子 男孩三个手指

5
00:00:25,210 --> 00:00:27,400
boy three fingers and this guy has used the goose. 
这个家伙用了鹅

6
00:00:27,810 --> 00:00:31,560
They got popped last year for passing bad checks. 
他们去年因使用空头支票而被逮捕

7
00:00:31,570 --> 00:00:34,880
And so they asked me to come in a as a character witness for them. 
所以他们请我作为他们的品德证人

8
00:00:35,890 --> 00:00:38,080
It's not looking good in there for them. 
他们在那里看起来不太好

9
00:00:38,640 --> 00:00:39,790
That's their problem, not mine.
那是他们的问题 不是我的

10
00:00:39,800 --> 00:00:43,070
So I figured, let's come out here and do the class lecture.
所以我想 让我们来这里做课堂演讲

11
00:00:43,240 --> 00:00:44,310
Let's talk about databases. 
让我们来谈谈数据库

12
00:00:46,560 --> 00:00:48,430
For today's class, we're going to be talking about.
在今天的课上 我们将讨论

13
00:00:49,510 --> 00:00:55,820
Sort of is that an overview of what with these sort of modern query engines
在某种程度上 这是对这些现代查询引擎或现代lap

14
00:00:55,830 --> 00:00:57,380
or these modern olap database
数据库系统在高层次上的概述

15
00:00:57,390 --> 00:01:00,380
systems look like at a high level, 
这将为我们

16
00:01:00,880 --> 00:01:05,030
that'll then set us up for how we progress through the various topics
在本学期剩下的时间里如何完成各种

17
00:01:05,040 --> 00:01:06,750
throughout the rest of the semester. 
主题做好准备

18
00:01:08,160 --> 00:01:10,510
This is a high level outline of where we're going in this course. 
这是我们在本课程中要去的地方的一个高级大纲

19
00:01:10,520 --> 00:01:12,230
And I didn't want to bring this up last class, 
我不想在上节课讲这个问题

20
00:01:12,240 --> 00:01:16,720
because I want to spend time talking about the sort of historical overview
因为我想花时间谈谈标准

21
00:01:16,730 --> 00:01:18,840
of the standard databases. 
数据库的历史概况

22
00:01:18,850 --> 00:01:22,740
And so to understand why we're at the point where today we're focusing
因此 为了理解为什么我们今天关注的

23
00:01:22,750 --> 00:01:24,220
on the relational database systems, 
是关系数据库系统

24
00:01:24,710 --> 00:01:29,040
but this is a high level architecture of the roadmap of how to build
但这是如何构建现代o lab

25
00:01:29,050 --> 00:01:32,680
a modern olap database system. 
数据库系统的路线图的高级架构

26
00:01:33,850 --> 00:01:35,880
We're going to first talk about storage. 
我们将首先讨论存储

27
00:01:36,220 --> 00:01:39,650
Essentially, how do we actually going to represent data on disk?
从本质上讲 我们如何在磁盘上表示数据

28
00:01:39,660 --> 00:01:40,650
Compress it? 
压缩它

29
00:01:40,660 --> 00:01:46,760
Maybe build some indexes that we can use for accelerating query execution. 
也许可以建立一些索引 我们可以用来加速查询执行

30
00:01:47,610 --> 00:01:52,280
And then we'll use all that storage ideas about how we want to store the datas. 
然后 我们将使用所有关于如何存储数据的存储思想

31
00:01:52,520 --> 00:01:57,030
Then spend most of time discussing how we actually want to execute queries
然后花大部分时间讨论我们实际上希望如何对这些数据执行

32
00:01:57,040 --> 00:01:57,750
on this data. 
查询

33
00:01:58,250 --> 00:02:00,800
We'll talk about processing models how to schedule queries. 
我们将讨论处理模型和如何调度查询

34
00:02:02,210 --> 00:02:03,480
The task within them. 
他们的任务

35
00:02:03,760 --> 00:02:06,380
We'll spend a lot of time talking about how to do vector rise execution. 
我们将花很多时间讨论如何执行向量上升

36
00:02:07,070 --> 00:02:12,710
Taking advantage of cindy query compilation will be another major theme or method. 
利用Cindy查询编译将是另一个主要主题或方法

37
00:02:12,720 --> 00:02:13,950
We can use to speed things up. 
我们可以用来加快速度

38
00:02:14,220 --> 00:02:16,970
We'll spend a lot of time talking about different joint algorithms. 
我们将花很多时间讨论不同的联合算法

39
00:02:17,740 --> 00:02:20,250
And something new that we'll discuss this year that I haven't discussed
今年我们将讨论一些新的东西

40
00:02:20,260 --> 00:02:22,160
in previous years is materialized views. 
我在前几年没有讨论过 那就是物化视图

41
00:02:22,170 --> 00:02:25,240
And I have to make fully materialized views is actually something I know, 
我必须制作完全物化的视图

42
00:02:25,250 --> 00:02:27,480
the least about in database systems. 
这实际上是我所知道的 在数据库系统中最少的

43
00:02:27,490 --> 00:02:29,850
So we'll spend some time on this. 
所以我们会花一些时间在这上面

44
00:02:29,860 --> 00:02:31,610
And then we'll talk about query optimization. 
然后我们将讨论查询优化

45
00:02:31,620 --> 00:02:32,810
How do you take a query plan? 
如何获取查询计划

46
00:02:32,820 --> 00:02:37,250
Convert it into to single query, generate the query plan,
将其转换为单个查询 生成查询计划

47
00:02:38,490 --> 00:02:40,520
how to design sort of modern network interfaces. 
如何设计现代网络接口

48
00:02:40,760 --> 00:02:42,760
And then the rest of the semester after that, 
在那之后剩下的学期里

49
00:02:43,140 --> 00:02:49,090
will be doing an analysis and deep introspection into real systems
我们将对真正的系统进行分析和深刻的反思

50
00:02:49,100 --> 00:02:50,410
like the snowflake system. 
比如雪花系统

51
00:02:50,420 --> 00:02:52,060
You guys read today's paper, 
你们读了今天的报纸

52
00:02:52,870 --> 00:02:58,100
but also a big query spark and a bunch of other modern systems. 
但也有一个大的查询火花和一堆其他的现代系统

53
00:02:58,940 --> 00:03:03,890
This this sort of layer diagram showing here is very similar to what we
这里显示的这种层图与我们在介绍课程中讨论的非常相似

54
00:03:03,900 --> 00:03:05,210
discussed in the intro class, 
它

55
00:03:05,220 --> 00:03:07,730
where it's essentially the system for going from the bottom up. 
本质上是从下往上的系统

56
00:03:07,740 --> 00:03:10,780
You're gonna build these different abstraction interfaces. 
您将构建这些不同的抽象接口

57
00:03:11,010 --> 00:03:13,380
And they'll have, 
他们会有 你会公开一个api

58
00:03:14,250 --> 00:03:18,680
you'll expose an api that we then integrate with into the layer above it. 
然后我们把它集成到它上面的层

59
00:03:18,690 --> 00:03:22,000
And then to the nbn we end up with the client interface will be exposed
然后到NBN

60
00:03:22,010 --> 00:03:22,960
to the application. 
我们最终将客户端接口暴露给应用程序

61
00:03:24,340 --> 00:03:27,340
This going for the next three lectures for lectures or so, 
在接下来的三节课中

62
00:03:27,350 --> 00:03:30,020
we'll focus on the storage lab. 
我们将重点关注存储实验

63
00:03:30,660 --> 00:03:34,970
That'll then define how we actually want to when api will have exposed
然后 当我们想要运行查询时 这将定义我们实际想要的api

64
00:03:34,980 --> 00:03:36,250
to us when we want to run queries. 
何时向我们公开

65
00:03:38,790 --> 00:03:40,020
Today's execution, as I said,
今天的执行

66
00:03:40,030 --> 00:03:46,440
this is a high level overview of what of what modern overlap systems look like. 
正如我所说 这是对现代重叠系统的一个高层次的概述

67
00:03:47,630 --> 00:03:48,580
So we're not going to go too deep. 
所以我们不会太深入

68
00:03:48,590 --> 00:03:49,980
And like, what are the algorithms you do?
比如 你做的算法是什么

69
00:03:49,990 --> 00:03:51,900
Fixing queries that also come later? 
修复稍后也会出现的查询

70
00:03:53,510 --> 00:03:56,100
This class isn't necessarily about distributed databases, 
这门课不一定是关于分布式数据库的

71
00:03:56,110 --> 00:04:01,570
because I want to spend time discussing what are the modern techniques
因为我想花时间讨论在单个节点上

72
00:04:01,580 --> 00:04:04,770
of what you do on a single node to process data? 
处理数据的现代技术是什么

73
00:04:04,950 --> 00:04:09,790
And then we'll layer on top of that the distributed architecture, 
然后我们将在分布式架构的基础上进行分层

74
00:04:09,800 --> 00:04:12,980
but it's good to understand a little bit of what these what it
但最好了解一下现代插座系统

75
00:04:12,990 --> 00:04:15,510
should be said would look like for a modern outlet system. 
应该是什么样子的

76
00:04:16,060 --> 00:04:19,050
So understand the big fish were gone, 
所以明白大鱼不见了

77
00:04:19,060 --> 00:04:22,490
but it really has been the said before and many other classes. 
但它确实是以前说过的 还有很多其他的课

78
00:04:23,000 --> 00:04:26,550
We need to get the single node system to be high performance
首先

79
00:04:26,560 --> 00:04:28,030
and super optimized first. 
我们需要让单节点系统具有高性能和超级优化

80
00:04:28,330 --> 00:04:29,760
Before we make it distributed, 
在我们使它成为分布式之前

81
00:04:30,090 --> 00:04:32,040
just like making a database distributed, 
就像使数据库成为分布式一样

82
00:04:32,050 --> 00:04:34,920
isn't this magic wand or magic solution? 
这不是魔棒或神奇的解决方案吗

83
00:04:35,080 --> 00:04:36,910
That solves all the scalable problems? 
这解决了所有可扩展的问题

84
00:04:36,920 --> 00:04:39,990
Like if you have a crappy single node system making distributed as means
就像如果你有一个蹩脚的单节点系统

85
00:04:40,000 --> 00:04:41,550
you have a multi node crappy system. 
那么分布式就意味着你有一个蹩脚的多节点系统

86
00:04:42,550 --> 00:04:43,660
We need to kneel that down first. 
我们得先把它跪下来

87
00:04:43,670 --> 00:04:45,610
And I want to spend a little time talking at the end
我想在最后花

88
00:04:45,620 --> 00:04:51,450
about this bigger trend about the the commodization of the components
一点时间谈谈这个更大的趋势

89
00:04:51,940 --> 00:04:53,810
of these olap systems. 
关于这些olap系统的组件的商业化

90
00:04:53,820 --> 00:04:57,930
So we're mostly talking about systems like snowflake and spark, 
所以我们主要谈论的是像雪花和spark这样的系统

91
00:04:58,870 --> 00:05:03,180
where they're these monolithic systems that are all where the vendor
它们是供应商已经实现了所有

92
00:05:03,190 --> 00:05:05,460
has implemented all the bits and pieces of them. 
零碎功能的整体系统

93
00:05:05,700 --> 00:05:09,110
But there is a trend now where people are trying to build sort
但现在有一种趋势

94
00:05:09,120 --> 00:05:13,070
of stand alone pieces of actually the layer diagram I showed before. 
人们试图构建我之前展示的层图的独立部分

95
00:05:13,080 --> 00:05:14,390
And in theory, 
从理论上讲

96
00:05:14,400 --> 00:05:16,790
one could then tie these put these things together. 
人们可以把这些东西联系在一起

97
00:05:17,220 --> 00:05:19,450
At later point, I hope that new whole lab system,
稍后

98
00:05:19,460 --> 00:05:21,430
but we're not entirely there yet, 
我希望新的整个实验室系统

99
00:05:21,440 --> 00:05:22,790
but i'll talk about some early, 
但我们还没有完全做到这一点

100
00:05:23,600 --> 00:05:26,150
some early work in this space. 
但我将谈论这个领域的一些早期工作

101
00:05:31,020 --> 00:05:35,750
The first thing I understand what does distributed query what the execution
首先 我要了解什么是分布式查询

102
00:05:35,760 --> 00:05:37,950
distributed query look like in a
分布式查询的执行是什么样子的

103
00:05:39,270 --> 00:05:41,860
what does an oled query look like in a distributed database system? 
分布式数据库系统中的OLED查询是什么样子的

104
00:05:42,310 --> 00:05:43,830
And I will say at a high level, 
我要说的是 在高层次上

105
00:05:43,840 --> 00:05:46,350
it's essentially the same as you would have in a single system. 
它本质上与你在单一系统中所拥有的是一样的

106
00:05:47,000 --> 00:05:47,160
Right? 
对的

107
00:05:47,170 --> 00:05:51,100
Sql query shows up the you run through the parser, 
当您通过解析器 绑定器

108
00:05:51,110 --> 00:05:55,060
the binder, the optimizer, and so forth.
优化器等运行时 sql查询会显示出来

109
00:05:55,360 --> 00:06:00,110
It'll generate a directed acyclic graph of physical operators, 
它将生成物理操作符的有向无环图

110
00:06:00,500 --> 00:06:04,810
where you have these physical operators scan and join implementation. 
其中您可以扫描和连接这些物理操作符的实现

111
00:06:05,130 --> 00:06:09,840
And they're taking some input coming from its children operators, 
它们从它的子运算符中获取一些输入

112
00:06:10,210 --> 00:06:11,820
doing some amount of computation on them, 
对它们进行一些计算

113
00:06:11,830 --> 00:06:19,600
and then passing that along to the to its parent operator. 
然后将其传递给它的父运算符

114
00:06:20,780 --> 00:06:26,940
And so the fact that these operators may be excluded across multiple nodes
事实上 这些操作符可以在多个节点上被排除

115
00:06:27,310 --> 00:06:30,390
versus all running on the single box, 
而不是在单个盒子

116
00:06:30,400 --> 00:06:31,110
single node. 
单个节点上运行

117
00:06:31,810 --> 00:06:32,890
At a higher there, again,
在更高的地方

118
00:06:32,900 --> 00:06:34,970
there really isn't any difference. 
再一次 真的没有任何区别

119
00:06:38,390 --> 00:06:40,300
There's the orchestration of the task and so forth, 
还有任务的编排等等

120
00:06:40,310 --> 00:06:41,820
but that's sort of outside the scope of what
但这超出了我们今天在这里

121
00:06:41,830 --> 00:06:42,940
we're talking about right here today. 
讨论的范围

122
00:06:43,150 --> 00:06:44,580
Like the day, there's a query plan.
就像白天一样 有一个查询计划

123
00:06:44,590 --> 00:06:46,140
We break it up into task, 
我们把它分解成任务

124
00:06:46,150 --> 00:06:47,420
redistribute across nodes. 
在节点之间重新分配

125
00:06:47,430 --> 00:06:49,580
It's no different than taking a query plan, 
这与采用查询计划

126
00:06:49,590 --> 00:06:55,920
breaking up the task and distribute across cores on a a multi cpu box, 
分解任务并在多CPU机器上的核心之间分布没有什么不同

127
00:06:57,240 --> 00:07:00,270
all the operators that we had before, one of the single node system,
我们之前所有的操作符 一个单一节点系统

128
00:07:00,280 --> 00:07:01,990
table scans, join, and aggregation sorting.
table扫描 连接和聚合排序

129
00:07:02,220 --> 00:07:06,970
All these exist in an initiative system. 
所有这些都存在于一个系统中

130
00:07:08,240 --> 00:07:09,870
At a high level, it's gonna look at this, right?
在高层次上 它会看这个 对吗

131
00:07:10,040 --> 00:07:13,270
Say there's gonna be some notion of a worker node. 
比如说会有一个工人节点的概念

132
00:07:13,800 --> 00:07:17,010
And worker is going to have acpu it's going to have local memory. 
工人将拥有ACPU 它将拥有本地内存

133
00:07:17,020 --> 00:07:18,210
It's going to have local disk. 
它将有本地磁盘

134
00:07:18,680 --> 00:07:20,340
And and say, 
比如说

135
00:07:20,350 --> 00:07:22,620
if we go from sort of left to right in the diagram here, 
如果我们在图中从左到右

136
00:07:22,630 --> 00:07:23,940
we're trying to execute a query. 
我们试图执行一个查询

137
00:07:24,380 --> 00:07:29,170
The each of these nodes are responsible for taking some input in doing
这些节点中的每一个都负责在为给定

138
00:07:29,180 --> 00:07:31,810
some computation for the operator that they've been assigned
任务分配的操作符进行一些

139
00:07:31,820 --> 00:07:32,690
for a given task, 
计算时获取一些输入

140
00:07:32,700 --> 00:07:37,560
and then producing output as results that then either goes
然后产生输出作为结果

141
00:07:37,570 --> 00:07:42,270
back to the client or goes off to the next worker nodes. 
然后返回给客户端或发送给下一个工作节点

142
00:07:42,900 --> 00:07:44,000
Say we start in the very beginning, 
假设我们从最开始开始

143
00:07:44,010 --> 00:07:47,500
say this is the bottom of query plan and we want to read what we call
假设这是查询计划的底部

144
00:07:47,510 --> 00:07:48,580
persistent data. 
我们想要读取我们所说的持久数据

145
00:07:49,210 --> 00:07:52,880
The persistent data is going to be the the underlying two poles
持久数据将是数据库中定义的

146
00:07:52,970 --> 00:07:55,440
of the tables that are defined in the database. 
表的底层两极

147
00:07:55,530 --> 00:07:59,880
These going to be some files that are going to have data that we know
这将是一些文件 这些文件将包含数据

148
00:07:59,890 --> 00:08:03,620
how to interpret their types based on some scheme information, 
我们知道如何根据一些方案信息来解释它们的类型

149
00:08:03,630 --> 00:08:04,300
a catalog. 
一个目录

150
00:08:04,770 --> 00:08:08,480
We know how to access it and scan and do something, right?
我们知道如何访问它 扫描和做一些事情 对不对

151
00:08:09,010 --> 00:08:12,680
I'm not showing this diagram whether the persistent data is
我不会在这个图中显示持久数据是否在工作节点的本地

152
00:08:12,690 --> 00:08:15,540
local to the worker node, 
或者它是某个共享磁盘体系

153
00:08:15,550 --> 00:08:17,860
or it's some shared disk architecture or object store, 
结构或对象存储

154
00:08:17,870 --> 00:08:18,900
which will cover it a second. 
这将涉及到它

155
00:08:19,300 --> 00:08:20,810
It for our purposes here doesn't matter. 
这对我们在这里的目的并不重要

156
00:08:21,900 --> 00:08:23,770
Again, we take in the persistent data,
同样 我们接受持久数据

157
00:08:23,780 --> 00:08:25,010
we're doing some kind of scan, 
我们正在进行某种扫描

158
00:08:25,980 --> 00:08:28,130
and we're going to boost some amount of intermediate data. 
并且我们将增加一些中间数据

159
00:08:29,090 --> 00:08:31,640
And then when you now should need to ship this intermediate data
然后

160
00:08:31,650 --> 00:08:35,830
to the sort of the next stage of the query plan, 
当您现在需要将此中间数据发送到查询计划的下一阶段时

161
00:08:35,840 --> 00:08:38,550
which either could be on the same node or a different node. 
可以在同一节点或不同节点上

162
00:08:39,600 --> 00:08:45,920
One of the ways we can do this is through what is called a shuffle operation. 
我们可以做到这一点的方法之一是通过所谓的洗牌操作

163
00:08:46,240 --> 00:08:49,610
Where after we do some operator, 
在我们做了一些操作之后

164
00:08:50,080 --> 00:08:54,430
we would pass along this intermediate data to another set of nodes that
我们将把这个中间数据传递给另一组节点

165
00:08:54,440 --> 00:08:57,990
are going to distribute the the results based
这些节点将根据一些哈希键

166
00:08:58,000 --> 00:09:00,870
on some hash key or some range partitioning. 
或一些范围分区来分发结果

167
00:09:02,210 --> 00:09:04,600
And then there will be some other workers that can then pull that data
然后会有一些其他的工作人员可以

168
00:09:05,090 --> 00:09:06,480
from the shuffle notes. 
从洗牌笔记中提取数据

169
00:09:07,120 --> 00:09:10,160
Now, i'm putting the shuffle node phase as optional here,
现在 我把洗牌节点阶段作为可选的

170
00:09:11,600 --> 00:09:16,420
because I don't think snowflake does this with explicit notes, but big,
因为我不认为snowflake 使用这种明确的节点做到这一点

171
00:09:16,430 --> 00:09:17,580
gray and drummer do this. 
但 gray 和 drummer 能做到这一点

172
00:09:18,840 --> 00:09:23,360
The ideas like these in memory nodes that are have the shuffle notes, 
这样的想法主要是在shuffle节点

173
00:09:23,370 --> 00:09:24,280
have a lot of memory. 
拥有大量的内存

174
00:09:24,290 --> 00:09:26,480
And so they try to keep everything at cache without in a disk. 
所以他们试着把所有东西都放在缓存里 而不是在磁盘

175
00:09:26,490 --> 00:09:28,740
And it's a way to sort of redistribute things. 
这是一种重新分配事物的方式

176
00:09:28,970 --> 00:09:31,120
I'm also showing here there's a sort of 1 to 1 correspondence
我在这里还展示了在number shuffle

177
00:09:31,130 --> 00:09:32,720
between number shuffle nodes and worker nodes
节点和worker节点之间有一种1

178
00:09:32,730 --> 00:09:36,040
doesn't necessarily need to be the case you could fan out or fan
对1的对应关系

179
00:09:36,050 --> 00:09:37,550
in at this different stage. 
不一定需要在这个不同的阶段你可以扇出或扇入

180
00:09:37,560 --> 00:09:41,420
But it's just way to get the intermediate data from the previous stage
但这只是一种将中间数据从上一阶段传递到下一

181
00:09:41,750 --> 00:09:42,420
to the next stage. 
阶段的方法

182
00:09:42,430 --> 00:09:46,040
And if necessary redistributed on some new partition game, 
并且如果有必要在一些新的分区游戏上重新分配

183
00:09:46,800 --> 00:09:49,060
then this network or node does additional computations. 
则该网络或节点进行额外的计算

184
00:09:49,820 --> 00:09:50,970
It has more intermediate data. 
它有更多的中间数据

185
00:09:50,980 --> 00:09:53,130
They say the trouble phase or not doesn't matter. 
他们说麻烦阶段与否并不重要

186
00:09:53,590 --> 00:09:55,780
But they say, now it descends into this other worker node,
但他们说 现在它下降到另一个工作节点

187
00:09:55,790 --> 00:09:59,460
who then does some file final aggregation of coalescing of the data
然后由该节点执行数据合并的一些文件最终聚合

188
00:09:59,470 --> 00:10:01,740
to produce the result of the application? 
以产生应用程序的结果

189
00:10:02,650 --> 00:10:02,920
Again, 
又

190
00:10:06,260 --> 00:10:08,080
there's nothing true, 
我们在属性系统上执行查询的方式与在单节点系统上执行查询的方式相比

191
00:10:08,330 --> 00:10:08,920
drastically, 
没有什么真正的

192
00:10:08,930 --> 00:10:12,080
dramatically different than how we'd execute a query on attribute system
彻底的

193
00:10:12,090 --> 00:10:13,400
versus a single node system. 
显著的不同

194
00:10:13,550 --> 00:10:17,640
We just may have to do spend more time doing data movement which
我们可能不得不花更多的时间进行数据移动

195
00:10:17,650 --> 00:10:21,390
nowadays is actually not as bad as it used to be that
而现在的数据移动实际上并不像过去那样糟糕

196
00:10:22,900 --> 00:10:26,550
the hierarchy was that the the network going with the network was
层次结构是与网络一起运行的网络是最慢的

197
00:10:26,560 --> 00:10:29,470
the slowest and reading from this is the slowest and the memory, 
从这里读取是最慢的 内存也是

198
00:10:29,480 --> 00:10:31,590
but never gotten really fast now. 
但现在从来没有真正快过

199
00:10:31,960 --> 00:10:34,750
Especially in the case of big grade, they have accelerated.
特别是在大等级的情况下 他们加快了速度

200
00:10:36,680 --> 00:10:39,660
You have hard acceleration for these fish shuffle operation. 
你有这些鱼洗牌操作的硬加速

201
00:10:40,030 --> 00:10:41,310
In some cases, 
在某些情况下

202
00:10:41,320 --> 00:10:44,040
it's actually faster to send things for the network than read from disk. 
向网络发送内容实际上比从磁盘读取内容更快

203
00:10:44,570 --> 00:10:48,130
This shuffle phase was a big deal. 
这个洗牌阶段是一件大事

204
00:10:48,490 --> 00:10:52,300
But the main thing I want to expose here is this idea of the notion
但我想在这里展示的主要内容是持久数据的概念

205
00:10:52,310 --> 00:10:53,110
of persistent data, 
中间

206
00:10:53,120 --> 00:10:53,650
intermediate data, 
数据

207
00:10:53,860 --> 00:10:55,820
because that's going to depend on or that. 
因为这将取决于或

208
00:10:56,100 --> 00:10:58,430
What we actually do with the different types of data is going to depend
我们实际上如何处理不同

209
00:10:58,440 --> 00:11:03,070
on what the architecture of whether to share nothing to share disk system. 
类型的数据将取决于是否共享磁盘系统的体系结构

210
00:11:05,280 --> 00:11:08,670
This taxonomy between persistent data and intermediate data comes
持久数据和中间数据之间的分类来自于你们

211
00:11:08,680 --> 00:11:10,470
from the significant paper that you guys read. 
读过的一篇重要的论文

212
00:11:11,720 --> 00:11:16,190
What they would call it persistent data is the data that's called a source
他们称之为持久数据的数据被称为

213
00:11:16,200 --> 00:11:17,830
of record for the database. 
数据库的记录源

214
00:11:17,960 --> 00:11:21,860
If I inserted to pull part of it, 
如果我插入拉出它的一部分

215
00:11:22,390 --> 00:11:25,420
bulk, loading some data or copying or insert operation.
批量 加载一些数据或复制或插入操作

216
00:11:25,740 --> 00:11:33,520
The I expect that two would reside in these persistent files
我希望这两个将驻留在这些持久数据中的持久文件中

217
00:11:33,530 --> 00:11:34,640
in the persistent data. 


218
00:11:35,990 --> 00:11:37,220
In a modern system, 
在现代系统中

219
00:11:37,230 --> 00:11:40,510
we're gonna assume that the persistent data of the files of persistent data
我们将假设持久数据文件的持久数据是不可

220
00:11:40,680 --> 00:11:41,630
is immutable. 
变的

221
00:11:42,420 --> 00:11:47,780
And that's a dramatic change from how people designed assumed data systems
这与人们设计假设数据系统的方式有很大的不同

222
00:11:47,790 --> 00:11:49,520
were built from the years past, 
数据系统是在过去几年中构建的

223
00:11:49,530 --> 00:11:51,940
because you assume you're running with a local disk that you
因为您假设您正在使用本地磁盘运行

224
00:11:51,950 --> 00:11:53,700
can read and write as it as needed. 
可以根据需要进行读写

225
00:11:53,940 --> 00:11:56,010
But in a modern cloud setting, 
但在现代云环境中

226
00:11:57,180 --> 00:11:58,350
we are running on an object store, 
我们在对象存储上运行

227
00:11:58,360 --> 00:12:01,230
or even if you're running on the early predecessor, 
或者即使您在早期的前身上运行

228
00:12:01,240 --> 00:12:04,980
like hgfs those file systems were, 
如HGFS文件系统

229
00:12:05,440 --> 00:12:06,620
some of them were append only, 
其中一些只是附加的

230
00:12:06,630 --> 00:12:09,010
but you couldn't do in place updates. 
但您不能进行就地更新

231
00:12:09,800 --> 00:12:13,400
So in the case of a modern lab system, 
所以在现代实验室系统的情况下

232
00:12:13,410 --> 00:12:14,960
they're gonna assume that these files are immutable. 
他们会假设这些文件是不可变的

233
00:12:14,970 --> 00:12:17,720
And he put you to support updating them and you have to maintain somebody
他让你支持更新它们

234
00:12:17,730 --> 00:12:21,730
to keep track of how whether something is, 
你必须维护某个人来

235
00:12:21,740 --> 00:12:27,270
whether a version of a tube has been overwritten by a newer version of. 
跟踪某些东西是否被更新的版本覆盖

236
00:12:28,980 --> 00:12:29,900
But we'll come to that. 
但我们会谈到这一点

237
00:12:29,910 --> 00:12:31,380
We'll talk about more of that next class. 
下节课我们将讨论更多内容

238
00:12:31,950 --> 00:12:35,860
And then the other interesting part effect is going to be these, 
然后另一个有趣的部分效应是这些

239
00:12:37,130 --> 00:12:38,650
what they would be called intermediate data. 
它们被称为中间数据

240
00:12:38,660 --> 00:12:43,140
And this is the output of the operators that we execute in a query plan
这是我们在查询计划中执行的操作符的输出

241
00:12:43,370 --> 00:12:47,760
that we then need to send along to the next operator in the query plan. 
然后我们需要将其发送到查询计划中的下一个操作符

242
00:12:48,900 --> 00:12:50,890
This actually can be a lot of data. 
这实际上可以是大量的数据

243
00:12:50,900 --> 00:12:52,090
And in the sense of paper, 
在论文的意义上

244
00:12:52,100 --> 00:12:56,290
they talk about how the amount of intermediate data that a query
他们讨论了查询将生成的中间

245
00:12:56,300 --> 00:13:00,000
would generate has no little to no correlation to the amount
数据量如何与持久数据读取的数据量有很小

246
00:13:00,010 --> 00:13:01,520
of data persistent data reading. 
或没有相关性

247
00:13:01,800 --> 00:13:03,710
Being like the table can be really, really small,
就像表可以非常非常小

248
00:13:03,720 --> 00:13:05,710
but then they still end up producing a lot of intermediate data
但它们最终仍然会产生大量的中间数据

249
00:13:05,720 --> 00:13:07,870
because of some computation that they're doing, 
因为它们正在进行一些计算

250
00:13:08,330 --> 00:13:10,720
or could also be, it's not correlated to the execution time.
或者也可能是 这与执行时间无关

251
00:13:10,730 --> 00:13:12,560
So if a query takes a long time to run, 
因此 如果查询需要很长时间才能运行

252
00:13:12,570 --> 00:13:15,760
that doesn't necessarily mean they have to generate a lot of intermediate data. 
这并不一定意味着它们必须生成大量中间数据

253
00:13:17,610 --> 00:13:25,590
This animated data is interesting because it's it's you need to move
这个动画数据很有趣 因为你需要尽可能快地移动它

254
00:13:25,600 --> 00:13:26,630
along as fast as possible, 
但你不必

255
00:13:26,640 --> 00:13:28,910
but you don't need necessarily need to worry about durability, 
担心它的持久性

256
00:13:29,520 --> 00:13:31,510
fault tolerance in persisting it. 
和容错性

257
00:13:31,520 --> 00:13:33,790
Because as soon as you finish the query, 
因为一旦你完成了查询

258
00:13:34,750 --> 00:13:37,700
the next stage going from up to the next immediately throw it away. 
下一个阶段就会立即把它扔掉

259
00:13:38,220 --> 00:13:38,350
Right? 
对的

260
00:13:38,360 --> 00:13:43,180
So that the paper talks about how this sort of trade off between deciding
所以这篇论文讨论了如何在决定磁盘处理数据

261
00:13:43,390 --> 00:13:45,020
what the spelling disk processing data, 
和中间数据之间进行

262
00:13:45,030 --> 00:13:45,980
intermediate data. 
权衡

263
00:13:46,180 --> 00:13:49,800
And it was often the case you wanna keep intermediate data in memory as you
通常情况下 当你移动中间数据时

264
00:13:49,810 --> 00:13:50,400
move it along. 
你想把它保存在内存中

265
00:13:55,280 --> 00:13:59,320
So we need to talk about what the sort of system architecture would look
因此 我们需要讨论在现代超圈系统中

266
00:13:59,330 --> 00:14:01,560
like in a modern olap system. 
系统架构会是什么样子

267
00:14:02,250 --> 00:14:06,450
And there's a couple ways to think about this is, but the end of the day,
有几种方法可以思考这个问题 但在一天结束时

268
00:14:11,000 --> 00:14:13,440
there's how it's going to move data. 
这是它移动数据的方式

269
00:14:14,110 --> 00:14:19,100
There are mood persistent data to or to start beginning the execution
有情绪持久数据或开始在持久数据上开始执行

270
00:14:19,110 --> 00:14:20,220
on the persistent data, 
如开始

271
00:14:20,230 --> 00:14:21,340
like start scanning it. 
扫描它

272
00:14:21,350 --> 00:14:23,860
And it will determine also where it's located. 
它也将决定它的位置

273
00:14:25,740 --> 00:14:26,930
Again, in the interclass,
再一次

274
00:14:26,940 --> 00:14:30,930
we talk about this sort of shared nothing and shared disk or just push
在类间 我们讨论这种共享的东西和共享磁盘

275
00:14:30,940 --> 00:14:31,570
the query the data, 
或者只是推送查询数据

276
00:14:31,900 --> 00:14:33,050
pull data, the query.
拉数据 查询

277
00:14:33,060 --> 00:14:36,280
We talk of these things as being very these music exclusive, 
我们把这些东西说成是这些音乐独有的

278
00:14:36,290 --> 00:14:37,280
but it's actually not the case, 
但事实并非如此

279
00:14:37,290 --> 00:14:43,080
especially in a modern platforms where systems don't clean divide
尤其是在现代平台上

280
00:14:43,090 --> 00:14:44,120
along these lines. 
系统不会沿着这些线进行划分

281
00:14:44,130 --> 00:14:45,840
They actually do a little bit of everything. 
他们实际上什么都做一点

282
00:14:47,370 --> 00:14:51,240
We can understand the tradeoffs and the applications of these approaches. 
我们可以理解这些方法的权衡和应用

283
00:14:52,580 --> 00:14:53,730
The first thing is to say, again,
第一件事是说

284
00:14:53,740 --> 00:14:56,550
if we want to start reading processing data, 
如果我们想要开始读取和处理数据

285
00:14:56,890 --> 00:14:59,160
how should we begin that process? 
我们应该如何开始这个过程

286
00:15:00,090 --> 00:15:01,960
And then as we move from one stage, 
然后我们从一个阶段开始

287
00:15:01,970 --> 00:15:03,480
the next we have two operators. 
下一个阶段我们有两个操作符

288
00:15:03,780 --> 00:15:05,050
How do we transfer? 
我们怎么换车

289
00:15:05,060 --> 00:15:08,490
Or what do we transfer for the intermediate results? 
或者中间结果我们传递什么

290
00:15:09,530 --> 00:15:09,880
First, 
首先

291
00:15:09,890 --> 00:15:12,160
just talking about how do we begin the execution of things
讨论我们如何开始

292
00:15:12,170 --> 00:15:14,040
or or the movement of data. 
执行或移动数据

293
00:15:14,050 --> 00:15:14,920
The two choices, again,
同样 有两种选择

294
00:15:14,930 --> 00:15:17,780
are pushing the query to the data or pulling the data, the query.
一种是将查询推到数据 另一种是将数据拉到查询

295
00:15:19,610 --> 00:15:21,080
Again, these ideas aren't new.
再说一次 这些想法并不新鲜

296
00:15:21,090 --> 00:15:25,320
They go back to a lot of the work that was done in the late 80s, early 90s,
他们可以追溯到80年代末 90年代初所做的许多工作

297
00:15:25,330 --> 00:15:28,360
and some of the first like parallel distributed data system architectures. 
以及一些最早的工作 比如并行分布式数据系统架构

298
00:15:28,600 --> 00:15:32,430
And the tradeoffs of how to understand these things, I think,
以及如何理解这些事情的权衡 我认为

299
00:15:32,440 --> 00:15:33,710
of change in modern times, 
现代的变化

300
00:15:33,720 --> 00:15:36,250
just because of honestly, 
只是因为老实说

301
00:15:36,700 --> 00:15:37,890
how much harbor has gotten better, 
港口变得更好了

302
00:15:37,900 --> 00:15:42,180
especially the the proliferation of cloud services, 
特别是云服务的激增 就像对象存储

303
00:15:42,430 --> 00:15:46,950
like object stores where maybe things in the past like shared nothing
过去的东西可能什么都不共享

304
00:15:46,960 --> 00:15:50,310
and was the dominant architecture that we would assume that was
并且是我们认为是最好的方法的主导架构

305
00:15:50,320 --> 00:15:50,950
the best approach. 


306
00:15:50,960 --> 00:15:52,630
And you see, it was always logical,
你看 它总是合乎逻辑的

307
00:15:52,640 --> 00:15:55,830
always out to push the data, the query,
总是推出数据 查询

308
00:15:55,840 --> 00:15:57,590
to push the computation where the data is located. 
推出数据所在的计算

309
00:15:57,600 --> 00:16:00,590
But now that's not always the case in the cloud. 
但现在在云中并不总是这样

310
00:16:01,920 --> 00:16:02,710
The first thing is, again,
同样

311
00:16:03,000 --> 00:16:07,680
how we want to initiate the process of executing portions of a query
第一件事是我们希望如何启动在数据库或数据上执行

312
00:16:08,130 --> 00:16:09,720
on a database or on data. 
部分查询的过程

313
00:16:10,560 --> 00:16:13,470
The first approach is you pushing the query to the data. 
第一种方法是将查询推送到数据

314
00:16:13,480 --> 00:16:19,490
And this means that we want to move as much computation as we can to
这意味着我们希望将尽可能多的计算移动

315
00:16:19,500 --> 00:16:23,230
the location of where data is located. 
到数据所在的位置

316
00:16:23,880 --> 00:16:31,710
The thought process here is that the cost of executing some
这里的思维过程是 执行数据所在的查询的某些

317
00:16:31,720 --> 00:16:34,830
portion of a query where the data is located was always gonna be
部分的成本总是比将

318
00:16:34,840 --> 00:16:39,290
much cheaper than the cost of transferring the data
数据传输到其他计算节点的成本低

319
00:16:39,700 --> 00:16:40,850
to some other compute node. 
得多

320
00:16:41,180 --> 00:16:41,810
Again, think about it.
再一次 考虑一下

321
00:16:41,820 --> 00:16:42,610
I have a disk. 
我有一张磁盘

322
00:16:42,800 --> 00:16:45,190
On one node, I have a computation another.
在一个节点上 我有另一个节点的计算

323
00:16:45,200 --> 00:16:49,030
I'd rather send the query to the data where on the disk has the disk
我宁愿将查询发送到磁盘上有它所在的磁盘的数据

324
00:16:49,040 --> 00:16:49,830
where it's located, 
然后

325
00:16:50,170 --> 00:16:51,490
then have to transfer it out. 
将其传输出去

326
00:16:52,080 --> 00:16:54,030
Because the network, the net cost is so high.
因为网络 网络成本太高了

327
00:16:54,040 --> 00:16:58,360
The alternative is to pull the data of the query. 
另一种方法是提取查询的数据

328
00:16:59,140 --> 00:17:01,720
This is where you are, 
这是您所在的位置

329
00:17:02,460 --> 00:17:03,610
where the query is. 
也是查询所在的位置

330
00:17:03,620 --> 00:17:05,490
You leave it at that computational resource, 
你把它留在那个计算资源上

331
00:17:06,260 --> 00:17:08,290
and then the data needs access to the operator, 
然后数据需要访问操作员

332
00:17:08,300 --> 00:17:09,770
the data that needs opportunities access. 
需要机会访问的数据

333
00:17:10,200 --> 00:17:12,150
You transferred over to that node. 
你转到了那个节点

334
00:17:14,040 --> 00:17:18,390
This could be necessary in a situation where you can't run any queries. 
在无法运行任何查询的情况下 这可能是必要的

335
00:17:18,400 --> 00:17:20,950
You can't do any computation where the day is located. 
你不能在日期所在的地方做任何计算

336
00:17:21,410 --> 00:17:24,160
If I have aa remote file system where I can't run anything, 
如果我有一个远程文件系统

337
00:17:24,170 --> 00:17:27,280
I I can only do gets and sets and delete, 
在那里我不能运行任何东西 我只能做获取 设置和删除

338
00:17:27,680 --> 00:17:29,430
you'd have to do pull data the query. 
你必须从查询中提取数据

339
00:17:31,400 --> 00:17:31,790
As I said, 
正如我所说的

340
00:17:31,800 --> 00:17:36,550
the harbor has gotten much better in recent years where this isn't
Harbor在最近几年变得更好了

341
00:17:36,560 --> 00:17:41,520
always the best design choice upholding the query because now again, 
这并不总是支持查询的最佳设计选择

342
00:17:41,530 --> 00:17:45,480
the network on in cloud systems is pretty fat. 
因为现在云系统中的网络非常庞大

343
00:17:47,220 --> 00:17:48,440
The other interesting also, too,
另一个有趣的地方是

344
00:17:48,450 --> 00:17:52,740
is saying these are always usually exclusive anymore because there are
这些通常都是排他性的

345
00:17:52,750 --> 00:17:56,130
some there are some cloud object
因为有一些云对象存储

346
00:17:56,140 --> 00:18:01,920
stores where you can actually push some computation to to the data, 
你可以将一些计算推送到数据中

347
00:18:01,930 --> 00:18:03,920
even though you wouldn't think you would be able to do it. 
即使你不认为你能做到

348
00:18:03,930 --> 00:18:05,680
So on s three, 
所以在S3上

349
00:18:05,690 --> 00:18:09,040
they have this select operator select operation, 
他们有这个选择操作符选择操作

350
00:18:09,280 --> 00:18:12,070
where you can do something that sort of looks like pseudo sequel. 
在那里你可以做一些看起来像伪Sequel的事情

351
00:18:15,040 --> 00:18:20,800
So where clause and you can send that to as an s three command to do
所以where子句 你可以把它作为一个s3命令发送到

352
00:18:20,810 --> 00:18:23,400
some initial filtering directly in s three, 
直接在s3中进行一些初始过滤

353
00:18:23,410 --> 00:18:28,010
so that you're only now trenching the data that would satisfy your where clause. 
这样你现在只需要挖掘满足你的where从句

354
00:18:28,020 --> 00:18:30,350
So again, the idea is that I do a select state,
同样 我的想法是做一个选择状态

355
00:18:30,360 --> 00:18:33,720
but I can maybe send portion of the where clause to s3, 
但我可以将where子句的一部分发送到s3

356
00:18:33,730 --> 00:18:37,640
s three knows how to negatively understand csv file, 
s3知道如何负面理解csv文件

357
00:18:37,650 --> 00:18:39,200
jason or public files, 
jason或公共文件 然后进行筛选

358
00:18:39,700 --> 00:18:44,740
and then do that filter and only send back the results that within the rows
只发送回满足它的行中的

359
00:18:44,750 --> 00:18:45,660
that satisfy it. 
结果

360
00:18:46,690 --> 00:18:47,760
That's actually pretty powerful now, 
这实际上是非常强大的

361
00:18:47,770 --> 00:18:49,880
because that's basically pushing the data query, 
因为这基本上是推动数据查询

362
00:18:50,330 --> 00:18:51,540
but not all of the query, 
但不是所有的查询

363
00:18:51,550 --> 00:18:55,260
but at least enough where you could do get a big way in doing some filtering. 
但至少在你可以做一些过滤的地方足够了

364
00:18:57,340 --> 00:18:58,250
Am amazon? 
我是亚马逊

365
00:18:59,300 --> 00:18:59,730
Sorry, 
抱歉

366
00:18:59,740 --> 00:19:04,090
microsoft azure blob storage has something pretty similar where you can do. 
Microsoft Azure Blob Storage有一些非常类似的功能

367
00:19:05,390 --> 00:19:08,270
You can do something again that looks like sql to do some initial filtering. 
您可以再次执行一些类似于sql的操作 以执行一些初始过滤

368
00:19:08,280 --> 00:19:10,110
I don't know what file formats they support, 
我不知道它们支持什么文件格式

369
00:19:10,120 --> 00:19:12,180
but the basic idea is the same thing. 
但基本思想是一样的

370
00:19:13,110 --> 00:19:19,400
So again, just going forward were or gonna be using pull query,
同样 接下来我们将使用pull查询

371
00:19:19,410 --> 00:19:24,030
pull data to the query for the lower portions of the query plan, 
将数据拉到查询计划的较低部分的查询中

372
00:19:24,040 --> 00:19:25,790
like the access methods of scanning the tables. 
比如扫描表的访问方法

373
00:19:26,320 --> 00:19:28,890
But then we're probably gonna really push the, 
但之后我们可能真的会推

374
00:19:30,180 --> 00:19:31,870
but and as we go from the intermediate phases, 
但是 当我们从中间阶段开始时

375
00:19:31,880 --> 00:19:34,030
it's also probably pushing data query as well. 
它也可能推动数据查询

376
00:19:37,580 --> 00:19:40,010
Now we want to talk about the two high level approaches
现在

377
00:19:40,020 --> 00:19:41,490
for better distributed system. 
我们想要讨论用于更好的分布式系统的两种高级方法

378
00:19:41,500 --> 00:19:42,370
Again, these aren't.
再说一次 这些不是

379
00:19:43,900 --> 00:19:45,130
This one is actually pretty, 
这个其实很漂亮

380
00:19:46,980 --> 00:19:51,120
this is pretty rigid or how this is a clean dichotomy for systems. 
这是非常严格的 或者说这是系统的一个清晰的二分法

381
00:19:51,130 --> 00:19:51,390
Today's. 
今天的

382
00:19:51,400 --> 00:19:55,500
But the the first type of architecture is the more traditional. 
但是第一种类型的架构是比较传统的

383
00:19:55,510 --> 00:19:59,020
One people think of distributed systems is a shared nothing system. 
一种人认为分布式系统是一种无共享的系统

384
00:19:59,250 --> 00:20:03,600
This is actually aa phrase or term coined by mike stern breaker going
这实际上是迈克·斯特恩·布雷克在20世纪80年代创造的一个短语

385
00:20:03,610 --> 00:20:04,800
back to like the 1980s. 
或术语

386
00:20:04,810 --> 00:20:05,160
And again, 
再一次 这里的想法是

387
00:20:05,170 --> 00:20:09,410
the idea here was this was presumed to be the better way to build
几十年来

388
00:20:09,420 --> 00:20:12,570
distributed data systems for decades up until
这一直被认为是建立分布式数据系统的更好方法

389
00:20:13,300 --> 00:20:15,520
maybe10 years ago. 
直到10年前

390
00:20:15,530 --> 00:20:19,600
The idea here is that each can have its own on cpu its own memory, 
这里的想法是 每一个都可以有自己的cpu 自己的内存

391
00:20:19,610 --> 00:20:21,200
its own locally attached disk. 
自己的本地连接磁盘

392
00:20:21,210 --> 00:20:21,320
Right? 
对的

393
00:20:21,330 --> 00:20:26,440
Think of like mvme and the only way that nodes can communicate
考虑一下mvme 节点可以相互通信并查看数据库的其他

394
00:20:26,450 --> 00:20:29,560
with each other and see other portions of the database is to send messages
部分的唯一方法是通过网络发送

395
00:20:29,570 --> 00:20:30,280
over the network. 
消息

396
00:20:31,330 --> 00:20:34,480
Like if i'm running on one node on the left, 
比如 如果我在左边的一个节点上运行

397
00:20:34,730 --> 00:20:37,000
and need access data on the right, I just can't peek into.
而需要访问右边的数据 我就无法窥视

398
00:20:38,050 --> 00:20:38,600
It's disk. 
是磁盘

399
00:20:38,610 --> 00:20:40,560
I gotta go send a message to send me that data. 
我得去发信息把数据发给我

400
00:20:41,450 --> 00:20:45,620
The database is going to be partitioned to disjoint subset across these nodes, 
数据库将被划分成一些跨节点的子集

401
00:20:45,630 --> 00:20:47,100
sometimes called shards, 
（有时称为碎片）

402
00:20:47,790 --> 00:20:54,010
where each node will have a right unique portion of the database of tables. 
其中每个节点将具有表数据库的正确唯一部分

403
00:20:54,020 --> 00:20:54,770
And ideally, 
理想情况下

404
00:20:54,780 --> 00:20:59,120
you want to pick a partition key that reduces the amount of data transfer
您希望选择一个分区键

405
00:20:59,130 --> 00:20:59,680
you have to do. 
以减少您必须执行的数据传输量

406
00:20:59,690 --> 00:21:00,680
When you do joins. 
当你加入的时候

407
00:21:01,130 --> 00:21:02,560
We'll talk a little bit this next class, 
我们将在下一节课上讨论一下

408
00:21:02,570 --> 00:21:05,200
which is historically a very hard problem to do. 
这在历史上是一个很难解决的问题

409
00:21:06,590 --> 00:21:08,270
But it does make a big difference. 
但这确实有很大的不同

410
00:21:08,280 --> 00:21:15,530
The challenge is going to be now that since the storage is tied to the compute, 
现在的挑战是 由于存储与计算绑定

411
00:21:15,540 --> 00:21:19,860
I can't easily add either more capacity at the compute side
如果不引入一个全新的节点

412
00:21:19,870 --> 00:21:22,930
of the storage side without bringing in a whole new node. 
我就无法轻松地在存储端的计算端添加更多容量

413
00:21:23,130 --> 00:21:24,160
Now, if I bring a whole new node,
现在 如果我带来一个全新的节点

414
00:21:24,170 --> 00:21:26,680
i've got to do re partitioning to move data around, 
我必须重新分区来移动数据

415
00:21:27,330 --> 00:21:28,440
to balance things out. 
以平衡这些数据

416
00:21:30,210 --> 00:21:31,920
Again, we'll discuss the implications this.
再一次 我们将讨论它的含义

417
00:21:31,930 --> 00:21:33,080
And so sorry, 
很抱歉

418
00:21:33,090 --> 00:21:34,480
each of these are also kind of quantitative. 
每一个都是定量的

419
00:21:34,490 --> 00:21:36,960
This is, when I say, I know this, I one of these.
这是 当我说 我知道这个 我是其中之一

420
00:21:38,210 --> 00:21:38,750
The other thing, too,
另一件事也是 因为数据被认为是本地的

421
00:21:38,760 --> 00:21:43,390
is that because data is considered local like what they locally attached disk, 
就像它们本地连接的磁盘一样

422
00:21:44,570 --> 00:21:48,080
traditionally, a database system will access the disk using it,
传统上 数据库系统将使用它来访问磁盘

423
00:21:48,090 --> 00:21:55,860
like a posix api or nvme like it'll be a syscalls to go read data, 
就像posix api或nvme一样

424
00:21:56,130 --> 00:21:56,740
directly disk. 
它将被系统调用去读取数据 直接访问磁盘

425
00:21:57,190 --> 00:22:00,880
There's the you can do kernel bypass, 
你可以绕过内核

426
00:22:02,220 --> 00:22:04,550
which the stories need to play in kit from intel. 
这些故事需要在英特尔的套件中播放

427
00:22:04,970 --> 00:22:05,810
That's a nightmare. 
那是一场噩梦

428
00:22:05,820 --> 00:22:07,330
We'll talk about that later on. 
我们稍后再谈这个问题

429
00:22:07,340 --> 00:22:08,640
But in general, 
但通常情况下

430
00:22:08,650 --> 00:22:11,840
you're using posix to do read and writes to the local disk. 
您使用posix来对本地磁盘进行读写

431
00:22:12,300 --> 00:22:15,730
If I need to send things over the network to communicate with another node, 
如果我需要通过网络发送东西来与另一个节点通信

432
00:22:16,420 --> 00:22:18,740
make the perfect calls into it as well. 
也可以对它进行完美的调用

433
00:22:20,000 --> 00:22:22,440
The alternative approach for the shipment is called shared disk. 
另一种运送方式称为共享磁盘

434
00:22:22,450 --> 00:22:24,320
And with this one now, 
现在有了这个

435
00:22:24,650 --> 00:22:28,760
is we still have every single database data system node that have
我们仍然有每一个数据库数据系统节点

436
00:22:28,770 --> 00:22:30,360
their own cpu memory and disk. 
它们都有自己的CPU内存和磁盘

437
00:22:30,840 --> 00:22:35,590
But that disk on that each node is gonna be essentially just a cache, 
但每个节点上的磁盘本质上只是一个缓存

438
00:22:36,060 --> 00:22:40,530
and that the persistent data or the persistent files of the database system
数据库系统的持久数据或持久文件将位于

439
00:22:40,540 --> 00:22:43,370
is going to be down on this separate storage layer. 
这个单独的存储层上

440
00:22:44,050 --> 00:22:45,640
At the top here, we'll have the compute layer.
在顶部 我们将有计算层

441
00:22:45,650 --> 00:22:47,480
This is where we're gonna execute queries. 
这就是我们要执行查询的地方

442
00:22:47,850 --> 00:22:51,650
They can communicate with each other over network of the network, 
它们可以通过网络的网络彼此通信

443
00:22:51,660 --> 00:22:55,050
but that their storage is essentially used for ephemeral data, 
但是它们的存储基本上用于短暂的数据

444
00:22:55,370 --> 00:22:58,040
either a as a wirte through cache in the case of snowflake
或者像snowflake，作为一个write-through cache

445
00:22:58,050 --> 00:23:01,680
or a holding area staging area for intermediate data. 
或者用于中间数据的保持区域暂存区域

446
00:23:01,890 --> 00:23:04,360
But the node is essentially stateless, 
但节点本质上是无状态的

447
00:23:04,370 --> 00:23:05,440
meaning if it crashes, 
这意味着如果它崩溃

448
00:23:05,450 --> 00:23:07,760
the compute layer of the node is stateless, mean it crashes.
节点的计算层是无状态的 这意味着它崩溃

449
00:23:08,160 --> 00:23:10,670
We don't lose the database because that's all stored
我们不会丢失数据库

450
00:23:10,680 --> 00:23:13,870
down here in the storage layer. 
因为它们都存储在这里的存储层

451
00:23:13,880 --> 00:23:17,850
And the the storage essentially looks as one giant disk. 
存储本质上看起来就像一个巨大的磁盘

452
00:23:18,780 --> 00:23:20,750
It's not entirely true in an object store, 
这在对象存储中并不完全正确

453
00:23:20,760 --> 00:23:28,000
but because you don't access it through a posix api you use a use
但因为您不通过posix api访问它

454
00:23:28,010 --> 00:23:29,360
like a user space, 
所以您使用类似于用户空间

455
00:23:29,370 --> 00:23:33,520
api that the cloud vendor or the whatever the object store. 
云供应商的API或任何对象存储

456
00:23:34,120 --> 00:23:35,680
If you're un they would provide. 
如果你不在 他们会提供

457
00:23:37,450 --> 00:23:40,400
In the reason why this matters instead of using the posix api
为什么这很重要 而不是使用posix api

458
00:23:40,410 --> 00:23:43,970
because if it's a posix api the database system is not aware of
因为如果它posix api 数据库系统是不知道的

459
00:23:47,210 --> 00:23:51,510
of maybe the the different physical occasions of the data. 
可能是数据的不同物理场合

460
00:23:51,520 --> 00:23:54,240
I you would have different file pass and so forth like that. 
你会有不同的文件传递等等

461
00:23:54,250 --> 00:23:56,970
But like the data system, 
但就像数据系统一样

462
00:23:58,530 --> 00:23:59,330
it would just see, 
它只会看到

463
00:23:59,340 --> 00:24:03,850
here's some file system ii can read and write to and maybe not understand
这里的一些文件系统ii可以读取和写入

464
00:24:03,860 --> 00:24:05,850
or could not push down, 
可能不理解或不能向下推

465
00:24:06,650 --> 00:24:09,440
like the selects in the filtering stuff that we talked about before
就像我们之前讨论过的过滤中的选择

466
00:24:09,450 --> 00:24:12,150
if you're using a of using posix, 
如果你正在使用使用posix

467
00:24:12,160 --> 00:24:15,010
because there's no way to do, hey, read this,
因为没有办法做 嘿 读这个

468
00:24:15,020 --> 00:24:17,860
but also filter some stuff for me down and storage there. 
但也为我过滤了一些东西并储存在那里

469
00:24:18,380 --> 00:24:19,500
This to recap everything, 
重述这一切

470
00:24:19,590 --> 00:24:21,300
the two choices we share nothing. 
这两个选择我们没有任何共同之处

471
00:24:22,370 --> 00:24:26,490
And the pros and cons of this that it potentially achieves better performance, 
它的优点和缺点是它可能实现更好的性能

472
00:24:26,500 --> 00:24:31,550
because the each data system node, 
因为每个数据系统节点

473
00:24:31,560 --> 00:24:34,110
if it's accessing only data is local to it, 
如果它只访问本地数据

474
00:24:37,440 --> 00:24:40,250
it'll perform better because there's less data movement
它的性能会更好

475
00:24:40,260 --> 00:24:42,890
potentially between the nodes, 
因为节点之间的数据移动可能会更少

476
00:24:42,900 --> 00:24:43,330
right? 
对吧

477
00:24:43,730 --> 00:24:51,790
And I said with modern mvmvme drives with pcicpcie five or six. 
我说的是现代的MVMVME驱动器 有五个或六个PCICpcie

478
00:24:51,800 --> 00:24:53,830
The lower the lays one is those things pretty fat pipes, 
较低的一层是那些相当大的管道

479
00:24:53,840 --> 00:24:55,270
but you can let that you can get. 
但你可以让你可以得到

480
00:24:55,280 --> 00:24:58,150
But and modern networks are pretty good that it is too. 
但是现代网络也很好

481
00:24:58,160 --> 00:25:03,070
So traditionally it was always gonna be faster to read from local disc
所以传统上

482
00:25:03,080 --> 00:25:03,790
than over the network, 
从本地磁盘读取总是比通过网络读取更快

483
00:25:03,800 --> 00:25:05,430
but it's not always the case these days. 
但现在情况并非总是如此

484
00:25:06,490 --> 00:25:10,520
But the big challenge is going to be harder to scale out capacity, 
但最大的挑战是更难横向扩展容量

485
00:25:12,460 --> 00:25:14,010
actually, in both directions up and down.
实际上 在上下两个方向

486
00:25:14,020 --> 00:25:17,170
But i can't add new nodes without shuffling data around to sort
但是我不能添加新的节点而不移动数据来重新

487
00:25:17,180 --> 00:25:18,490
of re balance things we participate. 
平衡我们参与的事情

488
00:25:18,990 --> 00:25:22,630
I can't take node away without doing the same thing with coalescing. 
如果不对合并做同样的事情 我不能把节点拿走

489
00:25:24,850 --> 00:25:26,160
Now with a shared disk system, 
现在有了一个共享磁盘系统

490
00:25:26,410 --> 00:25:29,640
a a in a modern setting in the cloud, 
在云中的现代环境中

491
00:25:30,000 --> 00:25:32,990
the great big advantages that would be able to scale out the compute
能够扩展计算和存储的巨大

492
00:25:33,000 --> 00:25:34,630
there and stores are independently. 
优势是独立的

493
00:25:34,910 --> 00:25:38,820
Meaning if i'm by myself being cpu bound on my nodes, 
这意味着如果我自己在我的节点上被cpu绑定

494
00:25:38,830 --> 00:25:41,720
then I can add new compute nodes because they're stateless. 
那么我可以添加新的计算节点 因为它们是无状态的

495
00:25:41,990 --> 00:25:43,740
I don't have to do, 
我不需要这样做

496
00:25:44,070 --> 00:25:48,620
I don't have to move a lot of data around to rebound things. 
我不需要移动大量的数据来反弹

497
00:25:48,630 --> 00:25:49,020
Again, 
雪花

498
00:25:49,030 --> 00:25:53,140
the snowflake paper talks about how to use consistent hashing to change
论文再次讨论了如何使用一致的

499
00:25:53,150 --> 00:25:56,530
the assignment of what files are processed from
哈希来更改处理文件的分配

500
00:25:57,540 --> 00:26:04,700
persistent storage layer to the new nodes. 
新节点的持久存储层

501
00:26:06,610 --> 00:26:08,520
But that's you're warming it up. 
但那是你在热身

502
00:26:08,730 --> 00:26:11,100
And this telling here we got here. 
我们在这里讲述的是

503
00:26:11,910 --> 00:26:13,940
Here's the data you start processing. 
这是你开始处理的数据

504
00:26:13,950 --> 00:26:16,660
It's not you have to always copy things from one note to the other, 
这并不是说你必须总是把东西从一个笔记复制到另一个笔记

505
00:26:16,800 --> 00:26:17,550
which other has, 
如果你想安全地做到这一点

506
00:26:17,800 --> 00:26:21,030
if you want to do that safely and not have any false positive plus negatives, 
并且没有任何错误的肯定和否定

507
00:26:21,040 --> 00:26:22,230
requires you to use transactions. 
你需要使用事务

508
00:26:22,240 --> 00:26:23,470
It's a little bit more complicated. 
这有点复杂

509
00:26:26,070 --> 00:26:27,460
The other nice advantage, too.
另一个很好的优势

510
00:26:27,950 --> 00:26:33,630
And this matters a lot in for sort of database systems that want to claim
这对于那些想要声明他们的服务的数据库系统来说非常

511
00:26:33,640 --> 00:26:34,550
that their service. 
重要

512
00:26:35,200 --> 00:26:40,050
I can shut down the compute layer that That portions that i'm not needing, 
我可以关闭计算层 我不需要的部分

513
00:26:40,060 --> 00:26:43,970
the knows i'm not needing if i'm not exiting any queries and the data
如果我没有退出任何查询 并且数据不会再次消失 我就知道我不需要

514
00:26:44,220 --> 00:26:46,230
doesn't go away again, 
在一个没有共享的系统中

515
00:26:46,240 --> 00:26:47,270
in a shared nothing system, 
因为

516
00:26:47,280 --> 00:26:49,070
because the computer is tied to the storage. 
计算机与存储相关联

517
00:26:49,080 --> 00:26:50,070
If I shut down a computer, 
如果我关闭了一台计算机

518
00:26:50,080 --> 00:26:53,440
know what that means that that portion of the database that's stored
知道这意味着存储在该节点中的

519
00:26:53,450 --> 00:26:55,480
in that node is no longer accessible. 
数据库部分不再可访问

520
00:26:56,090 --> 00:26:58,400
Whereas if I shut down compute nodes in a shared disk system, 
然而 如果我关闭共享磁盘系统中的计算节点

521
00:26:58,960 --> 00:27:03,100
then the other remaining nodes can go get data from the share disk. 
那么其他剩余的节点可以从共享磁盘获取数据

522
00:27:03,110 --> 00:27:03,620
No problem. 
没问题

523
00:27:05,840 --> 00:27:06,950
In a service abiding, 
在一个持久的服务中

524
00:27:07,920 --> 00:27:10,550
this can make a big difference because you can shut down idle things. 
这可以产生很大的不同 因为你可以关闭闲置的东西

525
00:27:12,280 --> 00:27:14,990
Then last one is the disadvantage we talked about. 
那么最后一个就是我们讲的劣势

526
00:27:15,490 --> 00:27:18,160
If I had to go retreat persistent data that isn't cached
如果我必须撤销没有在计算机节点

527
00:27:18,170 --> 00:27:19,920
locally in the computer node, 
本地缓存的持久数据

528
00:27:21,650 --> 00:27:25,610
I I had to pull it from the share, just layer the storage layer.
我必须将其从共享中拉出 只需对存储层进行分层

529
00:27:25,980 --> 00:27:29,880
And maybe I can do some early filtering that we talked
也许我可以做一些我们在亚马逊或azure上

530
00:27:29,890 --> 00:27:32,120
about from amazon or azure. 
讨论过的早期过滤

531
00:27:34,260 --> 00:27:35,960
But that that's not always the case, 
但情况并非总是如此

532
00:27:35,970 --> 00:27:38,760
because it depends on what the filter is actually want to do. 
因为这取决于过滤器实际上想要做什么

533
00:27:38,770 --> 00:27:45,420
Like you can't do another new complex rejects or execute audf on s three, 
就像你不能做另一个新的复杂的拒绝或者在S3上执行AUDF

534
00:27:45,430 --> 00:27:45,540
right? 
对吗

535
00:27:45,550 --> 00:27:47,100
They're not going to do that. 
他们不会这么做的

536
00:27:48,710 --> 00:27:49,900
Our focus going forward. 
我们未来的重点

537
00:27:50,160 --> 00:27:52,430
The rest semester we're gonna focus on shared systems. 
剩下的学期我们将专注于共享系统

538
00:27:52,440 --> 00:27:54,590
Now, for some of the algorithms we'll talk about,
现在 对于我们将要讨论的一些算法

539
00:27:54,600 --> 00:27:57,270
it doesn't actually matter like the join algorithms. 
它实际上并不重要 比如联合算法

540
00:27:57,430 --> 00:27:58,110
They don't know, 
他们不知道

541
00:27:58,120 --> 00:28:01,110
don't care whether it's reading from shared disc or share nothing system. 
也不关心是从共享光盘读取还是从无共享系统读取

542
00:28:01,120 --> 00:28:05,710
Because at that point of when you're actually computing the join, 
因为当您实际计算连接时 您已经扫描了表

543
00:28:06,060 --> 00:28:07,650
you've already scanned the table, right?
对吗

544
00:28:07,660 --> 00:28:08,930
And fed it into a scan operator. 
并将其输入扫描操作员

545
00:28:08,940 --> 00:28:10,370
So it doesn't matter where it came from. 
所以它来自哪里并不重要

546
00:28:11,490 --> 00:28:13,040
But just in the background minds, 
但只是在后台头脑中

547
00:28:13,050 --> 00:28:15,800
going to be mindful that we shouldn't think about, 
要注意我们不应该考虑

548
00:28:16,530 --> 00:28:18,800
if i'm scanning this table or scanning data, 
如果我在扫描这个表或扫描数据

549
00:28:20,640 --> 00:28:24,190
what does it mean to be coming from s3 or an object store? 
来自S3或对象存储意味着什么

550
00:28:24,800 --> 00:28:26,350
As I said before, traditionally,
正如我之前所说的

551
00:28:26,360 --> 00:28:29,500
the in a shared architecture, 
传统上 在共享体系结构中

552
00:28:29,510 --> 00:28:34,120
it was assumed that the storage layer was was initial. 
假设存储层是初始的

553
00:28:34,130 --> 00:28:36,200
The stories with me some dedicated
与我的故事有些执着

554
00:28:37,590 --> 00:28:42,030
on prem appliance or storage device or knobs that expose like a
在本地设备或存储设备或旋钮上 像posits api一样公开

555
00:28:42,040 --> 00:28:44,950
posits api that you could read and write to. 
您可以对其进行读取和写入

556
00:28:46,500 --> 00:28:48,900
In the more high end systems like rural exeter data, 
在像Rural Exeter Data这样的高端系统中

557
00:28:49,230 --> 00:28:51,300
they do use a storage appliance. 
他们确实使用了存储设备

558
00:28:52,600 --> 00:28:53,710
These racks are looking for this. 
这些架子在找这个

559
00:28:53,720 --> 00:28:54,950
You have to compute in storage layer, 
您必须在存储层进行计算

560
00:28:54,960 --> 00:28:58,550
but the the daily system is aware that it's talking
但日常系统知道

561
00:28:58,560 --> 00:29:02,470
to a special storage storage appliance. 
它正在与aaa特殊存储设备进行通信

562
00:29:03,230 --> 00:29:04,420
It goes over fiber channel, 
它通过光纤通道传输

563
00:29:04,430 --> 00:29:07,180
and it's not making posix calls to read data. 
不会主动调用来读取数据

564
00:29:07,190 --> 00:29:09,700
It's extending proprietary commands. 
它扩展了专有命令

565
00:29:10,100 --> 00:29:14,560
They actually can do some predicate push down in these systems. 
他们实际上可以在这些系统中做一些谓词下推

566
00:29:16,290 --> 00:29:18,680
But the systems, the environment we're gonna focus on, as I said,
但是系统 我们要关注的环境 正如我所说的

567
00:29:18,690 --> 00:29:19,680
is the object stores. 
是对象存储

568
00:29:20,690 --> 00:29:23,000
Because this is how everyone's building these modern o lab systems
因为这是每个人围绕云构建这些现代实验室

569
00:29:23,010 --> 00:29:23,760
around the cloud. 
系统的方式

570
00:29:24,910 --> 00:29:27,870
And then I said is that they're infinitely scalable. 
然后我说它们是无限可扩展的

571
00:29:28,270 --> 00:29:29,170
Put that in quotes, obviously,
很明显

572
00:29:29,180 --> 00:29:33,150
but like as long as your credit card has enough money on it, 
把它放在引号里 但只要你的信用卡上有足够的钱

573
00:29:33,240 --> 00:29:34,870
your account has money on it. 
你的账户上就有钱

574
00:29:34,880 --> 00:29:37,430
You can keep putting as much data as you want in amazon s three
你可以继续把你想要的数据放在亚马逊的三个和一个本地出口中

575
00:29:37,440 --> 00:29:39,990
and a local outlet stored for you with pretty high guarantees
为你存储相当高的保证

576
00:29:40,120 --> 00:29:40,870
and durability, 
和耐用性

577
00:29:41,800 --> 00:29:44,390
insane for azure in gc ps cloud storage. 
这对gc ps云存储中的azure来说是疯狂的

578
00:29:44,930 --> 00:29:51,880
So you no longer have to worry about provisioning storage space, 
因此 您不必再担心配置存储空间

579
00:29:51,890 --> 00:29:54,480
but he would have to do a a in a own premise system. 
但他必须在自己的前提系统中执行a

580
00:29:54,730 --> 00:29:58,440
You just keep throwing more and more data in your stores layer. 
你只是不断地把越来越多的数据扔进你的存储层

581
00:30:02,140 --> 00:30:07,680
And you don't have to scale you scale compute layer separately either. 
而且您也不必单独缩放计算层

582
00:30:08,070 --> 00:30:10,040
You just keep putting more data in and things are great. 
你只要不断输入更多的数据 事情就会变得很好

583
00:30:13,250 --> 00:30:15,040
These object stores what do they actually look like? 
这些对象存储它们实际上看起来像什么

584
00:30:17,010 --> 00:30:20,520
They're gonna expose you a pretty simple api we talk about the amazon select, 
他们将向您展示一个非常简单的api

585
00:30:20,530 --> 00:30:21,320
but in general, 
我们讨论了amazon select

586
00:30:21,520 --> 00:30:24,140
they're going to have three commands put, getting delete.
但通常 他们将有三个命令put 获取delete

587
00:30:25,650 --> 00:30:28,350
So what happens is you have these, 
所以发生的是你有这些

588
00:30:29,330 --> 00:30:31,480
you have your data that you're ingesting, 
你有你的数据 你正在摄取

589
00:30:31,490 --> 00:30:32,960
put your database. 
把你的数据库

590
00:30:32,970 --> 00:30:37,960
You're going to break them up into to these large immutable blocks of data, 
你要把它们分解成这些大的不可变的数据块

591
00:30:37,970 --> 00:30:38,880
these files, 
这些文件

592
00:30:38,890 --> 00:30:42,000
and you can do it put in to store it in the object store. 
你可以把它们存储在对象存储中

593
00:30:42,390 --> 00:30:47,300
Then you'll have some catalogue that'll keep track of for some file. 
然后你会有一些目录 可以跟踪一些文件

594
00:30:47,310 --> 00:30:49,780
Id is found at this location in the object store. 
在对象存储中的此位置找到

595
00:30:50,720 --> 00:30:54,580
We'll talk more about what these files look like next class. 
我们将在下一节课中更多地讨论这些文件的外观

596
00:30:55,550 --> 00:30:56,030
But in general, 
但一般来说

597
00:30:56,040 --> 00:31:00,410
they're going to be some binary format of a column storm and 1 kilometer format. 
它们将是柱风暴和1公里格式的一些二进制格式

598
00:31:01,340 --> 00:31:05,770
And it's gonna use a pax layout where within each block, 
它将使用PAX布局 在每个块中

599
00:31:07,570 --> 00:31:10,640
the attributes of the table will be broken up in a columnar fashion, 
表的属性将以列的方式分解

600
00:31:10,650 --> 00:31:16,240
but all the attributes for a single logical triple will be found in that block. 
但单个逻辑三元组的所有属性都将在该块中找到

601
00:31:16,690 --> 00:31:18,120
This is different than how, 
这与垂直存储文件和存储数据的方式不同

602
00:31:18,130 --> 00:31:20,920
like maybe the vertical store files, store data,
在垂直存储文件和存储数据的方式中

603
00:31:20,930 --> 00:31:24,280
where you have a separate file for each column at a table. 
表中的每一列都有一个单独的文件

604
00:31:24,490 --> 00:31:24,780
With packs, 
使用包

605
00:31:24,790 --> 00:31:28,990
you basically have all the data within a single attribute in the single file. 
您基本上可以在单个文件中的单个属性中拥有所有数据

606
00:31:29,440 --> 00:31:30,230
There'll be a header. 
会有一个标题

607
00:31:30,240 --> 00:31:31,430
Sometimes the footer, 
有时

608
00:31:31,440 --> 00:31:36,950
like in part k will contain metadata about the offsets when the file
页脚（如第K部分）将包含有关偏移的元数据

609
00:31:36,960 --> 00:31:38,150
for where columns start, 
当文件中的列开始时

610
00:31:38,420 --> 00:31:42,320
what kind of compression schemes are using for the different columns that
对于每个配置文件可以更改的不同列

611
00:31:42,330 --> 00:31:45,190
can change per file? 
使用哪种压缩方案

612
00:31:45,600 --> 00:31:49,390
Sometimes there's pre computed indexes to do filtering or zone maps. 
有时会有预先计算的索引来进行过滤或区域映射

613
00:31:49,400 --> 00:31:51,890
Again, these are things will cover later in the semester,
同样 这些内容将在本学期晚些时候介绍

614
00:31:51,900 --> 00:31:54,310
but he would store all that information within the file itself. 
但他会将所有信息存储在文件本身中

615
00:31:55,640 --> 00:31:58,420
Now, when you want to start accessing this data in the object store,
现在 当您想要开始访问对象存储中的数据时

616
00:32:01,120 --> 00:32:04,920
you would make a request on the other store just to retrieve the header
您将在另一个存储上发出请求

617
00:32:05,380 --> 00:32:09,150
of the of the file block. 
只是为了检索文件块的头

618
00:32:09,400 --> 00:32:13,700
And then that'll get all that meta data about where the columns are located
然后将获得所有关于列的位置以及使用哪种专业

619
00:32:13,710 --> 00:32:15,580
and what kind of professional teams are being used. 
团队的元数据

620
00:32:15,590 --> 00:32:20,420
And then you can then do selective gets just to get the byte ranges
然后 您可以执行选择性获取

621
00:32:20,430 --> 00:32:21,500
of the data you actually need. 
以获取实际需要的数据的字节范围

622
00:32:21,510 --> 00:32:25,950
So you don't need to bring in the full page or the full block
因此 您不需要从对象存储中引入整个页面或整个块

623
00:32:26,310 --> 00:32:28,500
from the object store and then parse it. 
然后对其进行解析

624
00:32:29,070 --> 00:32:31,780
You can just bring a little bit in figure out where you need to go to get
你可以带一点点进去

625
00:32:31,790 --> 00:32:32,420
the rest of it. 
弄清楚你需要去哪里得到剩下的部分

626
00:32:33,320 --> 00:32:35,110
Have retrieved this the minimum amount. 
已经取回了这个最小的金额

627
00:32:35,120 --> 00:32:37,270
Again, that's different than what we talked about doing.
再说一次 这与我们所说的不同

628
00:32:38,270 --> 00:32:42,860
Disk io in the interclass where we'd say every pages eight kilobytes
类间的磁盘IO

629
00:32:42,870 --> 00:32:43,380
or four kilobytes, 
我们说每页8千字节或4千字节

630
00:32:43,390 --> 00:32:44,500
you go fetch the whole thing. 
你去获取整个东西

631
00:32:44,680 --> 00:32:46,320
Even though you only need a portion of it, 
即使你只需要它的一部分

632
00:32:46,330 --> 00:32:48,790
you can be a bit more intelligent here, because the blocks are larger,
你可以在这里更聪明一点

633
00:32:48,800 --> 00:32:50,690
and you obviously don't want to achieve the whole block. 
因为块更大 你显然不想实现整个块

634
00:32:51,880 --> 00:32:53,990
I think in snowflake, the block size is,
我认为在雪花中 块的大小是

635
00:32:54,000 --> 00:32:56,390
I think the lowest is like 50 megs, and maybe goes up to the corner,
我认为最低的是50兆

636
00:32:56,400 --> 00:32:57,350
max or something like that. 
可能会上升到角落 最大值或类似的东西

637
00:32:57,480 --> 00:32:58,390
I see these aren't small blocks. 
我看这些不是小块

638
00:32:58,400 --> 00:32:59,030
These are pretty big. 
这些相当大

639
00:32:59,040 --> 00:33:03,420
So you don't need to fetch the whole thing if you don't need it. 
所以如果你不需要的话 你不需要把整个东西都拿出来

640
00:33:03,430 --> 00:33:06,620
So one interesting system I like to bring up is yellow brick. 
我想提出的一个有趣的系统是黄砖

641
00:33:07,040 --> 00:33:09,350
So this is actually a slide from a talk they gave with us
这实际上是几年前流感大流行期间

642
00:33:09,800 --> 00:33:11,630
during the pandemic a few years ago, 
他们给我们的演讲中的一张幻灯片

643
00:33:12,440 --> 00:33:16,830
but it shows this sort of how they're using the object store with s three. 
但它展示了他们是如何使用对象存储和s3的

644
00:33:17,950 --> 00:33:20,100
Again, this architecture looks a lot like what i've already showed you.
同样 这个体系结构看起来与我已经向您展示的非常相似

645
00:33:20,110 --> 00:33:21,660
You have these worker nodes, 
你有这些工人节点

646
00:33:22,230 --> 00:33:25,540
and they have this locally attached mdmmessd
他们有这个本地连接的mdmmessd

647
00:33:26,070 --> 00:33:29,850
so it's interesting about yellow brick is
所以关于yellow brick的有趣之处

648
00:33:29,860 --> 00:33:33,900
that they talked about how they tried using amazons, 
在于他们谈到了他们如何尝试使用亚马逊

649
00:33:35,020 --> 00:33:36,210
s three library, 
S三库

650
00:33:36,620 --> 00:33:38,730
the access library to communicate with us three. 
Access三库与我们交流

651
00:33:39,170 --> 00:33:41,190
And they found it was super slow. 
他们发现它超级慢

652
00:33:41,200 --> 00:33:44,010
So they wrote their own library. 
所以他们写了自己的图书馆

653
00:33:46,050 --> 00:33:49,560
They were to get a three x performance using retrieving the same data
他们使用自定义库而不是Amazons从S3检索相同的数据

654
00:33:49,570 --> 00:33:52,480
from s three using their custom library instead of using amazons. 
从而获得三倍的性能

655
00:33:53,780 --> 00:33:55,930
I'm bringing this up just to say that you don't necessarily, 
我提出这个问题只是想说

656
00:33:55,940 --> 00:33:58,650
you don't have to use whatever that the cabinet provides you. 
你不需要 你不需要使用内阁提供给你的任何东西

657
00:33:58,660 --> 00:33:59,810
For the client side library. 
用于客户端库

658
00:33:59,820 --> 00:34:02,810
You can write your own thing and potentially get much better performance. 
您可以编写自己的东西 并可能获得更好的性能

659
00:34:02,820 --> 00:34:04,530
I forget what the secret they used here. 
我忘了他们在这里使用的秘密是什么

660
00:34:04,540 --> 00:34:05,530
They might be doing. 
他们可能在做

661
00:34:07,070 --> 00:34:09,860
You may be doing kernel bypass to get through the optic store, 
您可能正在执行内核绕过以通过光学存储

662
00:34:09,870 --> 00:34:11,180
and you can use on the client side. 
并且您可以在客户端使用

663
00:34:11,190 --> 00:34:13,540
You can't do it in the server side because amazon controls the server. 
你不能在服务器端做这件事 因为亚马逊控制着服务器

664
00:34:14,900 --> 00:34:17,330
But I another cool thing they were doing is that they would communicate
但他们正在做的另一件很酷的事情是

665
00:34:17,340 --> 00:34:21,860
between the different nodes using udp with the intel
他们将使用udp

666
00:34:21,870 --> 00:34:26,130
dpdk which is the data plane development kit. 
和英特尔dpdk（数据平面开发套件）在不同节点之间进行通信

667
00:34:26,480 --> 00:34:29,780
It's a way to do kernel bypass on talking to the nick, which is,
这是一种在与Nick对话时绕过内核的方法

668
00:34:31,710 --> 00:34:33,500
she's not easy to use it. 
她不容易使用它

669
00:34:33,510 --> 00:34:33,740
Again. 
又

670
00:34:33,750 --> 00:34:34,540
We'll talk about this later. 
我们以后再谈这个

671
00:34:34,550 --> 00:34:35,260
All this. 
这一切

672
00:34:35,270 --> 00:34:38,380
Intel current life passed off is like super tricky and super painful. 
英特尔目前的生活就像是超级棘手和超级痛苦

673
00:34:39,540 --> 00:34:40,890
We'll cover that throughout the semester. 
我们将在整个学期中讨论这个问题

674
00:34:43,010 --> 00:34:46,040
I there's a bunch of other stuff that we're not gonna talk about today, 
还有很多其他的东西我们今天不会讨论

675
00:34:46,050 --> 00:34:49,720
but we'll start covering these things over the next throughout the semester. 
但我们会在下个学期开始讨论这些东西

676
00:34:50,080 --> 00:34:51,470
There's different file formats. 
有不同的文件格式

677
00:34:51,480 --> 00:34:53,110
Again, I mentioned the tax layout,
我再次提到了税收布局

678
00:34:53,120 --> 00:34:54,950
but we'll talk about different approaches. 
但我们将讨论不同的方法

679
00:34:56,100 --> 00:34:57,170
Again, all this is gonna be very,
再一次 所有这些都将是非常非常令人沮丧的

680
00:34:57,180 --> 00:34:58,970
highly depressed because it's kilometer storage. 
因为它是公里存储

681
00:34:59,360 --> 00:35:00,250
To talk about next class. 
谈论下一节课

682
00:35:00,260 --> 00:35:02,980
How do you actually decide how to do horizontal partitioning on your tables
您实际上如何决定如何对表进行

683
00:35:03,480 --> 00:35:06,980
and split them up based on values? 
水平分区并根据值对其进行拆分

684
00:35:06,990 --> 00:35:11,300
So you can minimize the data transfer between nodes for inmate results. 
因此 您可以最大限度地减少囚犯结果节点之间的数据传输

685
00:35:11,850 --> 00:35:14,820
Then there's a whole process of how you wanna do ingest new data. 
然后是你如何摄取新数据的整个过程

686
00:35:15,170 --> 00:35:18,900
Like a big part of databases are absolutely you want to put data in it. 
就像数据库的一大部分 你绝对想把数据放进去

687
00:35:19,890 --> 00:35:24,670
There are there's fast path instead of running a certain
有快速的路径

688
00:35:24,680 --> 00:35:26,910
as fast as new bulk loading and get hit
而不是运行某种像新的批量加载一样快的速度

689
00:35:26,920 --> 00:35:28,790
into the system quickly as possible. 
并尽可能快地进入系统

690
00:35:29,640 --> 00:35:31,470
Obviously, want to get update attention, delete it.
显然 想要得到更新的关注 删除它

691
00:35:31,480 --> 00:35:32,390
How do you handle that? 
你是怎么处理的

692
00:35:32,400 --> 00:35:33,950
Even though the files are immutable? 
即使文件是不可变的

693
00:35:34,280 --> 00:35:35,900
And then we're not gonna talk about too much about this, 
然后我们不会谈论太多

694
00:35:35,910 --> 00:35:36,620
but this will come up. 
但这会出现

695
00:35:36,630 --> 00:35:43,530
We talk about data lakes, especially with the photon and database later on.
稍后我们将讨论数据湖 特别是光子和数据库

696
00:35:45,180 --> 00:35:48,320
But the a in a monaco system, 
但是在摩纳哥系统中

697
00:35:48,330 --> 00:35:52,720
you also want to support the ability for people to have existing files
您还希望支持人们在s3中拥有现有

698
00:35:52,730 --> 00:35:54,240
just sitting in s three. 
文件的能力

699
00:35:55,300 --> 00:35:58,930
They can see in a party format without having to first bulk load them
他们可以在Party格式中查看

700
00:35:58,940 --> 00:35:59,570
to your data system. 
而不必首先将它们批量加载到您的数据系统中

701
00:35:59,580 --> 00:36:01,810
You want to be able to operate directly in the files as exist. 
您希望能够直接在现有的文件中进行操作

702
00:36:02,210 --> 00:36:05,400
How you discover that these files exist? 
如何发现这些文件的存在

703
00:36:06,010 --> 00:36:07,290
Then what have you scheduling? 
那你有什么安排

704
00:36:07,600 --> 00:36:09,390
Productivity will spend a little time talking about. 
生产力将花一点时间谈论

705
00:36:09,400 --> 00:36:12,980
It means that as the query is running, 
这意味着 当查询运行时

706
00:36:12,990 --> 00:36:17,940
how do we maybe make changes on the fly to better utilize resources, 
我们如何动态地进行更改 以更好地利用资源 可能是调配不足或过度调配

707
00:36:17,950 --> 00:36:20,060
maybe under provision or over provision, 
任务或工作人员

708
00:36:20,620 --> 00:36:24,490
the task or the workers and we can scale that and make changes as we go along. 
我们可以对其进行扩展 并在运行过程中进行更改

709
00:36:24,500 --> 00:36:24,730
Again. 
又

710
00:36:24,940 --> 00:36:26,450
There's a lot of things we're not covering here. 
有很多事情我们在这里没有涉及

711
00:36:26,920 --> 00:36:27,510
That are important. 
这很重要

712
00:36:27,520 --> 00:36:29,390
We'll get to later this semester. 
我们将在这学期晚些时候开始

713
00:36:31,720 --> 00:36:35,560
Ii talked to this at the beginning when I make an observation here, 
我一开始就谈到了这一点

714
00:36:35,570 --> 00:36:38,160
that in the snowflake paper, you guys read,
当我在这里做一个观察时 在雪花的论文中 你们读到

715
00:36:38,440 --> 00:36:40,150
even or even though it's about snowflake, 
即使它是关于雪花的

716
00:36:40,520 --> 00:36:41,990
they talk about snowflake does certain things, 
他们谈论雪花做某些事情

717
00:36:42,000 --> 00:36:44,310
but they make some important observations about, 
但他们做了一些重要的观察

718
00:36:44,700 --> 00:36:47,220
again, what a monolith system looks like.
再一次 一个整体系统是什么样子的

719
00:36:47,510 --> 00:36:48,700
That's why I had you guys read it. 
这就是我让你们读它的原因

720
00:36:49,170 --> 00:36:51,370
But the thing to.out, 
但是要.出来的东西

721
00:36:51,730 --> 00:36:55,960
a system like snowflake, like drama, like big query, like yellow brick,
一个像雪花一样的系统 像戏剧一样的系统 像大查询一样的黄砖

722
00:36:57,350 --> 00:36:58,840
like data bricks. 
比如数据砖块

723
00:36:58,850 --> 00:37:00,840
These are monolithic systems, 
这些是整体系统

724
00:37:01,210 --> 00:37:05,210
meaning that they're building all the components that that you need
这意味着它们正在构建数据系统所需的所有

725
00:37:05,220 --> 00:37:06,170
for the data system. 
组件

726
00:37:11,260 --> 00:37:12,440
These are monolithic systems, 
这些是整体系统

727
00:37:12,450 --> 00:37:14,440
meaning they're building all the components you need to build. 
这意味着它们正在构建您需要构建的所有组件

728
00:37:14,450 --> 00:37:16,720
It is entirely in house with their engineers. 
这完全是在他们的工程师内部

729
00:37:18,930 --> 00:37:22,760
Most of the non academic systems that we'll talk about the semester
我们这学期要讨论的大多数非学术系统都会有几乎

730
00:37:23,210 --> 00:37:25,040
are going to have pretty much the same architecture, 
相同的架构

731
00:37:25,050 --> 00:37:25,320
right? 
对吧

732
00:37:25,330 --> 00:37:27,560
You have query engine, you have some storage layer,
你有查询引擎 你有一些存储层

733
00:37:28,020 --> 00:37:32,680
you have an orchestra to move data around xe task, scheduler, some sort,
你有一个管弦乐队来移动xe任务

734
00:37:33,010 --> 00:37:33,310
right? 
调度程序 某种类型的数据 对吗

735
00:37:33,620 --> 00:37:37,380
They're all pretty much implementing the same thing. 
他们几乎都在实现同样的事情

736
00:37:38,530 --> 00:37:43,870
And that means basically that we have people where they have companies
这基本上意味着我们有很多人

737
00:37:43,880 --> 00:37:45,390
in a lot of money, 
他们有很多钱的公司

738
00:37:45,880 --> 00:37:50,130
writing the same database and software over and over again. 
一遍又一遍地编写相同的数据库和软件

739
00:37:50,140 --> 00:37:50,360
Right? 
对的

740
00:37:50,370 --> 00:37:52,890
They're writing the same thing to do the same thing. 
他们写同样的东西 做同样的事情

741
00:37:55,290 --> 00:37:59,450
And obviously, that's a it's a huge labor effort to do this.
很明显 这是一个巨大的劳动努力

742
00:37:59,880 --> 00:38:07,720
And it may not be in the best interest for the database community, 
从广义上讲 这样做可能不符合数据库社区的最佳利益

743
00:38:07,730 --> 00:38:09,880
broadly speaking, to do this, right?
对吗

744
00:38:09,890 --> 00:38:11,760
If everybody's implementing the same sequel parser, 
如果每个人都实现相同的Sequel解析器

745
00:38:13,480 --> 00:38:16,220
what is that actually that moving us forward? 
推动我们前进的到底是什么

746
00:38:16,230 --> 00:38:16,620
Right? 
对的

747
00:38:16,910 --> 00:38:21,280
We may be better to be better spent doing other more innovative things. 
我们最好花更多的时间去做其他更具创新性的事情

748
00:38:21,290 --> 00:38:27,680
And so there's this interesting development in the last5 years or so, 
所以在过去5年左右的时间里

749
00:38:27,690 --> 00:38:30,730
where you're starting to see various projects
有一个有趣的发展

750
00:38:30,820 --> 00:38:35,630
of in the own source committee or organizations to
你开始看到在自己的源代码委员会或组织中建立各种项目

751
00:38:35,640 --> 00:38:40,160
build to break out portions of a modern olap system
将现代重叠系统的部分分解为独立的组件

752
00:38:40,430 --> 00:38:43,580
into standalone components that are open source
这些组件是开源的

753
00:38:43,590 --> 00:38:45,180
that other people could build on. 
其他人可以在其上进行构建

754
00:38:45,190 --> 00:38:46,580
Other people could take advantage of, 
其他人可以利用

755
00:38:46,590 --> 00:38:53,820
and other people could could reuse either I for their own new olap systems. 
其他人可以在他们自己的新旧实验室系统中重复使用

756
00:38:53,830 --> 00:38:54,220
Right? 
对的

757
00:38:55,280 --> 00:38:57,180
Again, the idea here is that everyone said, everyone,
再一次 这里的想法是每个人都说

758
00:38:57,190 --> 00:38:58,060
everybody the same thing, 
每个人

759
00:38:58,070 --> 00:39:01,390
this all redundant parts of the system. 
每个人都说同样的事情 这都是系统的冗余部分

760
00:39:02,040 --> 00:39:07,710
Let's everyone work together on one thing and have that be really, 
让每个人在一件事上一起努力 让它变得非常非常好

761
00:39:07,720 --> 00:39:09,990
really good and everyone gets to reap the rewards. 
每个人都能收获回报

762
00:39:10,000 --> 00:39:11,350
They're sort of like linux, right?
它们有点像Linux 对吧

763
00:39:11,360 --> 00:39:13,690
Linux is, 
Linux是

764
00:39:15,260 --> 00:39:19,790
but it'd be kind of stupid now or insane outside of research and academia
但现在在研究和学术界之外

765
00:39:19,800 --> 00:39:22,350
to try to build a new operating system from scratch and to compete
试图从零开始构建一个新的操作系统并与Linux竞争

766
00:39:22,360 --> 00:39:22,910
with linux, 
这是一种愚蠢或疯狂的行为

767
00:39:23,730 --> 00:39:25,960
where the sort of potential wisdom where the mindset is, 
在这种潜在的智慧中

768
00:39:25,970 --> 00:39:28,040
everyone spoke about linux is what we're using. 
每个人都在谈论我们正在使用的Linux

769
00:39:28,050 --> 00:39:29,320
Let's make that really, really good.
让我们把它做得非常非常好

770
00:39:29,720 --> 00:39:32,100
It's not exactly the same because the os is a major undertaking
这并不完全相同 因为操作系统是一项主要任务

771
00:39:32,110 --> 00:39:34,000
where these other parts i'm talking about are much smaller. 
而我所说的其他部分要小得多

772
00:39:34,010 --> 00:39:36,260
But the idea is the same thing. 
但想法是一样的

773
00:39:36,560 --> 00:39:36,570
Right? 
对的

774
00:39:36,580 --> 00:39:42,010
Instead of having two people build competing parts that do the same thing, 
让我们一起工作

775
00:39:42,260 --> 00:39:43,050
let's work together. 
而不是让两个人制造做同样事情的相互竞争的部件

776
00:39:43,630 --> 00:39:46,940
You see this in sort of 4 sort of categories. 
你可以在4个类别中看到这一点

777
00:39:47,920 --> 00:39:51,540
And it's you can sort of see how this is tied to the different layers. 
你可以看到这是如何与不同的层联系在一起的

778
00:39:51,550 --> 00:39:52,820
I talked about the beginning, right?
我谈到了开始 对吗

779
00:39:53,290 --> 00:39:55,360
There's people getting system catalogues, query optimizer,
人们得到系统目录 查询优化器

780
00:39:55,370 --> 00:39:58,040
which is the hardest part of the system is file for us to act. 
这是系统中最难的部分 是我们要处理的文件

781
00:39:58,050 --> 00:40:01,940
So I want to quickly go through these different components and just sort
所以我想快速浏览一下这些不同的组件

782
00:40:01,950 --> 00:40:03,260
of talk about the major efforts, 
并讨论一下主要的工作

783
00:40:03,270 --> 00:40:08,380
the major projects just can be aware of that as we talked
主要的项目

784
00:40:08,390 --> 00:40:11,350
about the semester for these different engines and different systems. 
就像我们在学期中讨论的这些不同的引擎和不同的系统一样

785
00:40:11,620 --> 00:40:13,330
A lot of them aren't going to use these things, 
他们中的很多人不会使用这些东西

786
00:40:14,460 --> 00:40:18,270
but they'll be potentially open source alternatives for them. 
但对他们来说 它们将是潜在的开源替代品

787
00:40:20,280 --> 00:40:23,070
This idea of breaking the system up into reusable components
这种将系统分解为可重用组件

788
00:40:23,080 --> 00:40:25,830
or modular components is not new. 
或模块化组件的想法并不新鲜

789
00:40:26,510 --> 00:40:31,500
There's this paper by a famous researcher at msr surgery, 
MSR外科的一位著名研究员写了一篇论文

790
00:40:32,030 --> 00:40:32,860
where he back. 
他回到了那里

791
00:40:32,870 --> 00:40:35,700
And I think it's like 19 year, 2000,
我认为这就像19年 2000年

792
00:40:35,710 --> 00:40:39,130
we talked about what they call risk dial data systems. 
我们讨论了他们所谓的风险拨号数据系统

793
00:40:39,140 --> 00:40:39,290
Now, 
现在

794
00:40:39,820 --> 00:40:43,260
this argument here is built for how to make these different sort
这个论点是建立在如何使这些不同

795
00:40:43,270 --> 00:40:45,380
of sub components modular, 
种类的子组件模块化

796
00:40:45,390 --> 00:40:46,180
reusable, 
可重用

797
00:40:48,930 --> 00:40:51,720
to make it easier to automate the tuning of them, 
为了更容易地自动调整它们

798
00:40:52,050 --> 00:40:53,280
make of autonomous navy system. 
建立了自主海军系统

799
00:40:53,290 --> 00:40:54,440
But the idea of, again,
但是

800
00:40:54,450 --> 00:40:58,210
breaking the system into these separate layers that can be implemented
再一次

801
00:40:58,220 --> 00:41:02,100
through some standard api is basically the person that they were talking
把系统分解成这些可以通过一些标准PI实现的独立层的想法

802
00:41:02,110 --> 00:41:03,220
about this paper long ago. 
基本上是他们很久以前谈论这篇论文的人

803
00:41:05,010 --> 00:41:06,600
The first thing is the system catalogs. 
第一件事是系统目录

804
00:41:08,050 --> 00:41:09,280
Again, we're talking about this next class,
同样

805
00:41:09,290 --> 00:41:12,680
but system catalog basically is the registry where the data system keeps
我们正在讨论下一个类

806
00:41:12,690 --> 00:41:13,040
track of, 
但系统目录基本上是数据系统跟踪的注册表

807
00:41:13,390 --> 00:41:14,620
what the schema looks like. 
模式是什么样子的

808
00:41:14,630 --> 00:41:15,460
Potatoes they have, 
他们有土豆

809
00:41:15,470 --> 00:41:21,280
what calms they have if there's persistent files where they located
如果他们在磁盘上的对象存储中有持久文件

810
00:41:21,290 --> 00:41:22,560
in the object store on the disk. 
他们就会平静下来

811
00:41:22,900 --> 00:41:27,910
And so the way basically works that if you're calling your talent assessment, 
所以这种方法基本上是这样的 如果你调用你的人才评估

812
00:41:27,920 --> 00:41:29,390
insert the data for you, 
为你插入数据

813
00:41:29,400 --> 00:41:30,990
like an insert query or copy command, 
比如插入查询或复制命令

814
00:41:31,250 --> 00:41:34,520
then the data some will maintain its catalogs because it's trading
那么数据将会维护它的目录

815
00:41:34,530 --> 00:41:35,600
the files as it goes along. 
因为它正在交换文件

816
00:41:36,550 --> 00:41:38,380
If there's a discovery process, as they said,
如果有一个发现过程

817
00:41:38,390 --> 00:41:40,780
where there's a bunch of existing three files and the catalogue
就像他们说的 有一堆现有的三个文件

818
00:41:40,790 --> 00:41:43,980
needs to have an epi needs a way to be told these files that you beware
目录需要有一个epi 需要一种方式来告诉这些文件

819
00:41:43,990 --> 00:41:44,340
of them. 
你要小心它们

820
00:41:44,850 --> 00:41:49,700
There's now several projects where people build these sort
现在有几个项目 人们建立这种云目录 本质上

821
00:41:49,710 --> 00:41:52,140
of cloud catalogs that essentially, 
再一次 只是跟踪

822
00:41:52,150 --> 00:41:52,300
again, 
存在什么数据

823
00:41:52,310 --> 00:41:54,100
just keep track of what data exists, 
这些文件

824
00:41:55,090 --> 00:41:57,720
what cables and columns are in these files. 
中有什么电缆和列

825
00:41:57,970 --> 00:42:01,000
Sometimes there's some basic statistics about what's in them, 
有时会有一些关于其中内容的基本统计信息

826
00:42:01,010 --> 00:42:02,000
but as I said before, 
但正如我之前所说

827
00:42:02,010 --> 00:42:04,760
oftentimes the statistics are stored in the file themselves. 
统计信息通常存储在文件本身中

828
00:42:06,040 --> 00:42:09,030
Probably the most famous one or why the used one is the h catalog. 
可能最著名的一个或为什么使用的是H目录

829
00:42:09,040 --> 00:42:10,870
And this came out of the hive project. 
这来自蜂巢项目

830
00:42:11,900 --> 00:42:13,170
It's a wrapper of the hive, 
它是蜂巢的一个包装器

831
00:42:13,180 --> 00:42:14,420
meta store. 
元存储

832
00:42:14,430 --> 00:42:16,180
And I know you can use this for spark. 
我知道你可以用这个来点燃火花

833
00:42:16,190 --> 00:42:18,300
I think there's some other systems that take advantage of it. 
我认为还有其他一些系统可以利用它

834
00:42:18,680 --> 00:42:18,870
Again, 
再一次

835
00:42:18,880 --> 00:42:22,000
think of it's like a key value store that keeps track
把它想象成一个跟踪数据库

836
00:42:22,010 --> 00:42:23,720
of again the database schema. 
模式的键值存储

837
00:42:24,030 --> 00:42:25,540
Google has its data catalogue, 
谷歌有自己的数据目录

838
00:42:25,550 --> 00:42:27,300
amazon got its blue cattle data club. 
亚马逊有自己的蓝牛数据俱乐部

839
00:42:27,750 --> 00:42:29,060
They're all doing basically the same thing, 
他们基本上都在做同样的事情

840
00:42:29,070 --> 00:42:31,020
but h catalog is actually the only one. 
但H目录实际上是唯一的一个

841
00:42:33,140 --> 00:42:34,060
The next is the query optimizers. 
接下来是查询优化器

842
00:42:34,750 --> 00:42:38,720
So we'll spend a several classes talking about query optimization, 
所以我们会花几节课来讨论查询优化

843
00:42:38,730 --> 00:42:43,560
but think of these are just generic frame works where you can define
但这些只是通用的框架工作

844
00:42:43,570 --> 00:42:47,520
the heuristic or the cost based search rules to do career acquisition, 
你可以定义启发式或基于成本的搜索规则来进行职业获取

845
00:42:47,530 --> 00:42:51,440
how to take a some of them actually keep the parts the sql query for you, 
如何利用其中的一些来为你保留SQL查询的部分

846
00:42:51,450 --> 00:42:55,220
but then it convert it into a a logical plan through transformation rules
但随后它通过转换规则将其转换为逻辑计划

847
00:42:55,230 --> 00:42:58,300
and then run a cost based search to generate a physical plan. 
然后运行基于成本的搜索以生成物理计划

848
00:43:01,450 --> 00:43:02,760
The hardest part of building data system
构建数据系统最困难的部分

849
00:43:02,770 --> 00:43:05,170
because there's all these corner cases and sequel is
因为存在所有这些极端情况

850
00:43:05,180 --> 00:43:07,010
all these optimization rules they can apply. 
结果是它们可以应用的所有这些优化规则

851
00:43:07,810 --> 00:43:11,920
Deal with correlated sub queries like it gets very messy, very quick.
处理相关子查询 就像它变得非常混乱 非常快

852
00:43:13,300 --> 00:43:16,380
And the ideas that rather than everybody building
而不是每个人都建立自己的第一个蹩脚的

853
00:43:16,390 --> 00:43:17,780
their own first crappy heuristic optimize, 
启发式优化

854
00:43:17,790 --> 00:43:18,820
which everyone does. 
每个人都这样做

855
00:43:19,410 --> 00:43:19,760
In theory, 
从理论上讲

856
00:43:19,770 --> 00:43:24,110
you could have a cost based search that was pretty powerful out of the box. 
你可以有一个基于成本的搜索 这是非常强大的开箱即用

857
00:43:25,390 --> 00:43:27,700
The two nodal implications are apache, 
两个节点的含义是阿帕奇

858
00:43:27,710 --> 00:43:29,140
calcite and green lump orca. 
方解石和绿色块状逆戟鲸

859
00:43:29,670 --> 00:43:34,170
Calcite is came out of lucy db lucy db a was ai think, 
方解石是从露西db出来的

860
00:43:34,180 --> 00:43:36,030
commercial system, 
露西db是人工智能认为

861
00:43:36,040 --> 00:43:39,550
late 2000s that failed. 
商业系统 2000年代末失败了

862
00:43:39,560 --> 00:43:40,670
But for whatever reason, 
但不知出于什么原因

863
00:43:41,360 --> 00:43:43,550
calcite was pulled out and was reused, 
方解石被提取出来并被重用

864
00:43:44,290 --> 00:43:46,560
getting as a standalone query optimizer, which is pretty cool.
作为一个独立的查询优化器 这非常酷

865
00:43:46,570 --> 00:43:50,200
And then the green thumb guys built this thing called orca. 
然后绿拇指的人建造了这个叫做奥卡的东西

866
00:43:50,400 --> 00:43:54,450
It was designed to be the query optimizer for both green plum, 
它被设计为green plum的查询优化器

867
00:43:54,770 --> 00:43:57,200
which is a olap system, as we talked about last class,
green plum是一个实验室系统

868
00:43:58,030 --> 00:44:00,990
that vm ware now also vm ware owns. 
正如我们上一节课所讨论的 vm ware现在也拥有该系统

869
00:44:02,600 --> 00:44:04,190
But they also built for this other thing called hawk, 
但他们也建造了另一个叫做霍克的东西

870
00:44:04,200 --> 00:44:07,600
which is like their version of sql on top of head do. 
就像他们在头顶上做的续集

871
00:44:07,610 --> 00:44:10,120
And so they try to build a single optimized framework that could be used
因此

872
00:44:10,130 --> 00:44:10,760
in both systems. 
他们试图建立一个可以在两个系统中使用的单一优化框架

873
00:44:10,770 --> 00:44:12,560
I hawke is still around. 
我霍克还在

874
00:44:12,570 --> 00:44:13,960
It's not that people actually use it, 
不是人们真的用它

875
00:44:13,970 --> 00:44:16,030
but green plum uses it as well. 
而是青梅也用它

876
00:44:17,390 --> 00:44:20,860
The main thing is to think about this green plum is less known than calcite. 
主要是考虑到这个青梅比方解石更不为人所知

877
00:44:20,870 --> 00:44:22,260
It's written in c plus. 
它是用C+写的

878
00:44:23,050 --> 00:44:25,360
Calcite is written in java, 
方解石是用Java编写的

879
00:44:25,370 --> 00:44:28,720
and that's more use that there's more systems using calcite. 
它的用途比使用方解石的系统更多

880
00:44:30,130 --> 00:44:32,320
I don't know whether which one is more powerful or not like. 
我不知道哪一个更强大或不喜欢

881
00:44:34,200 --> 00:44:36,470
They both look roughly equivalent. 
它们看起来大致相当

882
00:44:38,460 --> 00:44:40,450
Then there's also these source file formats. 
然后还有这些源文件格式

883
00:44:40,460 --> 00:44:47,180
And actually a this is a pretty novel idea that is definitely
实际上 这是一个非常新颖的想法

884
00:44:48,690 --> 00:44:51,400
made possible by the amount modern cloud systems. 
现代云系统的数量使之成为可能

885
00:44:52,050 --> 00:44:56,120
So historically database systems always use their own proprietary data format, 
从历史上看 数据库系统总是使用自己专有的数据格式

886
00:44:56,410 --> 00:44:56,920
right? 
对吗

887
00:44:56,930 --> 00:44:59,800
sql like , postgres, mysql, whatever you name it.
寻找光明 波斯特格雷斯 我的续集 不管你叫它什么

888
00:45:00,110 --> 00:45:01,660
Then when they write files to desk, 
然后 当他们将文件写入桌面时

889
00:45:01,670 --> 00:45:03,260
that's in their proprietary binary format, 
这是他们专有的二进制格式

890
00:45:03,270 --> 00:45:08,820
like you can't take up progress data files and read them in my single oracle. 
就像您不能在我的单个oracle中使用进度数据文件并读取它们一样

891
00:45:10,090 --> 00:45:12,420
Duck tv can read single light files, but it's designed to do that,
duck tv可以读取单灯文件

892
00:45:12,430 --> 00:45:13,380
but you get my point, 
但它的设计就是为了做到这一点

893
00:45:13,840 --> 00:45:16,960
the only way to really transfer data or be able to take data
但你明白我的意思 真正传输数据或能够从一天的

894
00:45:16,970 --> 00:45:20,960
from one day system and reuse it or do computations on another data system
系统中获取数据并重复使用它或在另一天的系统中进行计算的

895
00:45:20,970 --> 00:45:23,800
is if you dump it out as csv or json file, 
唯一方法是将其作为acsb或jason文件转储

896
00:45:23,810 --> 00:45:24,120
right? 
对吗

897
00:45:27,080 --> 00:45:29,510
Starting in the last decade, 
在过去的十年年里

898
00:45:29,520 --> 00:45:30,910
there was this moment to say, okay,
有这样一个时刻说 好吧

899
00:45:31,040 --> 00:45:33,780
let's actually build these binary file formats. 
让我们实际构建这些二进制文件格式

900
00:45:33,790 --> 00:45:34,900
So we don't have to dump. 
这样我们就不用倾倒了

901
00:45:34,910 --> 00:45:36,860
It is like text based formats like jason. 
它类似于基于文本的格式 如Jason

902
00:45:38,060 --> 00:45:40,810
Then we can build libraries for these different database systems that
然后我们可以为这些不同的数据库系统构建库

903
00:45:40,820 --> 00:45:42,570
can access this stuff very efficiently. 
这些库可以非常高效地访问这些东西

904
00:45:43,010 --> 00:45:43,840
These libraries potentially, 
对于这些库

905
00:45:43,850 --> 00:45:47,400
suppose you're an iterator to retreat batches of columns in these files. 
假设您是一个迭代器 可以对这些文件中的列进行批处理

906
00:45:47,410 --> 00:45:51,660
You can do some select the scans for like pick which columns you actually want. 
你可以做一些选择扫描 比如选择你真正想要的列

907
00:45:53,120 --> 00:45:55,710
And some of them, do they do all the depression stuff for you?
他们中的一些人 他们为你做了所有抑郁的事情吗

908
00:45:55,930 --> 00:45:58,200
But again, they're just like this.
但是 他们就像这样

909
00:45:58,210 --> 00:46:00,240
The lowest level of the query plan is the access methods. 
查询计划的最低级别是访问方法

910
00:46:01,580 --> 00:46:06,530
So these are probably the six most wide users are famous ones. 
所以这可能是六个最广泛的用户是著名的

911
00:46:07,720 --> 00:46:09,390
In 2013, there was parquet,
2013年

912
00:46:09,400 --> 00:46:10,990
org and carbon data came out. 
有了parquet 有了org 碳数据出来了

913
00:46:11,240 --> 00:46:13,380
Parquet is probably the most widely used one. 
拼花地板可能是使用最广泛的一种

914
00:46:13,390 --> 00:46:17,170
This is a compressed column on my format from claudia and twitter. 
这是来自Claudia和Twitter的关于我的格式的压缩专栏

915
00:46:20,110 --> 00:46:22,420
It has more aggressive the compression than work, 
它具有比工作更积极的压缩

916
00:46:24,780 --> 00:46:27,410
whether or not it's fast or not depends on what the data looks like, 
它的速度快不快取决于数据的样子

917
00:46:27,420 --> 00:46:29,130
depends on how you actually access it. 
取决于你实际访问它的方式

918
00:46:29,140 --> 00:46:31,250
So I can't say one's better than another, depends on a lot of things.
所以我不能说一个比另一个好 这取决于很多事情

919
00:46:31,260 --> 00:46:32,950
But at a high level, 
但是在高水平上

920
00:46:34,200 --> 00:46:37,890
they're basically equivalent or came out of facebook high project. 
它们基本上是等同的 或者来自Facebook的高项目

921
00:46:38,770 --> 00:46:40,360
Carbon data came out of hawaii. 
碳数据来自夏威夷

922
00:46:40,370 --> 00:46:42,320
And as I said, I think in the inter class,
正如我所说的 我认为在课堂上

923
00:46:42,370 --> 00:46:45,120
i'm working with a former student of mine. 
我和我以前的一个学生一起工作

924
00:46:45,490 --> 00:46:49,450
They actually tried using carbon data and all the open source libraries
他们实际上尝试使用碳数据

925
00:46:49,460 --> 00:46:49,890
don't compile, 
而所有的开源库都不能编译

926
00:46:49,900 --> 00:46:50,330
don't work. 
不能工作

927
00:46:50,340 --> 00:46:53,640
So I don't know if anybody else at hawaii that uses the carbon data. 
所以我不知道夏威夷是否还有其他人使用碳数据

928
00:46:54,580 --> 00:46:58,650
Iceberg is a newer one at a netflix and we'll see it
冰山是一个较新的网络电影

929
00:46:58,660 --> 00:47:02,490
over this next class where it's a way to do support updates
我们将在下一节课上看到它 它是一种在拼花文件上进行支持更新和方案演变的方法

930
00:47:02,500 --> 00:47:05,610
and scheme evolutions over parquet files so that you can do, 
这样你就可以做

931
00:47:05,620 --> 00:47:08,490
they have a delta store, you can do updates and then apply them,
他们有一个增量存储 你可以做更新

932
00:47:09,160 --> 00:47:12,270
do the party files plus read the historical data. 
然后应用它们 做聚会文件 再加上读取历史数据

933
00:47:12,670 --> 00:47:16,980
85 is a much older data format from the scientific community. 
85是来自科学界的一种更古老的数据格式

934
00:47:17,540 --> 00:47:18,930
Come from late 90s. 
来自90年代末

935
00:47:18,940 --> 00:47:20,680
This is for multidimensional data. 
这适用于多维数据

936
00:47:22,700 --> 00:47:23,210
Again, 
同样

937
00:47:23,220 --> 00:47:29,090
I don't know of any sort of commercial database system that uses ht five, 
我不知道有任何类型的商业数据库系统使用HTF5

938
00:47:29,100 --> 00:47:30,010
htf five. 
HTF5

939
00:47:30,020 --> 00:47:32,130
It's primarily used for, as I said,
正如我所说它

940
00:47:32,140 --> 00:47:35,960
for scientific calculations, scientific instruments,
主要用于科学计算 科学仪器

941
00:47:35,970 --> 00:47:42,720
and then apache arrow a is a memory kilometer format that's designed to be
然后apache arrow a是一种内存千米格式

942
00:47:42,730 --> 00:47:43,960
the exchange format for
它被设计为一种交换格式

943
00:47:43,970 --> 00:47:49,050
sort of sharing data between processes and over the network very efficiently. 
用于在进程之间和网络上非常有效地共享数据

944
00:47:49,660 --> 00:47:50,130
Again, 
再一次

945
00:47:50,980 --> 00:47:52,470
think of like it looks something like parquet, 
想象它看起来像镶木地板

946
00:47:52,480 --> 00:47:53,910
but it's for enemy data. 
但它是用于敌人的数据

947
00:47:54,980 --> 00:47:55,970
For our class, this class,
对于我们这门课

948
00:47:55,980 --> 00:47:59,210
we're mostly going to be focusing on things that look like parquet and orcanero. 
这门课 我们主要关注的是那些看起来像镶木地板和奥卡内罗的东西

949
00:48:00,340 --> 00:48:02,740
And then for project project one, 
然后对于项目一 我们将在下一节课讨论

950
00:48:02,750 --> 00:48:03,890
which we'll discuss next class, 
将建立一个外部数据的工作

951
00:48:03,900 --> 00:48:07,300
will be building a foreign data effort to parts of it, 
它的一部分 看起来像一个停车文件

952
00:48:07,310 --> 00:48:09,420
looks like a parking file without exactly. 
没有确切的

953
00:48:09,430 --> 00:48:09,980
Ok. 
好吧

954
00:48:11,540 --> 00:48:15,470
The last one that's super interesting is the standalone biopsy, 
最后一个非常有趣的是独立的活检

955
00:48:15,480 --> 00:48:16,630
query execution. 
查询执行

956
00:48:17,170 --> 00:48:19,120
This is kind of what the whole point of this class is. 
这就是这门课的意义所在

957
00:48:19,130 --> 00:48:20,600
As I showed in the very beginning, 
正如我在一开始所展示的

958
00:48:20,610 --> 00:48:23,360
we spend a lot of time talking how to build the execution engine, 
我们花了很多时间讨论如何构建执行引擎

959
00:48:23,490 --> 00:48:24,780
how to do vectors execution, 
如何执行向量

960
00:48:24,790 --> 00:48:26,400
how to do code compilation, 
如何编译代码

961
00:48:26,410 --> 00:48:27,480
how to do joints. 
如何连接

962
00:48:27,820 --> 00:48:32,930
That's the it's not the hardest part that he's not always the query optimizer, 
这并不是最难的部分 他并不总是查询优化器

963
00:48:32,940 --> 00:48:34,170
but this is certain the part that, 
但这是肯定的部分

964
00:48:34,180 --> 00:48:39,700
like can have a huge impact on performance. 
就像可以对性能产生巨大的影响

965
00:48:40,050 --> 00:48:41,870
Obviously, degenerate credit for your plan.
很明显 你的计划信誉下降了

966
00:48:43,040 --> 00:48:43,670
You're screwed anyway, 
无论如何

967
00:48:43,680 --> 00:48:47,510
but like having this during this rate can make a big difference. 
你都完蛋了 但在这个速度下 这会有很大的不同

968
00:48:47,520 --> 00:48:52,580
So there's now work being done to build standalone query engines that are vectors. 
因此 现在正在进行的工作是构建独立的查询引擎 这些引擎是向量

969
00:48:52,590 --> 00:48:54,780
It is all the modern techniques that we talked about before. 
就是我们前面讲的所有的现代技术

970
00:48:55,890 --> 00:48:58,160
Again, input just a dag of physical operators.
同样 只输入物理操作符的DAG

971
00:48:58,170 --> 00:49:01,080
They've required something else to do the scheduling and do access, 
他们需要其他的东西来做调度和访问

972
00:49:01,090 --> 00:49:02,160
to get the data, 
获取数据

973
00:49:02,170 --> 00:49:03,520
do illustration and do data around. 
做说明和做数据

974
00:49:03,890 --> 00:49:04,760
But the end of the day, 
但最终

975
00:49:05,530 --> 00:49:08,040
it's like the kernels are essentially the query operators. 
内核本质上是查询操作符

976
00:49:08,670 --> 00:49:11,340
The mixed famous one of this is going to be velox, 
其中最著名的是Velox

977
00:49:11,910 --> 00:49:16,450
which we'll spend time talking about at the end of the semester. 
我们将在学期末花时间讨论它

978
00:49:16,760 --> 00:49:18,810
But this is actually very fascinating now, 
但现在这实际上非常吸引人

979
00:49:18,820 --> 00:49:26,700
because vectors query execution is sort of what made snowflake so good, 
因为向量查询执行是使雪花如此出色

980
00:49:26,710 --> 00:49:27,780
so unique. 
如此独特的原因

981
00:49:28,110 --> 00:49:30,120
At least it was the starting point of making it so unique. 
至少这是让它变得如此独特的起点

982
00:49:30,570 --> 00:49:31,470
10 years ago, 
十年前

983
00:49:31,980 --> 00:49:32,290
right? 
对吧

984
00:49:32,300 --> 00:49:33,250
That they were building. 
他们正在建造的

985
00:49:33,260 --> 00:49:34,450
One of the first effect rises, 
第一个效果上升 数据

986
00:49:35,310 --> 00:49:36,230
data, use engines.
使用引擎

987
00:49:37,870 --> 00:49:38,840
Now it's so complex. 
现在它变得如此复杂

988
00:49:39,170 --> 00:49:40,880
Everybody has vector x execution for those words. 
每个人都有这些单词的向量X执行

989
00:49:40,890 --> 00:49:42,700
But can you just go? 
但你能走吗

990
00:49:42,710 --> 00:49:43,740
This was a big deal, 
这是一件大事

991
00:49:43,750 --> 00:49:46,460
but now facebook is building velox. 
但现在Facebook正在打造Velox

992
00:49:46,470 --> 00:49:49,700
There's data fusion from the arrow and influx tv guys. 
有来自Arrow和Influence TV的数据融合

993
00:49:49,920 --> 00:49:52,920
Intel is so ethnic. 
英特尔太有民族特色了

994
00:49:54,820 --> 00:49:59,200
So now this is super interesting because no, 
所以现在这非常有趣 因为不

995
00:50:00,300 --> 00:50:04,650
facebook is gonna spend a lot of time building up this execution engine
facebook将花费大量时间来构建这个执行引擎

996
00:50:04,660 --> 00:50:08,130
that anybody could come along and build a wrap around and make the
任何人都可以来构建一个包裹 并制作下一个雪花

997
00:50:08,140 --> 00:50:10,610
next snowflake and not have to worry about writing the execution engine. 
而不必担心编写执行引擎

998
00:50:12,150 --> 00:50:12,500
Essentially, 
从本质上讲

999
00:50:12,510 --> 00:50:14,900
this becomes a commodity like a vector execution engine using
这变成了一种商品

1000
00:50:14,910 --> 00:50:16,220
the modern techniques of today. 
就像使用当今现代技术的向量执行引擎一样

1001
00:50:16,770 --> 00:50:19,320
It does not differentiate one system for the next. 
它不区分一个系统与下一个系统

1002
00:50:19,330 --> 00:50:24,380
It's now gonna be the communication between the different nodes, 
现在是不同节点之间的通信

1003
00:50:24,390 --> 00:50:26,060
sort of the yellow books that we talked before. 
有点像我们之前讨论的黄皮书

1004
00:50:26,470 --> 00:50:27,750
And then all the query optimizers. 
然后是所有的查询优化器

1005
00:50:28,820 --> 00:50:29,260
Yeah, we're good.
是的 我们很好

1006
00:50:29,550 --> 00:50:30,220
All right. 
好吧

1007
00:50:30,550 --> 00:50:31,820
My boys in there. 
我的孩子们在里面

1008
00:50:31,830 --> 00:50:33,650
I got teaching class. 
我要去上课了

1009
00:50:34,660 --> 00:50:35,410
I'm teaching class. 
我在上课

1010
00:50:35,420 --> 00:50:36,090
I wrote like, 
我写的是

1011
00:50:38,060 --> 00:50:39,290
I is like this other guy. 
我就像另一个人

1012
00:50:39,700 --> 00:50:40,330
He's almost done. 
他快好了

1013
00:50:41,440 --> 00:50:41,950
He's coming down. 
他下来了

1014
00:50:42,440 --> 00:50:45,390
We got from the fbi no, 
我们从联邦调查局得到的不

1015
00:50:46,610 --> 00:50:47,960
it's like, okay.
就像 好吧

1016
00:50:48,230 --> 00:50:51,080
Like, thanks.
就像 谢谢

1017
00:50:54,430 --> 00:50:56,220
Anyway, so this is super fascinating.
总之 这是超级迷人的

1018
00:50:56,570 --> 00:50:57,710
We spent a lot of time with us. 
我们花了很多时间陪我们

1019
00:50:57,720 --> 00:50:57,990
All right. 
好吧

1020
00:50:58,000 --> 00:50:58,270
So, 
那么

1021
00:51:02,780 --> 00:51:03,010
right. 
对的

1022
00:51:03,020 --> 00:51:06,220
So the this was supposed to be the high level approach, 
所以这应该是一种高级方法

1023
00:51:06,230 --> 00:51:09,020
a high level overview of what the query optimizers look like. 
对查询优化器的高级概述

1024
00:51:10,330 --> 00:51:11,490
Again, as I said,
同样 正如我所说的

1025
00:51:11,860 --> 00:51:15,530
the way you on a single node itself, 
你在单个节点上构建这些系统的

1026
00:51:15,540 --> 00:51:18,610
you would build these systems is not where each worker knows
方式并不是每个工人都知道如何

1027
00:51:18,620 --> 00:51:20,450
not different how you build a single data system. 
构建单个数据系统

1028
00:51:20,460 --> 00:51:22,970
It's gonna be that data movement stuff that we talked before. 
这将是我们之前讨论的数据移动问题

1029
00:51:22,980 --> 00:51:23,890
And also, again,
同样 假设

1030
00:51:23,900 --> 00:51:26,770
assuming that we're coming from retrieving data from a cloud based
我们从基于云的专家共享磁盘

1031
00:51:26,780 --> 00:51:27,940
expert shared disk. 
中检索数据

1032
00:51:27,950 --> 00:51:30,700
So the rest of the special we focus on how to build
因此 在特别节目的其余部分

1033
00:51:31,280 --> 00:51:35,720
these different components that we talked about and make them using
我们将重点关注如何构建我们讨论过的这些不同的组件

1034
00:51:35,730 --> 00:51:36,760
the modern techniques. 
并使用现代技术来制作它们

1035
00:51:36,770 --> 00:51:37,280
Right? 
对的

1036
00:51:38,790 --> 00:51:40,540
Next class, we'll talk about storage models.
下节课 我们将讨论存储模型

1037
00:51:40,550 --> 00:51:44,340
We'll talk about how to represent data within the column themselves, 
我们将讨论如何在列本身中表示数据

1038
00:51:44,350 --> 00:51:45,060
the bytes. 
即字节

1039
00:51:45,260 --> 00:51:48,930
I do table positioning and then in system catalogs as well. 
我做表格定位 然后在系统目录中也做

1040
00:51:48,940 --> 00:51:51,250
So hopefully I got to deal with these guys, 
所以希望我能处理这些家伙

1041
00:51:51,260 --> 00:51:52,610
forgot what's going on with him. 
忘记他发生了什么事

1042
00:51:53,860 --> 00:51:55,270
Hopefully they'll be done soon. 
希望他们很快就能完成

1043
00:51:56,710 --> 00:52:01,940
And then then i'll film the next class and post that on youtube as well. 
然后我会拍摄下一节课 并将其发布到YouTube上

1044
00:52:01,950 --> 00:52:03,700
Later on the next class, 
在下一节课上

1045
00:52:03,710 --> 00:52:05,020
we'll talk about project one. 
我们将讨论项目一

1046
00:52:05,510 --> 00:52:07,000
Hi, guys have a good night.
嗨 伙计们 晚安

1047
00:52:07,010 --> 00:52:09,080
See, you.
看 你

1048
00:52:09,090 --> 00:52:09,920
That's my favorite. 
那是我的最爱

1049
00:52:09,930 --> 00:52:10,520
All that. 
所有这些

1050
00:52:13,220 --> 00:52:13,690
What is it? 
那是什么

1051
00:52:14,360 --> 00:52:19,990
It's the st cricket idesi make a mess unless I can do it
这是st cricket idesi弄得一团糟

1052
00:52:20,000 --> 00:52:22,340
like ago ice cube with the
除非我能像ago ice cube那样做

1053
00:52:22,350 --> 00:52:24,740
g to the e to the t it comes. 
从g到e到t 它来了

1054
00:52:24,750 --> 00:52:25,980
Do I play the game? 
我玩这个游戏吗

1055
00:52:25,990 --> 00:52:26,940
Where's no roof? 
哪里没有屋顶

1056
00:52:27,150 --> 00:52:30,620
Tommy saw the custody of a group of sun treat group with the buzzing cap
汤米在过道上看到了一群戴着嗡嗡作响的帽子的

1057
00:52:30,630 --> 00:52:31,340
on the aisle. 
日光族

1058
00:52:31,350 --> 00:52:35,260
For sure, we going to go with a blow to the aisle come on.
当然 我们要在走道上打一拳 来吧

1059
00:52:35,850 --> 00:52:36,560
What is it? 
那是什么

1060
00:52:36,570 --> 00:52:38,440
That's me rolling with that? 
那就是我在玩那个

1061
00:52:38,450 --> 00:52:42,800
What's up pork and sap it to see and then ask what a party. 
波克和萨普看到了什么 然后问什么是聚会

1062
00:52:43,300 --> 00:52:45,930
By the 12 pack case on the 46 pack, 
通过46包上的12包箱子

1063
00:52:45,940 --> 00:52:47,930
48 gets the real price. 
48得到了真实的价格

1064
00:52:48,210 --> 00:52:51,080
I drink fruit, but you are drinking Bob 12 hours.
我喝水果 但你喝鲍勃12小时

1065
00:52:51,090 --> 00:52:53,080
They say bill makes you fat, 
他们说比尔让你变胖

1066
00:52:53,090 --> 00:52:54,520
but saying eyes are straight, 
但他们说眼睛是直的

1067
00:52:54,530 --> 00:52:56,120
so it really don't matter. 
所以这真的不重要