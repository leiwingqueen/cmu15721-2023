1
00:00:04,650 --> 00:00:08,080
Hey, yo, yo pack the chrome stuff by like missus jones,

2
00:00:08,090 --> 00:00:09,200
liverpool mathematics. 

3
00:00:09,210 --> 00:00:10,720
We have the devil spoken stone. 

4
00:00:10,890 --> 00:00:12,560
Up one heads, the bed make shots.

5
00:00:12,570 --> 00:00:17,730
But today we talk about a lot of accents which is sort of as I said, 

6
00:00:17,740 --> 00:00:22,690
the spoiler is going to be that some of these a lot of techniques were described. 

7
00:00:22,700 --> 00:00:25,690
The modern columnar databases aren't going to support us. 

8
00:00:26,100 --> 00:00:31,730
But we'll mostly see the bit map indexes will be mostly used in row store

9
00:00:31,740 --> 00:00:35,580
in database systems that then would have sort of accelerated commoner

10
00:00:35,590 --> 00:00:35,940
access. 

11
00:00:37,560 --> 00:00:39,950
But let's go through a lot of different things. 

12
00:00:39,960 --> 00:00:43,140
So where we left off, 

13
00:00:44,660 --> 00:00:50,220
last class was we were discussing the pros and cons of the sort

14
00:00:50,230 --> 00:00:52,930
of the columnar storage versus row based storage. 

15
00:00:53,470 --> 00:00:57,360
And I talked about how most modern olap systems

16
00:00:57,370 --> 00:00:59,520
are going to be implementing some variant of packs, 

17
00:01:00,320 --> 00:01:03,030
because that's me better for doing sequential access potential scans. 

18
00:01:04,020 --> 00:01:05,210
On a large amounts of data. 

19
00:01:05,670 --> 00:01:08,770
There'll be a bunch of optimization that we can apply to make these run faster. 

20
00:01:09,890 --> 00:01:11,100
The keeping remember, 

21
00:01:11,110 --> 00:01:15,410
two is also all the attributes in a columnar database must be fixed length. 

22
00:01:15,830 --> 00:01:18,990
Because the way we're going to do addressing is through offset arithmetic. 

23
00:01:19,690 --> 00:01:20,720
We're scaling one column. 

24
00:01:20,730 --> 00:01:25,110
If we know what the 1/100 offset within that column, 

25
00:01:25,410 --> 00:01:27,660
we know how to do the simple math to jump to the hunter, 

26
00:01:28,200 --> 00:01:31,560
offset at the other columns to go stitch two tuples back together. 

27
00:01:31,570 --> 00:01:36,600
The other big thing there is that we're going to not always, 

28
00:01:36,610 --> 00:01:40,680
but if we assume that or the data system assumes that the data files are immutable, 

29
00:01:41,160 --> 00:01:42,950
meaning to write once, read many,

30
00:01:42,960 --> 00:01:45,830
like I write it once and I never can go back and make inline updates to it, 

31
00:01:46,340 --> 00:01:50,140
is going to open up a bunch of different optimization that we'll see today, 

32
00:01:50,660 --> 00:01:55,620
where if we had to support incremental updates or delete or insertion, 

33
00:01:56,150 --> 00:01:59,620
then some of these things just won't work or to be too expensive to maintain. 

34
00:02:00,180 --> 00:02:03,050
This is another big optimization or assumption that the modern services

35
00:02:03,060 --> 00:02:03,490
are making. 

36
00:02:06,510 --> 00:02:10,890
So the first thing we sort of motivate what we're talking about today is

37
00:02:10,900 --> 00:02:12,490
when we talk about olap indexes, 

38
00:02:12,980 --> 00:02:16,370
we have to sort of get all the things we learned in introduction class. 

39
00:02:17,160 --> 00:02:18,280
Spout indexes, 

40
00:02:19,030 --> 00:02:20,890
like, in particular, for b+ trees.

41
00:02:20,900 --> 00:02:22,280
Or if you remember, 

42
00:02:22,290 --> 00:02:26,450
radix trees are tries these tree based data structures, even hash tables,

43
00:02:27,200 --> 00:02:30,090
because in the old tv world, 

44
00:02:30,100 --> 00:02:32,290
which is what their inter class sort of focuses on without

45
00:02:32,300 --> 00:02:33,290
explicitly saying it. 

46
00:02:35,090 --> 00:02:38,480
These indexes are designed for doing finding individual tuples

47
00:02:38,490 --> 00:02:39,680
or small number of tuples. 

48
00:02:40,240 --> 00:02:42,310
With very selective predicates, 

49
00:02:42,320 --> 00:02:46,140
I go get andy's account or go get andy's orders, 

50
00:02:46,770 --> 00:02:47,140
right? 

51
00:02:47,800 --> 00:02:52,040
And in the oltp world also as well, 

52
00:02:52,390 --> 00:02:54,440
we're not going to assume that the files are immutable. 

53
00:02:55,450 --> 00:02:57,360
Therefore, these index data structures,

54
00:02:57,370 --> 00:03:05,270
the b plus trees going to have to store some extra room

55
00:03:05,280 --> 00:03:08,690
in their nodes to accommodate updates later on. 

56
00:03:09,400 --> 00:03:10,110
In the leaf nodes. 

57
00:03:10,120 --> 00:03:13,520
On average, the leaf nodes are actually any node in b plus tree.

58
00:03:13,990 --> 00:03:15,300
In a real world database system. 

59
00:03:15,310 --> 00:03:17,020
On average, it's about 70% full.

60
00:03:18,170 --> 00:03:18,250
Right? 

61
00:03:18,260 --> 00:03:20,010
So that extra 30% is weighted space, 

62
00:03:20,020 --> 00:03:22,810
because they want to be able to amortize the cost of someone inserting

63
00:03:23,310 --> 00:03:28,070
new data and not have to do a split for every single operation. 

64
00:03:28,410 --> 00:03:28,710
Right? 

65
00:03:30,380 --> 00:03:34,040
In the ola world, we we don't need to support increment updates again,

66
00:03:34,050 --> 00:03:35,580
assuming that our files are read only. 

67
00:03:36,210 --> 00:03:39,740
And we typically don't need to find individual tuples, 

68
00:03:39,750 --> 00:03:42,920
at least the lower points of the query plant, 

69
00:03:42,930 --> 00:03:45,040
the access methods on the tables themselves. 

70
00:03:45,490 --> 00:03:46,440
When we do joins, 

71
00:03:46,570 --> 00:03:48,880
we have to find individual two tuples or values that match. 

72
00:03:49,400 --> 00:03:50,950
But if you're just doing scans, 

73
00:03:51,640 --> 00:03:53,430
you're not going to look at andy's record. 

74
00:03:54,020 --> 00:03:55,340
Or in these orders, 

75
00:03:55,350 --> 00:03:58,600
you're going to look at all the orders from some demographic, 

76
00:03:58,610 --> 00:03:59,280
like some people, 

77
00:03:59,290 --> 00:04:00,600
like everybody lives in pittsburgh. 

78
00:04:03,520 --> 00:04:06,030
The data structure that we would use to make oltp workloads run

79
00:04:06,040 --> 00:04:08,600
really fast are not going to be what we want to use

80
00:04:08,610 --> 00:04:11,290
for for analytical queries. 

81
00:04:12,590 --> 00:04:18,300
So the question is, what can we actually do to speed up sequential scans?

82
00:04:20,950 --> 00:04:21,690
You have to do clicker. 

83
00:04:23,830 --> 00:04:24,110
Right? 

84
00:04:25,600 --> 00:04:27,190
Here's a big list of optimization of things you can do. 

85
00:04:27,710 --> 00:04:29,230
Some of these things were to cover the semester, 

86
00:04:29,240 --> 00:04:30,250
some of the things we won't. 

87
00:04:31,260 --> 00:04:34,430
So the first thing you do is obviously data prefetching, right?

88
00:04:34,440 --> 00:04:37,390
Instead of stalling, every single time i've got to go get new data.

89
00:04:37,710 --> 00:04:39,130
If I know i'm going to sequential scan, 

90
00:04:39,140 --> 00:04:42,000
you scan a bunch of files or a bunch of pages that are contagious. 

91
00:04:42,460 --> 00:04:44,100
Let me go ahead and why i'm scanning one, 

92
00:04:44,110 --> 00:04:47,230
start bringing in the next one off a disk into memory. 

93
00:04:48,390 --> 00:04:48,920
Then I can, 

94
00:04:49,960 --> 00:04:50,910
as i'm running my query, 

95
00:04:51,260 --> 00:04:55,090
instead of having just one thread be responsible for scanning terabyte of data, 

96
00:04:55,370 --> 00:04:56,690
I could have multiple threads do this. 

97
00:04:58,730 --> 00:04:59,800
Run the task in parallel. 

98
00:05:00,730 --> 00:05:01,970
I'll get some advantages also, too.

99
00:05:01,980 --> 00:05:04,050
If I can pre sort or cluster the data ahead of time, 

100
00:05:04,060 --> 00:05:07,320
because now I know that if i'm looking for things in a certain range, 

101
00:05:07,600 --> 00:05:12,460
that once I get past a a certain set of values, 

102
00:05:12,470 --> 00:05:15,540
I know that the thing i'm looking for is less than that, 

103
00:05:15,550 --> 00:05:16,780
what the value is looking at right now. 

104
00:05:16,790 --> 00:05:18,380
I know I don't need to look at anything else. 

105
00:05:19,840 --> 00:05:20,520
Late materialization. 

106
00:05:20,530 --> 00:05:23,080
We talked about last class where and we'll talk about more of this later

107
00:05:23,090 --> 00:05:23,640
in the semester, 

108
00:05:24,030 --> 00:05:26,020
where if I have a column stored, 

109
00:05:26,620 --> 00:05:28,900
instead of having to pass around the entire triple from one upper to the next, 

110
00:05:29,570 --> 00:05:30,720
we just pass around the offsets, 

111
00:05:31,350 --> 00:05:34,810
or just the columns of that I needed at that point of the operator. 

112
00:05:36,020 --> 00:05:40,520
I'm copying moving list data around materialize views, as I said,

113
00:05:40,730 --> 00:05:43,100
we're not going to discuss in this class, 

114
00:05:43,560 --> 00:05:46,060
but it's sort of a variation of result. 

115
00:05:46,070 --> 00:05:46,860
Caching is really simple. 

116
00:05:46,870 --> 00:05:48,680
Like, if I have the exact same query,

117
00:05:48,980 --> 00:05:50,510
like select star from table foo, 

118
00:05:50,520 --> 00:05:51,670
where id equals four, 

119
00:05:52,000 --> 00:05:53,500
I can take that result and cache it. 

120
00:05:53,750 --> 00:05:55,750
And if the exact same query shows up, I can reuse it.

121
00:05:56,370 --> 00:06:00,370
Materialize use are way more sophisticated because it's taking

122
00:06:00,380 --> 00:06:02,010
a more complex query, 

123
00:06:02,350 --> 00:06:06,550
be breaking it down into to smaller parts. 

124
00:06:08,100 --> 00:06:11,720
And then, as I make change to the database,

125
00:06:11,730 --> 00:06:14,000
I can incrementally update materialize view without having to rerun

126
00:06:14,010 --> 00:06:14,640
the entire query, 

127
00:06:14,650 --> 00:06:15,200
ideally. 

128
00:06:15,720 --> 00:06:19,230
But it's essentially a a it's a variation of result caching. 

129
00:06:19,240 --> 00:06:20,030
The idea is the same. 

130
00:06:21,670 --> 00:06:24,550
Data skipping is being able to figure out what data we actually don't

131
00:06:24,560 --> 00:06:29,360
even need to read ahead of time and just avoid having to do scans on that. 

132
00:06:30,940 --> 00:06:35,520
Data parallelization is where we can use vector rising instructions, 

133
00:06:35,530 --> 00:06:40,130
or like cindy to be able to apply different operations or do

134
00:06:40,140 --> 00:06:43,300
different steps in our sequential scan in

135
00:06:43,310 --> 00:06:46,910
parallel on a multiple pieces of data at the same time. 

136
00:06:48,070 --> 00:06:50,710
And then code specialization or query compilation, 

137
00:06:50,720 --> 00:06:51,670
just in time compilation, 

138
00:06:52,090 --> 00:06:56,310
is to generate machine code that does exactly what our scan is going to do, 

139
00:06:56,320 --> 00:06:57,990
or what our query is going to do. 

140
00:06:58,500 --> 00:07:00,540
Instead of how to use an interpreted system. 

141
00:07:01,780 --> 00:07:02,950
There's a lot of things here. 

142
00:07:03,470 --> 00:07:06,490
In this semester, we're going to cover these here.

143
00:07:06,830 --> 00:07:06,970
Right? 

144
00:07:06,980 --> 00:07:08,210
What is materialized views? 

145
00:07:08,660 --> 00:07:11,140
We've already discussed pre fetching an intro class, 

146
00:07:11,570 --> 00:07:12,590
clustering sorting. 

147
00:07:12,890 --> 00:07:14,810
It'll come up throughout the semester, but not going to say,

148
00:07:15,440 --> 00:07:20,580
I can teach you how to sort data we just use whenever the fastest one is

149
00:07:20,890 --> 00:07:21,270
this time. 

150
00:07:22,970 --> 00:07:24,930
Again, a bunch of these things would have covered throughout the semester,

151
00:07:25,520 --> 00:07:27,840
but today's class we're going to focus on data skipping. 

152
00:07:28,370 --> 00:07:29,280
You can think of like, again,

153
00:07:29,290 --> 00:07:32,520
all of these are things you can do to accelerate query execution. 

154
00:07:33,660 --> 00:07:36,450
So what we're still kind of focusing on the lower bounds of the system

155
00:07:36,460 --> 00:07:37,210
like the storage layer. 

156
00:07:37,220 --> 00:07:39,370
So that's why we're just going to focus on this for now. 

157
00:07:41,760 --> 00:07:44,060
At a high level, there's two things you can do to skip data.

158
00:07:45,700 --> 00:07:48,240
You can do a lossy approach or a lossless approach. 

159
00:07:49,150 --> 00:07:51,220
A loss approach would be approximate queries. 

160
00:07:52,460 --> 00:07:54,010
The way to think about this is say, 

161
00:07:54,020 --> 00:07:57,940
i'm doing an analytical query like select count star on the number

162
00:07:57,950 --> 00:08:02,880
of website visitors in most systems that doesn't do they don't do

163
00:08:02,890 --> 00:08:03,720
approximate queries. 

164
00:08:03,990 --> 00:08:05,070
You would get an exact answer, 

165
00:08:05,080 --> 00:08:09,060
would look at every single web visit and count all the entries. 

166
00:08:10,470 --> 00:08:11,950
But in many cases, 

167
00:08:11,960 --> 00:08:13,190
in many application domains, 

168
00:08:13,200 --> 00:08:14,950
you don't actually need the exact answer. 

169
00:08:15,750 --> 00:08:15,760
Right? 

170
00:08:15,770 --> 00:08:18,790
Do I really care that I need to know the exact number of visitors

171
00:08:18,800 --> 00:08:22,920
to my website to the exact person like999,000, 

172
00:08:22,930 --> 00:08:23,400
whatever, 

173
00:08:23,700 --> 00:08:25,810
or can I get something that's close enough? 

174
00:08:26,430 --> 00:08:28,150
And that's enough for my application. 

175
00:08:29,880 --> 00:08:32,150
This technique again is called approximate queries. 

176
00:08:33,960 --> 00:08:37,840
There are systems designed explicitly for doing nothing

177
00:08:37,850 --> 00:08:39,280
about approximate queries on top of it

178
00:08:39,290 --> 00:08:41,480
like a non approximate query database. 

179
00:08:41,490 --> 00:08:46,890
It was a project out of university of michigan called verdict db they

180
00:08:47,180 --> 00:08:48,170
with bars on safari. 

181
00:08:48,180 --> 00:08:51,680
He spun that off as a startup called kibo that basically sits in front

182
00:08:51,690 --> 00:08:53,760
of snowflake and does approximate queries in front of snowflake, 

183
00:08:53,770 --> 00:08:57,760
even though snowflake is not in its basic form, 

184
00:08:57,770 --> 00:08:59,000
an approximate query system. 

185
00:08:59,470 --> 00:09:01,530
But a bunch of these other systems blink. 

186
00:09:01,540 --> 00:09:02,690
Tv is a project kind of, 

187
00:09:04,360 --> 00:09:06,710
I was a research project out of berkeley and sort of did the same thing. 

188
00:09:06,720 --> 00:09:09,450
But these other systems here are like no flight bit query. 

189
00:09:09,670 --> 00:09:12,480
They have approximate query aggregates. 

190
00:09:12,490 --> 00:09:16,060
So you can call there's count star if there's the count thing that gives

191
00:09:16,070 --> 00:09:17,020
you the exact answer, 

192
00:09:17,310 --> 00:09:18,540
there's also an approximate count. 

193
00:09:19,470 --> 00:09:22,070
And it's basically using sampling and a way to give you

194
00:09:23,300 --> 00:09:25,970
some statistically bounded guarantee of the acts of the answer. 

195
00:09:27,110 --> 00:09:28,500
With proximal queries, you basically say,

196
00:09:28,830 --> 00:09:31,700
i'm all going to say i'm going to be a fraction of the data that

197
00:09:31,710 --> 00:09:32,500
I regularly have. 

198
00:09:34,630 --> 00:09:38,160
The other approach to do a lossless pruning or sorry, 

199
00:09:38,210 --> 00:09:39,840
data skipping is to do data pruning, 

200
00:09:40,310 --> 00:09:43,430
where we're going to rely on some auxiliary data structure

201
00:09:43,800 --> 00:09:46,380
that's been pre built on our data, 

202
00:09:46,890 --> 00:09:48,640
or be maintained by the database system. 

203
00:09:48,930 --> 00:09:51,840
That's going to allow it to identify portions of the database, 

204
00:09:51,850 --> 00:09:56,390
the portions of the data files for a table that's needed in a scan that it doesn't. 

205
00:09:56,680 --> 00:09:57,390
Does it need to read? 

206
00:09:58,840 --> 00:10:00,830
Again, ab plus tree is basically this, right?

207
00:10:00,840 --> 00:10:01,870
B plus tree says, 

208
00:10:02,280 --> 00:10:06,040
I can, if i'm looking up some value on a predicate, and I have an index,

209
00:10:06,050 --> 00:10:07,440
b bus tree on that predicate, 

210
00:10:07,450 --> 00:10:09,030
I can get down to the leaf node. 

211
00:10:09,410 --> 00:10:11,290
Find anything i'm looking for without having to scan the entire table. 

212
00:10:12,160 --> 00:10:13,100
It's basically the same idea, 

213
00:10:13,110 --> 00:10:16,990
but we're going to see a bunch of different ways that we can do in this class. 

214
00:10:19,490 --> 00:10:21,510
One of the tradeoffs we have to consider as we go along, 

215
00:10:21,520 --> 00:10:25,770
and you'll see this multiple times is the scope of this auxiliary data structure. 

216
00:10:25,780 --> 00:10:25,970
Like, 

217
00:10:26,620 --> 00:10:27,410
what is the purview? 

218
00:10:27,420 --> 00:10:32,350
How much data is it as actually covering in its it's the metadata. 

219
00:10:32,360 --> 00:10:34,170
It's generated for this auxiliary data structure

220
00:10:34,590 --> 00:10:38,190
versus how selective the kind of filter it could be. 

221
00:10:39,020 --> 00:10:39,500
Like think of this. 

222
00:10:39,510 --> 00:10:45,480
Like I if I have a always taken extremes of databases, 

223
00:10:45,490 --> 00:10:47,980
systems are always going to think about the scope of the problem. 

224
00:10:47,990 --> 00:10:50,170
Like on one extreme, 

225
00:10:50,180 --> 00:10:55,820
for the scope of one of these printing data structures would be the entire table. 

226
00:10:56,460 --> 00:10:57,650
I have 1 billion rows, 

227
00:10:57,660 --> 00:11:02,350
and maybe I keep the min and max value for for in a column, 

228
00:11:02,360 --> 00:11:03,110
in 1 billion rows. 

229
00:11:03,120 --> 00:11:04,990
That's not going to help me skip any data, 

230
00:11:05,000 --> 00:11:08,030
because the min and max is basically the entire domain of the column. 

231
00:11:08,810 --> 00:11:11,130
Or I could have an isolated data structure on every single two pole. 

232
00:11:11,910 --> 00:11:12,960
And that's useless, too, right?

233
00:11:13,170 --> 00:11:13,840
That's the other extreme. 

234
00:11:13,850 --> 00:11:16,800
So we want something in the middle where it's going to be

235
00:11:17,180 --> 00:11:19,780
large enough that it's going to allow us to throw a lot of data away. 

236
00:11:20,220 --> 00:11:20,780
But, 

237
00:11:22,920 --> 00:11:23,940
sorry, small enough that's going out.

238
00:11:23,950 --> 00:11:24,820
So throw a lot of data away, 

239
00:11:24,830 --> 00:11:30,040
but not too large where but I said large enough that we throw a lot of data, 

240
00:11:30,050 --> 00:11:30,920
but it's not too small. 

241
00:11:30,930 --> 00:11:33,160
We're spending all the time looking in this auxiliary data structure. 

242
00:11:36,220 --> 00:11:38,460
Then the other issue is it going to be manual automatic? 

243
00:11:38,970 --> 00:11:41,480
For most of the approaches, we'll see where zone maps.

244
00:11:41,490 --> 00:11:42,880
In particular, these are going to be automatic.

245
00:11:42,890 --> 00:11:43,880
The decent just does it. 

246
00:11:44,370 --> 00:11:46,570
For the bit map indexes, these are things you have to define.

247
00:11:47,130 --> 00:11:50,940
And you have to know like in some cases you have to define what the ranges are. 

248
00:11:53,340 --> 00:11:55,080
This class we're only focusing on this. 

249
00:11:55,380 --> 00:11:58,610
We won't discuss approximate queries for the rest of the semester. 

250
00:12:00,750 --> 00:12:04,070
This how to say this requires, again,

251
00:12:04,080 --> 00:12:08,500
outside knowledge of the external information about whether the application

252
00:12:08,510 --> 00:12:09,820
is ok with approximate answers, 

253
00:12:10,070 --> 00:12:11,610
that the data system can't figure out for you. 

254
00:12:12,230 --> 00:12:13,990
Like, it doesn't know when you're ok,

255
00:12:14,360 --> 00:12:17,790
a with aa rounding error in your bank account, you're probably not,

256
00:12:18,090 --> 00:12:21,040
but doesn't know that you have to be told I want to run approximate queries. 

257
00:12:21,360 --> 00:12:22,190
So we can order that for now. 

258
00:12:23,890 --> 00:12:26,410
The considerations we have to have for anything anytime we're doing

259
00:12:26,420 --> 00:12:28,930
data pruning or using auxiliary data structures. 

260
00:12:29,750 --> 00:12:32,210
It's going to be how selective we want or how selective these predators

261
00:12:32,220 --> 00:12:32,970
are going to be. 

262
00:12:33,390 --> 00:12:38,550
Like if if there's a column that's a bullying value, 

263
00:12:38,890 --> 00:12:40,910
and it's every 1 billion two balls, 

264
00:12:40,920 --> 00:12:42,310
this column is true. 

265
00:12:42,740 --> 00:12:44,500
We don't want to build an index on that because it's useless, 

266
00:12:44,510 --> 00:12:46,030
because not just let's scan a rip through it. 

267
00:12:48,150 --> 00:12:49,900
How selective the queries are going to panel, 

268
00:12:51,090 --> 00:12:53,830
how effective these are, 

269
00:12:53,840 --> 00:12:55,230
julia data structures are going to be. 

270
00:12:55,780 --> 00:12:57,900
We also have to consider the case of how skew the data is. 

271
00:12:57,910 --> 00:12:58,140
Again, 

272
00:12:58,150 --> 00:13:03,120
my extreme example where everything is set to true on 1 billion rows. 

273
00:13:03,710 --> 00:13:05,140
If I build ab plus tree on that, 

274
00:13:05,530 --> 00:13:06,620
or a bit map index on that, 

275
00:13:07,510 --> 00:13:10,860
it's useless because I get to leaf in the b plus tree. 

276
00:13:11,160 --> 00:13:14,070
Then there's a giant linked list or giant list of all the other triples. 

277
00:13:14,910 --> 00:13:15,100
Right? 

278
00:13:16,100 --> 00:13:17,950
There's a trade off here that we have to be considered. 

279
00:13:18,470 --> 00:13:21,390
Then again, we won't discuss this too much,

280
00:13:21,400 --> 00:13:22,950
but if the data is already pre sorted, 

281
00:13:23,840 --> 00:13:27,060
then that's going to allow us to do some optimization that we wouldn't be

282
00:13:27,070 --> 00:13:27,780
able to get otherwise. 

283
00:13:27,790 --> 00:13:31,040
So there's a lot of things we want to cover. 

284
00:13:31,090 --> 00:13:31,800
Again. 

285
00:13:31,810 --> 00:13:32,800
This is just like a quick, 

286
00:13:33,740 --> 00:13:35,650
some mortgage board, everything you could possibly do.

287
00:13:35,860 --> 00:13:36,730
There's other things too. 

288
00:13:36,740 --> 00:13:42,180
We're going to do our multi dimensional indexes and full inverted indexes. 

289
00:13:43,900 --> 00:13:45,600
Because I want to focus on the o lab stuff. 

290
00:13:46,570 --> 00:13:48,880
We're going to go through zone maps again, the most common ones.

291
00:13:48,890 --> 00:13:51,720
And i'll spend a lot of time talking about different bit map representations. 

292
00:13:52,340 --> 00:13:55,560
And then that'll lead us into the imprints and the column sketches. 

293
00:13:55,990 --> 00:13:56,830
The paper you guys read, 

294
00:14:00,980 --> 00:14:02,180
as I said before, many times,

295
00:14:02,190 --> 00:14:05,910
zoom maps are the most common sort of data skipping technique

296
00:14:05,920 --> 00:14:07,600
that most systems use, 

297
00:14:07,950 --> 00:14:08,910
where the olap system is used. 

298
00:14:09,180 --> 00:14:11,700
The basic idea is that these are just pre computer aggregates

299
00:14:12,260 --> 00:14:15,360
for all the individual values and all the values with individual columns, 

300
00:14:15,600 --> 00:14:16,510
a of a table. 

301
00:14:19,050 --> 00:14:21,610
The idea is that for certain predicates, 

302
00:14:21,620 --> 00:14:25,630
we could check the zone map first and see whether there could be any data

303
00:14:25,860 --> 00:14:27,170
in the blog about the reader, 

304
00:14:27,180 --> 00:14:30,050
the chunk of data about the read based on the zone map. 

305
00:14:30,350 --> 00:14:31,810
If we know according to zoom map, 

306
00:14:31,820 --> 00:14:33,280
there isn't then we skip it entirely. 

307
00:14:34,990 --> 00:14:35,150
Right? 

308
00:14:35,160 --> 00:14:40,860
So say a sample column, it has52 tuple with this consuming a single table,

309
00:14:40,870 --> 00:14:41,580
the single column, 

310
00:14:41,590 --> 00:14:43,100
and has52 balls. 

311
00:14:44,190 --> 00:14:45,220
There's no map for this. 

312
00:14:45,230 --> 00:14:48,480
We could compute the min, the max, the average, the sum, and the count.

313
00:14:49,490 --> 00:14:50,150
Pretty straightforward. 

314
00:14:51,200 --> 00:14:52,390
But now my query comes along. 

315
00:14:52,750 --> 00:14:54,910
Select start from table where value is greater than 600. 

316
00:14:56,720 --> 00:14:59,520
The first thing I would do is go look in the zone map and say, 

317
00:14:59,770 --> 00:15:01,920
I know i'm looking for value is greater than 600, 

318
00:15:02,200 --> 00:15:03,470
but the max is 400. 

319
00:15:03,740 --> 00:15:04,690
So therefore, 

320
00:15:05,380 --> 00:15:09,500
I know there isn't going to be a value in this block of data that

321
00:15:09,510 --> 00:15:10,540
could match my predicate. 

322
00:15:11,020 --> 00:15:12,580
So I could just skip it entirely. 

323
00:15:15,260 --> 00:15:15,900
Pretty straightforward. 

324
00:15:18,160 --> 00:15:18,310
Again, 

325
00:15:18,320 --> 00:15:23,600
I showed this diagram last class from the data breaks talk when they talk

326
00:15:23,610 --> 00:15:25,670
about part k and lo and behold, 

327
00:15:25,680 --> 00:15:27,030
without calling it in a zone map, 

328
00:15:27,740 --> 00:15:30,100
right here, they call page as metadata,

329
00:15:30,110 --> 00:15:34,190
the maintenance of the count that's stored in the header of every chunk. 

330
00:15:35,280 --> 00:15:36,190
This is the zone map. 

331
00:15:36,200 --> 00:15:39,210
I think oracle invented the term zone map. 

332
00:15:39,220 --> 00:15:40,960
Maybe it's copyrighted, I don't know, trademarked.

333
00:15:42,270 --> 00:15:46,400
But when people say the original definition was called small materialized aggregates, 

334
00:15:46,410 --> 00:15:50,940
that's the original paper from some this paper here from this german guy. 

335
00:15:52,190 --> 00:15:54,520
But everyone pretty much calls him zone maps, 

336
00:15:54,530 --> 00:15:56,400
even though oracle has him, calls him that,

337
00:15:57,700 --> 00:15:58,050
right? 

338
00:15:59,220 --> 00:16:01,060
Again, pretty straightforward, not hard to compute.

339
00:16:01,430 --> 00:16:04,530
And because I guess in the case of parquet, 

340
00:16:04,540 --> 00:16:05,690
these files are immutable. 

341
00:16:06,420 --> 00:16:07,250
We don't have to maintain this. 

342
00:16:07,260 --> 00:16:08,510
We don't have to change this all the time. 

343
00:16:08,520 --> 00:16:09,490
We just store it in the header. 

344
00:16:09,500 --> 00:16:10,530
And we're done. 

345
00:16:19,330 --> 00:16:19,930
As I said before, 

346
00:16:19,940 --> 00:16:23,210
there's a trade off between the scope of the zone map and its efficacy. 

347
00:16:23,570 --> 00:16:27,210
I have a zoom map on an individual to pull, that's going to be useless.

348
00:16:27,540 --> 00:16:30,790
I have a zoom map on the entire table. 

349
00:16:32,240 --> 00:16:34,090
That's not going to be either, 

350
00:16:34,100 --> 00:16:38,030
because if I have actually, it could help,

351
00:16:38,040 --> 00:16:40,950
because if I know that the thing i'm looking for is not in the bounds

352
00:16:40,960 --> 00:16:42,190
of the zone amount of the entire table, 

353
00:16:42,490 --> 00:16:44,050
then I avoid reading the entire table. 

354
00:16:46,260 --> 00:16:48,630
But if in the likely case that it is in there, 

355
00:16:49,380 --> 00:16:50,810
then there's no map is basically useless. 

356
00:16:50,820 --> 00:16:55,240
And I spent time processing and reading it. 

357
00:16:55,250 --> 00:16:57,880
The other planning to point out two of zoom maps is

358
00:16:57,890 --> 00:17:03,230
that they're only useful when the attributes position and the values

359
00:17:03,240 --> 00:17:04,050
are correlated. 

360
00:17:04,400 --> 00:17:07,280
Meaning if if the thing i'm looking for, 

361
00:17:09,380 --> 00:17:14,650
if on a certain column is and that columns values are completely random, 

362
00:17:15,160 --> 00:17:16,600
then the zoom out going to help, 

363
00:17:16,610 --> 00:17:17,680
not going to help me again, 

364
00:17:17,690 --> 00:17:21,470
because the the value domain that's encompassed by the blockade i'm looking

365
00:17:21,480 --> 00:17:23,260
at could be quite large. 

366
00:17:23,270 --> 00:17:23,940
And therefore, 

367
00:17:24,610 --> 00:17:25,610
every time I check the zone map, 

368
00:17:25,620 --> 00:17:27,450
it's going to come back as true for my predicate. 

369
00:17:28,070 --> 00:17:31,100
But I then I got to go scan it and finally think i'm looking for anyway. 

370
00:17:31,920 --> 00:17:32,300
Right? 

371
00:17:32,600 --> 00:17:37,350
But if I could pre sort the data on the attributes that are in my predicate, 

372
00:17:37,730 --> 00:17:40,460
then my zoom map will start help me throw things away. 

373
00:17:43,410 --> 00:17:44,710
So again, zoom maps,

374
00:17:44,760 --> 00:17:46,910
is it at index sort of right, 

375
00:17:46,920 --> 00:17:49,310
like it's a filter, not an index,

376
00:17:49,320 --> 00:17:51,590
so a filter would tell you is something here or no. 

377
00:17:52,150 --> 00:17:53,370
And have to go look for it. 

378
00:17:53,380 --> 00:17:54,770
Index would tell you where it is. 

379
00:17:55,540 --> 00:17:57,670
This is a filter, but it's still, again,

380
00:17:57,680 --> 00:17:59,270
this is the most commonly used one. 

381
00:17:59,690 --> 00:18:01,310
And it's not just like you, 

382
00:18:02,040 --> 00:18:05,290
you have to have additional code in your query execution engine to be

383
00:18:05,300 --> 00:18:09,270
able to consider these zone maps as part of the the process of deciding

384
00:18:09,280 --> 00:18:09,960
what data you

385
00:18:09,970 --> 00:18:12,530
actually want to read. 

386
00:18:13,830 --> 00:18:20,510
I have off the record numbers from i'll have probably put this out. 

387
00:18:21,610 --> 00:18:22,940
They said like, 

388
00:18:23,390 --> 00:18:28,070
I think like 80 to 90% of the queries get take advantage of his own maps. 

389
00:18:30,290 --> 00:18:31,680
And again, at their scale that's massive.

390
00:18:34,710 --> 00:18:34,860
Right? 

391
00:18:35,390 --> 00:18:37,220
So the win for these is quite obvious. 

392
00:18:37,230 --> 00:18:38,100
This is why everyone does it. 

393
00:18:40,100 --> 00:18:40,430
But again, 

394
00:18:40,440 --> 00:18:44,460
it's although 90% of the queries of they're talking about will take

395
00:18:44,470 --> 00:18:47,130
advantage of the zone maps doesn't mean they're going to throw

396
00:18:47,140 --> 00:18:48,890
away substantial portions of data. 

397
00:18:49,410 --> 00:18:51,970
It just means that there's some amount of data they were able to throw it away. 

398
00:18:52,620 --> 00:18:53,930
And in their world that matters, 

399
00:18:55,580 --> 00:18:57,650
if you have like a very large data, 

400
00:18:57,660 --> 00:19:01,050
then what's your overhead for like free computing with zone maps? 

401
00:19:01,870 --> 00:19:03,310
This question is, if you have very large data,

402
00:19:03,320 --> 00:19:05,590
what's the overhead of pre computing the zm maps? 

403
00:19:06,450 --> 00:19:06,920
What's your idea? 

404
00:19:06,930 --> 00:19:07,680
Very large? 

405
00:19:08,090 --> 00:19:08,690
Pet a bytes, 

406
00:19:15,280 --> 00:19:21,300
like most systems don't magically conjure up peta bytes of data instantaneously. 

407
00:19:23,180 --> 00:19:26,260
A it's usually a fire hose of something

408
00:19:26,870 --> 00:19:29,110
downstream in your application stack is generating data that

409
00:19:29,120 --> 00:19:30,510
you're now ingesting in storing, 

410
00:19:30,520 --> 00:19:32,300
but it doesn't happen all at once. 

411
00:19:33,230 --> 00:19:35,910
The only system I the only application I know where this does happen all

412
00:19:35,920 --> 00:19:41,240
at once was the large hadron collider at at cern. 

413
00:19:41,250 --> 00:19:43,810
Because when they did the experiment like that thing, 

414
00:19:43,820 --> 00:19:45,930
generally tetabytes instantaneously, right?

415
00:19:46,260 --> 00:19:47,140
And they got to deal with that. 

416
00:19:47,500 --> 00:19:50,090
Most systems are not smashing atoms, right?

417
00:19:50,100 --> 00:19:50,850
Most systems are like, 

418
00:19:51,400 --> 00:19:52,550
i'm collecting sensor data, 

419
00:19:52,560 --> 00:19:53,910
or I have people using my website. 

420
00:19:54,760 --> 00:19:57,980
I'm getting things in as a stream. 

421
00:19:57,990 --> 00:19:59,660
So in that case, I can batch things up.

422
00:20:00,080 --> 00:20:00,870
Compute these o maps. 

423
00:20:00,880 --> 00:20:02,030
Again, it's not expensive.

424
00:20:02,490 --> 00:20:04,010
I got to pack it into the file. 

425
00:20:04,020 --> 00:20:07,130
Anyway, somebody reading it because I got to split things up into columns.

426
00:20:07,770 --> 00:20:08,920
It's not expensive to maintain. 

427
00:20:16,290 --> 00:20:17,040
So the alternative, 

428
00:20:17,410 --> 00:20:20,790
another approach i'm going to look at is we're going to spend most

429
00:20:20,800 --> 00:20:23,760
of this class on is instead of using zoom zone. 

430
00:20:23,770 --> 00:20:26,000
So zone maps will help you filter out blocks. 

431
00:20:26,490 --> 00:20:29,940
If you still want to find things that are actually matching like you want

432
00:20:29,950 --> 00:20:30,440
an index. 

433
00:20:32,440 --> 00:20:33,630
For olap workloads, 

434
00:20:33,640 --> 00:20:36,270
bit map indexes are going to be the primary choice. 

435
00:20:36,730 --> 00:20:39,330
Again ignoring inverted indexes annoying multi dimensional indexes

436
00:20:39,340 --> 00:20:42,220
because those are sort of different types of queries. 

437
00:20:43,680 --> 00:20:46,900
Think of things like select * from table where attribute

438
00:20:46,910 --> 00:20:50,140
greater than something or attribute equal something like those. 

439
00:20:52,830 --> 00:20:57,730
The full text indexes are trying to find words in strings are text documents. 

440
00:20:57,980 --> 00:20:58,930
We can ignore that for now. 

441
00:21:00,590 --> 00:21:04,340
With bit that index is the idea is that for every unique value, 

442
00:21:06,400 --> 00:21:07,910
a in a column, in an attribute,

443
00:21:08,410 --> 00:21:12,330
we're going to maintain a separate vector with a bit set to one, 

444
00:21:12,340 --> 00:21:16,620
whether a tuple at that offset has that particular value. 

445
00:21:18,530 --> 00:21:19,840
Again, because everything's fixed length,

446
00:21:19,850 --> 00:21:23,040
we can just use the offset in the bit map to then map to the offset

447
00:21:23,550 --> 00:21:25,330
in the actual data columns themselves. 

448
00:21:25,950 --> 00:21:28,980
I th position in the bit map corresponds to the I th position in the tuple. 

449
00:21:30,040 --> 00:21:30,430
And obviously, 

450
00:21:30,440 --> 00:21:34,090
we're not going to want to have a giant bit map for the entire table. 

451
00:21:34,490 --> 00:21:37,730
We're going to do this as we chunk things up in our pack styles

452
00:21:37,740 --> 00:21:39,090
of row groups as we shared before. 

453
00:21:40,120 --> 00:21:41,950
Bit map indexes are an old idea. 

454
00:21:42,080 --> 00:21:43,950
They go back to the 1970s. 

455
00:21:45,670 --> 00:21:49,010
And then there's this paper from 87 that sort of describes the first way

456
00:21:50,050 --> 00:21:51,680
to put it into a database system. 

457
00:21:52,310 --> 00:21:56,660
This is just a sample of some of the systems that support bit map indexes. 

458
00:21:58,800 --> 00:22:00,080
Post code has these things called brain. 

459
00:22:00,090 --> 00:22:04,520
They're going to be range index by binary range indexes. 

460
00:22:04,530 --> 00:22:07,220
And that'll be we'll see in range coding, 

461
00:22:07,230 --> 00:22:10,600
a second pelosa is now commercialized as feature based. 

462
00:22:10,970 --> 00:22:15,010
This is nothing but aa giant distributed bit map index database. 

463
00:22:15,020 --> 00:22:16,860
That's all it is just bit maps. 

464
00:22:18,020 --> 00:22:20,450
Then you can pino has range indexes, 

465
00:22:21,760 --> 00:22:23,790
a bunch of other systems that I support this. 

466
00:22:29,120 --> 00:22:30,350
Here's a really simple example. 

467
00:22:30,730 --> 00:22:33,240
We have a table has 2 columns, the id field,

468
00:22:33,250 --> 00:22:35,000
and then whether somebody's lit or not, 

469
00:22:35,740 --> 00:22:36,370
yes or no, 

470
00:22:36,920 --> 00:22:39,230
say we want to build a bit map index on this column. 

471
00:22:40,360 --> 00:22:41,150
We're going to have, 

472
00:22:41,660 --> 00:22:43,460
since it's a binary value, 

473
00:22:44,230 --> 00:22:46,580
yes or no, we only have 2 bit maps.

474
00:22:47,000 --> 00:22:49,310
And so we'll have one for and one for no. 

475
00:22:49,670 --> 00:22:49,820
Again, 

476
00:22:49,830 --> 00:22:53,680
there's a one in the bit map corresponding to whether the attribute

477
00:22:53,690 --> 00:22:56,760
within the original data has that particular value. 

478
00:22:58,540 --> 00:23:02,020
If i'm looking inside this, if I jump to the 4th all set here,

479
00:23:02,340 --> 00:23:07,300
I can check to see that for the lit status is zero for it's one for no. 

480
00:23:07,390 --> 00:23:08,180
I know it's no. 

481
00:23:08,850 --> 00:23:10,270
That corresponds to the original value here. 

482
00:23:12,350 --> 00:23:15,100
This is like a bit map index in its most basic form, pretty simple.

483
00:23:16,930 --> 00:23:20,670
And they will see some ways we can use sim d to make this actually go faster. 

484
00:23:20,760 --> 00:23:24,150
You're not actually just running in a for loop and examining bits one by one. 

485
00:23:25,320 --> 00:23:26,470
We'll cover in a few more slides, 

486
00:23:26,480 --> 00:23:28,710
then we'll cover later in the semester a lot more. 

487
00:23:35,020 --> 00:23:37,770
Today's question is, which then leads to this next slide.

488
00:23:38,120 --> 00:23:40,810
What kind of data is suitable to be used in a bit map index? 

489
00:23:41,240 --> 00:23:44,950
Because if it's a the worst case scenario, again, thinking extremes,

490
00:23:44,960 --> 00:23:47,770
if it's a auto increment key or serial key, 

491
00:23:48,160 --> 00:23:51,550
where every tuple has had a unique value. 

492
00:23:51,850 --> 00:23:52,610
That's the worst case scenario, 

493
00:23:52,620 --> 00:23:56,690
because I have a bit map for every single one for every single value, 

494
00:23:56,700 --> 00:23:58,090
and most of them are going to be all zeros. 

495
00:23:58,880 --> 00:24:00,510
Every entry is going to be zero except for one. 

496
00:24:02,600 --> 00:24:06,610
And then my extreme case is like a it's a binary decision, yes or no.

497
00:24:07,370 --> 00:24:08,290
That's the best case scenario. 

498
00:24:09,380 --> 00:24:10,790
But most things are not that extreme. 

499
00:24:10,800 --> 00:24:11,550
It's somewhere in the middle. 

500
00:24:15,250 --> 00:24:15,740
Like on. 

501
00:24:16,650 --> 00:24:16,810
Her question. 

502
00:24:16,820 --> 00:24:17,470
Is her statement? 

503
00:24:17,700 --> 00:24:19,010
Isn't it just an edom value? 

504
00:24:19,870 --> 00:24:21,200
But no, is the edom?

505
00:24:23,810 --> 00:24:24,240
You only. 

506
00:24:33,450 --> 00:24:33,600
No. 

507
00:24:34,340 --> 00:24:36,540
He's correct that like this is like, 

508
00:24:37,560 --> 00:24:41,460
i'm just trying to show like there's 2 bit maps in a binary decision. 

509
00:24:41,470 --> 00:24:42,620
It's either one or zero. 

510
00:24:42,750 --> 00:24:43,700
You only need 1 bit map. 

511
00:24:44,200 --> 00:24:44,760
Yeah, right.

512
00:24:45,480 --> 00:24:45,790
So how about? 

513
00:24:46,080 --> 00:24:46,310
No? 

514
00:24:46,320 --> 00:24:47,910
Maybe you still need 3 bit maps. 

515
00:24:49,160 --> 00:24:50,270
Like your maybe lit, right?

516
00:24:51,010 --> 00:24:51,560
Yeah, see.

517
00:24:51,570 --> 00:24:54,760
Like, maybe so like you need 3 bit maps,

518
00:24:57,280 --> 00:25:02,170
but to your.like zero, 

519
00:25:02,180 --> 00:25:06,610
just having like storing like 1012 or something. 

520
00:25:10,640 --> 00:25:12,620
Really, I don't know how I call it a bit now.

521
00:25:14,200 --> 00:25:16,880
Because if you store it, like,

522
00:25:17,880 --> 00:25:18,430
so, 

523
00:25:19,180 --> 00:25:25,160
you're basically saying it's basically it's sort of a column. 

524
00:25:25,620 --> 00:25:25,990
Right? 

525
00:25:28,230 --> 00:25:31,100
We'll see why you want to do this as we go along, right?

526
00:25:32,580 --> 00:25:33,890
You can have more compact form. 

527
00:25:34,310 --> 00:25:37,060
I'm not even getting into like e gnom is

528
00:25:37,390 --> 00:25:41,460
maybe also like it's a simplified example where like you could do

529
00:25:42,450 --> 00:25:43,900
the compression techniques and things like that. 

530
00:25:44,250 --> 00:25:47,490
We'll go along and we'll see that because we store things in bits. 

531
00:25:48,060 --> 00:25:52,360
We can play games to make things run really fast. 

532
00:25:53,950 --> 00:25:57,920
Yeah, i'm going to do in.

533
00:25:57,930 --> 00:25:59,160
India is like you have english. 

534
00:26:00,580 --> 00:26:03,870
His statement is this only works for categorical variables of values, 

535
00:26:04,210 --> 00:26:05,900
which is the same thing, as she saying in enum.

536
00:26:06,650 --> 00:26:10,930
I will say through our analysis of schemers, 

537
00:26:11,370 --> 00:26:14,050
most people don't, there's like enum are super rare,

538
00:26:14,910 --> 00:26:14,950
right? 

539
00:26:15,160 --> 00:26:18,230
Even though like it may be like in the application, treat,

540
00:26:18,240 --> 00:26:19,270
it really isn't enough. 

541
00:26:19,680 --> 00:26:22,000
They define it as an inter, whatever, right?

542
00:26:22,230 --> 00:26:25,220
Without thinking about it to his comment, also, too,

543
00:26:25,230 --> 00:26:27,140
that like it's a category of variable. 

544
00:26:27,850 --> 00:26:32,090
But like it'll work on anything like you like, 

545
00:26:32,430 --> 00:26:36,130
I think, like an oracle or you can tell I want a bit map index.

546
00:26:36,650 --> 00:26:38,270
It doesn't come back say, hey, that's a bad idea.

547
00:26:38,280 --> 00:26:39,030
I'm not doing that. 

548
00:26:39,040 --> 00:26:44,810
Like if it's the worst case scenario between restrictions. 

549
00:26:44,820 --> 00:26:46,050
And I it says like, 

550
00:26:46,980 --> 00:26:48,090
we're all saying the same thing. 

551
00:26:48,260 --> 00:26:50,530
This is my example here to show why this doesn't always work. 

552
00:26:52,550 --> 00:26:54,990
They have a dimension table on customer information. 

553
00:26:55,320 --> 00:26:59,130
And then we have this zip code attribute here, right?

554
00:26:59,990 --> 00:27:03,820
My query is select id id email address from the customer dimension table

555
00:27:03,830 --> 00:27:05,930
where the zip code is in this range here. 

556
00:27:06,860 --> 00:27:08,290
These three possible values in pittsburgh, 

557
00:27:09,240 --> 00:27:11,390
I want to build an index on this column here. 

558
00:27:12,250 --> 00:27:13,920
If I build it as a bit map index, 

559
00:27:15,300 --> 00:27:21,150
then it's got to be fast because if I want to do a scan now to find

560
00:27:21,160 --> 00:27:22,230
all the matches for this, 

561
00:27:22,660 --> 00:27:28,130
I just scanned the 3 bit maps that correspond to15216152171218. 

562
00:27:28,450 --> 00:27:29,780
And I take the intersection of those. 

563
00:27:30,230 --> 00:27:31,730
Now I know the two poles that match, 

564
00:27:33,020 --> 00:27:33,270
right? 

565
00:27:33,280 --> 00:27:37,990
And I can do that evaluation of bits very efficiently, 

566
00:27:38,780 --> 00:27:41,110
much more faster than actually comparing to integers. 

567
00:27:42,560 --> 00:27:42,600
Right? 

568
00:27:42,610 --> 00:27:48,200
Because I I can pack together a bunch of bits in a single value, 

569
00:27:48,450 --> 00:27:50,700
and then do the bit wise comparison on those. 

570
00:27:51,730 --> 00:27:55,090
But this obviously is going to be problematic as you guys keep bringing up, 

571
00:27:55,100 --> 00:27:58,210
because the value domain for zip codes in the united states

572
00:27:58,220 --> 00:27:59,290
is actually quite large. 

573
00:28:00,930 --> 00:28:02,250
Anything guess how many tip codes there are in? 

574
00:28:02,740 --> 00:28:05,260
The us 10,000, 

575
00:28:05,270 --> 00:28:07,220
no100,000. 

576
00:28:08,260 --> 00:28:08,480
All right. 

577
00:28:08,730 --> 00:28:10,200
It's less than 1,000, more than 10,000.

578
00:28:11,030 --> 00:28:11,710
It's 43,000. 

579
00:28:13,240 --> 00:28:15,120
So if I have 10 million two poles, 

580
00:28:15,440 --> 00:28:18,530
and I build the bit map index on this, 

581
00:28:18,540 --> 00:28:25,150
I have to have43,000 different bit maps of length10 million bits. 

582
00:28:25,680 --> 00:28:27,420
And that's about 5033 gigs. 

583
00:28:28,780 --> 00:28:28,890
Right? 

584
00:28:29,060 --> 00:28:30,930
The original data of just like 10 million, 

585
00:28:30,940 --> 00:28:32,130
30 bit integers is like what? 

586
00:28:32,140 --> 00:28:32,770
40 megs. 

587
00:28:34,800 --> 00:28:37,270
We're paying a a lot of space for all these bit maps. 

588
00:28:37,280 --> 00:28:39,660
And most of the time they're going to be sparse, 

589
00:28:39,670 --> 00:28:40,660
much time is going to be zero. 

590
00:28:43,050 --> 00:28:43,460
Right? 

591
00:28:44,690 --> 00:28:46,060
This sort of naive scheme was like, 

592
00:28:46,150 --> 00:28:51,460
let's just create giant bit maps for the entire length of all the tuples, 

593
00:28:51,710 --> 00:28:55,380
or the size of the table and have for every unique possible value that I have. 

594
00:28:55,870 --> 00:28:55,970
Right? 

595
00:28:56,380 --> 00:28:57,600
This obviously won't work. 

596
00:28:57,610 --> 00:29:00,180
So we need to be smart about how we're going to do this. 

597
00:29:03,020 --> 00:29:05,210
The two design choices we have to consider is the encoding scheme

598
00:29:05,380 --> 00:29:07,650
how to represent and organize the data within the bit map. 

599
00:29:08,980 --> 00:29:10,970
In my simple examples, i'm just saying it's a giant.

600
00:29:11,660 --> 00:29:13,370
You mount a giant array of bits, 

601
00:29:14,120 --> 00:29:15,430
but we can be more clever. 

602
00:29:16,780 --> 00:29:20,790
Then how do we actually compress the size or reduce the size of bit maps

603
00:29:20,800 --> 00:29:22,150
where we know they're going to be sparse? 

604
00:29:23,530 --> 00:29:25,560
We'll cover this second one, the next class,

605
00:29:25,570 --> 00:29:27,360
and then this class will focus on the first one here. 

606
00:29:30,560 --> 00:29:32,790
The idea again, we want to get, we don't want to, basically we're trying to,

607
00:29:32,800 --> 00:29:35,590
can we get the benefits that we would get if we had a bit map

608
00:29:36,320 --> 00:29:40,430
to do accelerate scans without paying that huge georgia ahead

609
00:29:41,150 --> 00:29:43,340
of these giant arrays of empty data. 

610
00:29:46,510 --> 00:29:47,380
The before encoding schemes, 

611
00:29:47,390 --> 00:29:51,480
we can consider what i've already shown you is the equality coding. 

612
00:29:52,300 --> 00:29:55,610
You just have a giant bit map with for every unique value that you

613
00:29:55,620 --> 00:29:57,970
could possibly have in your column. 

614
00:30:00,470 --> 00:30:01,440
With range of coding, 

615
00:30:02,290 --> 00:30:08,010
it's a way to sort of store less bit maps for all the unique values

616
00:30:08,020 --> 00:30:10,960
instead of saying for exactly 15217, 

617
00:30:10,970 --> 00:30:12,860
and 15216 for those zip codes. 

618
00:30:13,220 --> 00:30:17,500
But if I take a range of zip codes and represent a bit map of those values, 

619
00:30:18,710 --> 00:30:21,630
and this is sort of what the column sketch paper you guys read. 

620
00:30:21,640 --> 00:30:23,350
It's viewed the basic same idea. 

621
00:30:23,740 --> 00:30:28,810
Whereas now that means that I could get false positives where I think I

622
00:30:28,820 --> 00:30:29,650
have a match, 

623
00:30:30,580 --> 00:30:33,630
I could possibly have a match within some range. 

624
00:30:35,160 --> 00:30:37,530
I think i'm matching more triples than I actually I am matching. 

625
00:30:37,750 --> 00:30:39,700
Then i've got to go check back the original values

626
00:30:39,710 --> 00:30:42,520
of the original base data and do the final pruning. 

627
00:30:44,050 --> 00:30:45,160
So postcards has this, 

628
00:30:45,170 --> 00:30:47,320
again with these grid indexes. 

629
00:30:47,750 --> 00:30:49,740
Patchy piano came out of, 

630
00:30:50,990 --> 00:30:52,350
it came out of linked in, I forgot,

631
00:30:52,620 --> 00:30:59,370
but a it's an open source system or that system that supports makes

632
00:30:59,380 --> 00:31:00,570
heavy use of range indexes. 

633
00:31:01,820 --> 00:31:04,210
Again, the human has to specify these ranges.

634
00:31:04,670 --> 00:31:06,860
As far as I know that no database system can actually automatically figure

635
00:31:06,870 --> 00:31:07,980
these things out for you yet. 

636
00:31:10,080 --> 00:31:13,670
Hydro coding is a way to use a tree structure to identify the mpu key ranges, 

637
00:31:14,190 --> 00:31:16,190
and then not actually have a store them. 

638
00:31:16,650 --> 00:31:18,190
We'll see that in the next slide, 

639
00:31:18,200 --> 00:31:22,270
and then bit slicing is a way to actually maintain a bit map

640
00:31:22,280 --> 00:31:27,430
of a single bit location a within a column, 

641
00:31:27,830 --> 00:31:29,900
and then do a bunch of tricks to speed things up. 

642
00:31:30,480 --> 00:31:31,860
When you violate this bit, 

643
00:31:31,870 --> 00:31:34,020
slicing is a really neat idea from the 1990s. 

644
00:31:35,740 --> 00:31:36,530
I don't think. 

645
00:31:36,870 --> 00:31:39,850
He says, maybe I think pino might use something like this or pelosi,

646
00:31:41,000 --> 00:31:46,350
but we'll see the modern variation of bit weaving from wisconsin how to do

647
00:31:46,360 --> 00:31:47,100
this very efficiently, 

648
00:31:47,560 --> 00:31:48,100
which is pretty cool. 

649
00:31:50,550 --> 00:31:51,740
Let's go through the last two, 

650
00:31:51,750 --> 00:31:53,540
because there's not much to say about range encoding. 

651
00:31:53,930 --> 00:31:55,710
Obviously, again, it's just the same idea.

652
00:31:56,930 --> 00:31:59,000
Instead of having a bit not per value, you have one per range.

653
00:32:01,450 --> 00:32:02,040
Hydro coding. 

654
00:32:02,050 --> 00:32:05,040
The idea is going to be is that we're going to have a tree structure. 

655
00:32:05,050 --> 00:32:07,350
We're going to organize the bit map as a tree structure. 

656
00:32:07,920 --> 00:32:12,150
Where at each node, 

657
00:32:12,600 --> 00:32:14,320
there's going to be a bit map itself. 

658
00:32:15,930 --> 00:32:20,070
And that's going to gonna tell you whether the child represented

659
00:32:20,080 --> 00:32:24,500
by the position at that node is gonna have a one below it in the subtree. 

660
00:32:26,280 --> 00:32:28,420
In this case here, every node is going to have 4 bits,

661
00:32:28,920 --> 00:32:31,350
every or in its bit internal bit map. 

662
00:32:31,360 --> 00:32:32,770
And then every node, 

663
00:32:32,780 --> 00:32:33,970
it's going to have four children. 

664
00:32:34,630 --> 00:32:36,710
So I a if I have a one in a position, 

665
00:32:36,720 --> 00:32:41,800
then that's going to tell me that the subtree pointed down by this pointer. 

666
00:32:42,230 --> 00:32:43,640
There's going to be a one somewhere. 

667
00:32:43,650 --> 00:32:44,710
They don't know where, 

668
00:32:44,720 --> 00:32:46,520
but somewhere in this case here, 

669
00:32:46,530 --> 00:32:47,680
the second position is zero. 

670
00:32:47,690 --> 00:32:50,660
So I know that in the second child going down, 

671
00:32:50,670 --> 00:32:52,600
this subtree isn't going to be anything. 

672
00:32:54,310 --> 00:32:56,730
The way to think about this is along the leaf nodes. 

673
00:32:57,170 --> 00:32:58,840
These are the offsets in the bit map, 

674
00:32:58,850 --> 00:32:59,880
just as we had before. 

675
00:33:03,010 --> 00:33:04,400
In this case here, i'm storing,

676
00:33:04,450 --> 00:33:06,670
say, offsets at the top here,

677
00:33:06,680 --> 00:33:08,390
and I have the bits at the corresponding location. 

678
00:33:10,570 --> 00:33:13,120
I'm not actually storing the bunch of zeros. 

679
00:33:13,600 --> 00:33:17,360
What you're really storing is just the locations where you have a one. 

680
00:33:19,520 --> 00:33:19,910
You see, 

681
00:33:19,920 --> 00:33:24,490
if I want to figure out whether I I have aa bit set in position one, 

682
00:33:25,220 --> 00:33:25,410
right? 

683
00:33:25,420 --> 00:33:26,290
I start at the top. 

684
00:33:26,510 --> 00:33:30,180
I say, I know I need to look at this side of the tree.

685
00:33:30,190 --> 00:33:31,740
So therefore, the first bit is one.

686
00:33:32,160 --> 00:33:33,540
I know, and not zero.

687
00:33:33,830 --> 00:33:34,700
I need to go down here. 

688
00:33:35,010 --> 00:33:36,800
This bit is set to one. 

689
00:33:37,190 --> 00:33:38,850
I know I need to go down here and then low and behold, 

690
00:33:38,860 --> 00:33:41,210
I can look at my bit map and see that there's something there. 

691
00:33:43,050 --> 00:33:43,310
Right? 

692
00:33:45,890 --> 00:33:51,280
What are some problems with this i's over here? 

693
00:33:52,350 --> 00:33:55,070
He says you decide the size of the tree before the size of the nodes. 

694
00:33:56,380 --> 00:33:57,420
That's not hard, right?

695
00:33:57,430 --> 00:34:01,330
Because it depends on the number of values I have. 

696
00:34:05,550 --> 00:34:08,800
There will be one friendship outside to just keep sending, 

697
00:34:08,810 --> 00:34:10,150
and then well, 

698
00:34:10,520 --> 00:34:14,960
versus like it will be a really deep like the type of opportunity. 

699
00:34:14,970 --> 00:34:15,300
Really. 

700
00:34:15,310 --> 00:34:23,470
I don't know that his statement is that if the again, 

701
00:34:23,960 --> 00:34:24,910
so this is a bit map, 

702
00:34:24,920 --> 00:34:28,030
this is the thing that says like with someone lit or not, yes or no.

703
00:34:28,960 --> 00:34:31,710
This is just instead of storing it again as a giant array. 

704
00:34:32,200 --> 00:34:32,990
This is the array. 

705
00:34:34,120 --> 00:34:36,250
So skew this in terms of what's, 

706
00:34:36,670 --> 00:34:41,510
because it's the bit set to one that corresponds to like this position

707
00:34:41,520 --> 00:34:42,670
here corresponds to one tube. 

708
00:34:46,360 --> 00:34:47,400
Distribution is unique. 

709
00:34:47,860 --> 00:34:49,810
And you don't say anything. 

710
00:34:50,580 --> 00:34:52,250
It says the distribution is uniform. 

711
00:34:52,420 --> 00:34:53,490
If all the bits are set to one, 

712
00:34:53,500 --> 00:34:54,570
then you don't save any space. 

713
00:34:55,610 --> 00:34:56,880
Yes, you spend more time.

714
00:34:56,890 --> 00:34:57,120
Yes. 

715
00:34:58,550 --> 00:35:00,510
Let's say, a little bit similar to that,

716
00:35:00,520 --> 00:35:04,150
but i'm kind of worried for memory optimization issues in terms of locality. 

717
00:35:04,510 --> 00:35:04,720
There you go. 

718
00:35:05,090 --> 00:35:06,040
And what else? 

719
00:35:07,130 --> 00:35:07,530
It's tied. 

720
00:35:07,540 --> 00:35:09,280
I what's reading the memory? 

721
00:35:10,340 --> 00:35:11,750
The cpu right? 

722
00:35:12,650 --> 00:35:14,420
So this seems like a great idea. 

723
00:35:15,870 --> 00:35:16,710
It's one of the things. 

724
00:35:16,720 --> 00:35:18,610
Again, this idea that came out of the 1990s.

725
00:35:18,620 --> 00:35:19,750
It seems like a good idea, 

726
00:35:19,760 --> 00:35:23,840
but nobody actually implements it because on a modern super scalar cpu this

727
00:35:23,850 --> 00:35:25,640
is a part came out of 2003, 

728
00:35:25,650 --> 00:35:29,730
but on a modern cpu this is a bad idea, 

729
00:35:29,740 --> 00:35:33,030
because now you have this in direction, 

730
00:35:33,040 --> 00:35:34,350
whereas i'm doing this probing, 

731
00:35:34,360 --> 00:35:35,830
i've got to go look at this bit map, 

732
00:35:35,840 --> 00:35:37,630
and I may go down one path versus another. 

733
00:35:37,970 --> 00:35:40,700
And then that might be another cache line miss to go fetch this. 

734
00:35:41,360 --> 00:35:41,620
Right? 

735
00:35:41,630 --> 00:35:49,240
So like the the cost ever had this storage savings you would get is

736
00:35:49,250 --> 00:35:52,950
not worth the sort of the performance panel you pay for using

737
00:35:53,210 --> 00:35:54,700
a probing data structure like this. 

738
00:35:55,320 --> 00:35:57,390
Again, think of like trying to scan large things.

739
00:35:57,810 --> 00:36:00,840
Again, it's not get b plus three where I can scan along the leaf nodes.

740
00:36:01,310 --> 00:36:02,910
I'd have to store some metadata, 

741
00:36:04,560 --> 00:36:07,110
either store some metadata about where what position i'm in, 

742
00:36:08,380 --> 00:36:11,690
and then maintain pointers across to go from 11 node to the next. 

743
00:36:12,200 --> 00:36:13,720
Or I have to do a depth first search, 

744
00:36:13,730 --> 00:36:16,590
I got to burst down here and then come back up, right?

745
00:36:16,970 --> 00:36:18,660
And that's how I scan along you all the leaf nodes. 

746
00:36:19,120 --> 00:36:21,280
All that back and forth is very costly. 

747
00:36:21,860 --> 00:36:22,540
In a superscalar, 

748
00:36:22,550 --> 00:36:27,150
cpu where you don't want to have stalls in your obstruction pipeline, 

749
00:36:27,160 --> 00:36:28,970
because now you're jumping to another location. 

750
00:36:30,360 --> 00:36:30,870
So also, 

751
00:36:31,580 --> 00:36:35,100
more recently, zone map says if you have all you're writing,

752
00:36:35,110 --> 00:36:38,060
so we can give one of them before these one. 

753
00:36:39,100 --> 00:36:40,940
You're missing production of space, right?

754
00:36:43,060 --> 00:36:43,770
So a statement is, 

755
00:36:45,110 --> 00:36:47,260
if it's, again,

756
00:36:47,430 --> 00:36:49,540
it's the enum category of value issue, right?

757
00:36:49,550 --> 00:36:50,660
If everything's one, 

758
00:36:50,670 --> 00:36:55,150
then this41 to the one. 

759
00:36:57,590 --> 00:36:58,720
So like in this case, 

760
00:36:58,730 --> 00:37:01,720
if if at least one value, 

761
00:37:02,610 --> 00:37:03,640
a in a node is one, 

762
00:37:03,650 --> 00:37:05,200
then I have to maintain the tree structure. 

763
00:37:05,830 --> 00:37:07,120
These nodes are really big. 

764
00:37:07,130 --> 00:37:09,060
I'm showing 44 bits per node, 

765
00:37:09,070 --> 00:37:10,700
because I make it fit in powerpoint. 

766
00:37:10,980 --> 00:37:13,780
Like if it's something larger as it should be, 

767
00:37:14,900 --> 00:37:17,210
a like a cache line size, 64 bytes,

768
00:37:17,570 --> 00:37:22,260
then the likelihood that something is going to be one is, yes.

769
00:37:22,930 --> 00:37:23,130
Correct. 

770
00:37:23,140 --> 00:37:23,410
Yes. 

771
00:37:26,030 --> 00:37:26,240
Right. 

772
00:37:26,250 --> 00:37:27,360
So he's basically saying, again,

773
00:37:29,090 --> 00:37:31,830
just i've got 1 bit set to true, 

774
00:37:32,220 --> 00:37:33,800
and i've got to store the entire node. 

775
00:37:34,320 --> 00:37:35,610
And the nodes are going to be kind of large. 

776
00:37:35,930 --> 00:37:36,020
Right? 

777
00:37:36,030 --> 00:37:37,020
Probably a cache line. 

778
00:37:38,950 --> 00:37:41,270
So again, this is seems cool, seems clever.

779
00:37:42,740 --> 00:37:43,740
The savings is real. 

780
00:37:43,750 --> 00:37:46,220
Are we go from 8 bytes to 4 bytes? 

781
00:37:47,810 --> 00:37:49,460
But again, in a modern cp architecture,

782
00:37:51,160 --> 00:37:53,780
this is going to be bad or better off just applying the data ripping

783
00:37:53,790 --> 00:37:54,140
through it. 

784
00:37:57,400 --> 00:38:02,790
Let me show you another variation to bit slicing another way to encode things. 

785
00:38:03,790 --> 00:38:04,420
For this one, again,

786
00:38:04,430 --> 00:38:09,190
we're going to have our original table is going to have this id field. 

787
00:38:09,680 --> 00:38:11,570
And everyone have a zip code column. 

788
00:38:11,920 --> 00:38:12,930
These are actually all the zip codes, 

789
00:38:12,940 --> 00:38:14,450
the places i've lived before in my life. 

790
00:38:15,630 --> 00:38:18,070
I grew up in maryland in the pittsburgh now, compton,

791
00:38:18,450 --> 00:38:19,920
wisconsin, and so forth.

792
00:38:20,180 --> 00:38:20,460
Right? 

793
00:38:21,280 --> 00:38:26,830
What we're going to do now is we're going to flip around how we're

794
00:38:26,840 --> 00:38:28,530
actually going to be storing

795
00:38:29,310 --> 00:38:33,050
the data within the values themselves instead of having this bit map to say, 

796
00:38:34,650 --> 00:38:37,070
this bit map corresponds to 15217. 

797
00:38:37,600 --> 00:38:39,460
And there's a one in that bit map. 

798
00:38:39,790 --> 00:38:40,820
If the people has that, 

799
00:38:41,210 --> 00:38:46,030
we're now actually going to store the values themselves in bit maps

800
00:38:46,310 --> 00:38:49,100
at a sort of single rate of single in a bit wide manner. 

801
00:38:50,090 --> 00:38:52,240
And then that's going to open up opportunities when we start doing

802
00:38:52,250 --> 00:38:53,400
predicate evaluation, 

803
00:38:53,690 --> 00:38:56,520
to do early pruning and identifying things that we don't actually need to read. 

804
00:38:56,530 --> 00:38:57,960
So again, 

805
00:38:57,970 --> 00:39:01,990
think of this as like you wouldn't want to store the table itself like this. 

806
00:39:02,540 --> 00:39:05,410
This would be an auxiliary data structure like an index that we

807
00:39:05,420 --> 00:39:08,820
can then identify what offsets in the column and the original table

808
00:39:08,830 --> 00:39:09,500
are matching. 

809
00:39:09,910 --> 00:39:13,410
Therefore, we then will need to go get the full data of the columns,

810
00:39:13,420 --> 00:39:14,810
get the process of the query. 

811
00:39:16,770 --> 00:39:16,920
Again, 

812
00:39:16,930 --> 00:39:20,210
the way to think about bit slicing is that it's like when we took

813
00:39:20,220 --> 00:39:22,010
a row store and convert it to a column store. 

814
00:39:22,430 --> 00:39:25,540
But now we're taking the bits that we're storing for each value

815
00:39:26,000 --> 00:39:27,320
And flipping that to be a column store. 

816
00:39:30,110 --> 00:39:31,100
This is an old idea. 

817
00:39:31,110 --> 00:39:35,800
I think it comes back to actually forget it for the 90s. 

818
00:39:36,990 --> 00:39:40,140
Let's take the first value here, 21, 042, where I was born.

819
00:39:41,580 --> 00:39:43,570
If we just convert that to its binary form, 

820
00:39:43,580 --> 00:39:45,060
we end up with this bit string here. 

821
00:39:47,150 --> 00:39:49,050
Again, I realize in a for,

822
00:39:49,360 --> 00:39:50,630
I showed that the table schema, 

823
00:39:51,190 --> 00:39:54,700
32 bit integer, we're going to show 17 bits, just they make it fit.

824
00:39:55,190 --> 00:39:59,250
So we're going to have17 bits for the actual data itself. 

825
00:39:59,590 --> 00:40:04,280
Then we'll have one bit map for to identify whether a value is null or not. 

826
00:40:05,120 --> 00:40:05,780
In this case here, 

827
00:40:06,870 --> 00:40:08,700
the value is not null, that's set to zero.

828
00:40:09,450 --> 00:40:10,280
Then, again,

829
00:40:10,290 --> 00:40:14,130
for every single bit along and the binary representation of the value, 

830
00:40:14,140 --> 00:40:17,250
we're going to store it in a bit map like this. 

831
00:40:17,910 --> 00:40:23,290
The bit map is going down not across with any single position in the value

832
00:40:23,860 --> 00:40:24,890
of the attribute. 

833
00:40:25,700 --> 00:40:27,650
Here's all the bits for all the tuples. 

834
00:40:29,630 --> 00:40:31,580
If you do the same thing for all the others. 

835
00:40:32,270 --> 00:40:32,650
Right? 

836
00:40:34,190 --> 00:40:35,460
Now, if our query comes along,

837
00:40:35,470 --> 00:40:39,910
find all the customers that are in a zip code less than 15217. 

838
00:40:40,990 --> 00:40:47,030
What we can do is we can look at what is for this value here. 

839
00:40:47,040 --> 00:40:52,630
What is the first position where in the binary representation of the value? 

840
00:40:52,940 --> 00:40:55,280
What is the first position of the one? 

841
00:40:56,140 --> 00:40:56,500
I therefore, 

842
00:40:56,510 --> 00:40:59,800
we know that anything before that is going to be zero and anything that, 

843
00:40:59,810 --> 00:41:00,360
therefore, 

844
00:41:01,240 --> 00:41:04,020
is not zero within those first positions. 

845
00:41:04,300 --> 00:41:05,250
We can throw it entirely. 

846
00:41:07,230 --> 00:41:07,440
Right? 

847
00:41:07,450 --> 00:41:11,090
So i'm going to basically walk through each slice and construct

848
00:41:11,100 --> 00:41:15,180
a a little bit and say whether a what two boy

849
00:41:15,190 --> 00:41:17,630
different positions in each row matches. 

850
00:41:18,840 --> 00:41:19,000
Right? 

851
00:41:19,010 --> 00:41:22,640
So15217 is this value here. 

852
00:41:23,410 --> 00:41:24,800
The first three bits are zero, 

853
00:41:24,810 --> 00:41:25,840
and then we have the one. 

854
00:41:26,350 --> 00:41:28,010
What we need to do is we need to examine, 

855
00:41:28,020 --> 00:41:34,230
find all the entries here where the first 3 bits are not zero. 

856
00:41:34,940 --> 00:41:36,960
Therefore, we know we can throw them out immediately.

857
00:41:38,990 --> 00:41:39,080
Right? 

858
00:41:39,250 --> 00:41:42,640
Again, this is we can go in using vectors instructions.

859
00:41:42,950 --> 00:41:46,010
We can do this evaluation very quickly across these bits. 

860
00:41:46,900 --> 00:41:49,500
And then we can sort of keep marching along, going down to the end.

861
00:41:49,510 --> 00:41:52,730
And at some point, we realized there's recently and we're done,

862
00:41:53,070 --> 00:41:55,220
or there's no matches having a stop early. 

863
00:41:57,320 --> 00:41:57,450
Right? 

864
00:41:57,460 --> 00:42:01,810
So this is a different way to think about how to evaluate tuples. 

865
00:42:01,820 --> 00:42:03,670
Then, like, here's for loop,

866
00:42:03,680 --> 00:42:05,510
for every single tubal apply a predicate. 

867
00:42:05,890 --> 00:42:08,280
Now we're looking a batch of tuples all at once, 

868
00:42:08,850 --> 00:42:13,530
while we're doing this by bit across all of them. 

869
00:42:18,990 --> 00:42:20,810
So bit slices can be, again,

870
00:42:20,820 --> 00:42:22,910
are used for efficient aggregate computations, 

871
00:42:24,000 --> 00:42:25,290
like a really simple trick. 

872
00:42:25,750 --> 00:42:27,560
If you want to do a summation on a column, 

873
00:42:27,570 --> 00:42:29,140
you can use the hamming weight, 

874
00:42:29,150 --> 00:42:31,310
which is just the or hamming distance. 

875
00:42:31,320 --> 00:42:33,110
It's the number of ones, 

876
00:42:33,120 --> 00:42:37,820
the number of you count the number of non 0 bits, 

877
00:42:37,830 --> 00:42:40,510
which is one number of ones within a string of bits. 

878
00:42:42,490 --> 00:42:46,590
You walk along the slice and then count all the number ones in the first slice, 

879
00:42:47,180 --> 00:42:49,240
and then multiply by two to the 17, right?

880
00:42:49,250 --> 00:42:50,280
Because that's their position. 

881
00:42:50,900 --> 00:42:52,090
In integer form, 

882
00:42:52,430 --> 00:42:55,560
then you count all the ones in the next slice, multiply 2 to 16,

883
00:42:55,570 --> 00:42:56,040
and you do this. 

884
00:42:56,050 --> 00:42:58,470
All of them go going across 1 by 1. 

885
00:42:58,800 --> 00:42:59,730
And you add those together, 

886
00:43:00,340 --> 00:43:01,200
you get the summation. 

887
00:43:02,740 --> 00:43:04,910
And intel added this pot, count instruction,

888
00:43:04,920 --> 00:43:06,270
which basically gives you the hamming weight. 

889
00:43:06,830 --> 00:43:09,300
You can do this with cindy very efficiently again, 

890
00:43:09,310 --> 00:43:12,360
instead of having to maintain an integer sum, 

891
00:43:12,370 --> 00:43:14,800
and then having a four lip looking at every single tube one by one, 

892
00:43:14,810 --> 00:43:16,610
you go across these bit slices. 

893
00:43:16,970 --> 00:43:18,000
Do this simple math trick? 

894
00:43:18,880 --> 00:43:20,230
You end up with aggregation very quickly. 

895
00:43:20,240 --> 00:43:23,210
All right? 

896
00:43:26,220 --> 00:43:27,780
So for aggregation, 

897
00:43:27,790 --> 00:43:28,900
sure this always works. 

898
00:43:28,910 --> 00:43:31,020
The challenge course is going back here. 

899
00:43:32,210 --> 00:43:32,620
For simplicity, 

900
00:43:32,630 --> 00:43:36,300
I said skip any entry where the first three slices have a one in it. 

901
00:43:36,810 --> 00:43:40,300
And I was sort of hand waving how quickly we found that out and how we

902
00:43:40,310 --> 00:43:42,020
actually go from down the line. 

903
00:43:42,610 --> 00:43:43,760
It's not that efficient. 

904
00:43:45,870 --> 00:43:47,980
If you have to look at all the bit slices, 

905
00:43:48,210 --> 00:43:50,090
if you can throw everything out in the first chunk, 

906
00:43:50,100 --> 00:43:51,490
the first bit slice is fantastic. 

907
00:43:51,780 --> 00:43:52,320
That's super fast. 

908
00:43:53,210 --> 00:43:54,430
But for larger values, again,

909
00:43:54,440 --> 00:43:55,510
more than 17 bits, 

910
00:43:57,450 --> 00:43:58,480
this can get expensive. 

911
00:43:59,820 --> 00:44:00,780
Again, you fall back to it.

912
00:44:00,790 --> 00:44:02,580
Can I just scan the data that would be quicker. 

913
00:44:06,170 --> 00:44:07,480
So any questions about bit slicing? 

914
00:44:10,920 --> 00:44:12,110
How big a word is? 

915
00:44:12,810 --> 00:44:15,530
You still have to have the full word within the cpu register

916
00:44:15,540 --> 00:44:18,190
and can only be within the first few bit. 

917
00:44:18,200 --> 00:44:22,280
So would you really say much because the majority of your cost

918
00:44:22,290 --> 00:44:24,790
in the database goes in terms of this diagram, 

919
00:44:25,440 --> 00:44:28,750
you still have to bring the whole word in to the memory to the cache. 

920
00:44:29,320 --> 00:44:30,590
His statement is, 

921
00:44:34,480 --> 00:44:36,990
statement is, how much is this actually going to save us?

922
00:44:37,000 --> 00:44:38,540
Because basically, 

923
00:44:38,550 --> 00:44:39,620
at the end of the day, 

924
00:44:39,630 --> 00:44:41,060
if you have to go get the disk, 

925
00:44:41,070 --> 00:44:43,040
isn't disk always going to be the main bottleneck? 

926
00:44:43,500 --> 00:44:43,840
Absolutely. 

927
00:44:45,220 --> 00:44:47,190
This is sort of assuming once it's in memory, 

928
00:44:47,770 --> 00:44:53,200
can you quickly rip through the the data that is in memory

929
00:44:53,210 --> 00:44:54,400
without having to go

930
00:44:55,800 --> 00:44:59,110
without having to go one tuple by tuple or attribute and attribute, 

931
00:45:01,300 --> 00:45:02,280
or no way to think about this, too.

932
00:45:02,290 --> 00:45:04,360
Again, we're using this to do data pruning.

933
00:45:04,370 --> 00:45:08,470
So going back to that parquet diagram, 

934
00:45:08,710 --> 00:45:11,340
they have the header has all this information. 

935
00:45:11,780 --> 00:45:13,380
So if I just bring that in, 

936
00:45:14,070 --> 00:45:14,750
then I can identify, 

937
00:45:15,000 --> 00:45:19,460
I don't need to read anything else because I had these bit sliced indexes filters. 

938
00:45:19,810 --> 00:45:23,290
Then I end up reading less data in total, and that's great.

939
00:45:25,630 --> 00:45:25,950
Right? 

940
00:45:26,340 --> 00:45:27,450
And then you can play the trick of like, 

941
00:45:29,550 --> 00:45:29,930
doesn't it mean? 

942
00:45:30,160 --> 00:45:31,470
I if I check this index, 

943
00:45:31,910 --> 00:45:34,600
and then I do have to find something that I got to stall and go fetch why I

944
00:45:34,610 --> 00:45:34,900
get it. 

945
00:45:36,150 --> 00:45:37,580
If you have multithreading, 

946
00:45:37,960 --> 00:45:39,310
you could bring one piece in, 

947
00:45:40,150 --> 00:45:42,980
have one thread chop through this, 

948
00:45:43,300 --> 00:45:45,750
while something else is waiting to bring the next piece in and staging that, 

949
00:45:45,760 --> 00:45:45,940
right? 

950
00:45:46,540 --> 00:45:49,010
There's always index to look at if you then have to hand something up, 

951
00:45:49,020 --> 00:45:50,810
say I do need to read rather this data. 

952
00:45:51,680 --> 00:45:53,430
You can block that thread and run something else. 

953
00:46:04,040 --> 00:46:05,310
Let's talk about bit weaving bit. 

954
00:46:05,320 --> 00:46:10,550
Weaving is a modern incarnation of a bit slicing. 

955
00:46:11,280 --> 00:46:18,930
And it's specifically designed for doing fast evaluation on compressed

956
00:46:18,940 --> 00:46:23,050
columnar data using sim d is everyone here taking six, 1814?

957
00:46:23,060 --> 00:46:24,290
Does everyone know what cindy is? 

958
00:46:25,950 --> 00:46:26,910
Who doesn't know what cindy is? 

959
00:46:28,880 --> 00:46:29,740
Awesome, fantastic.

960
00:46:30,510 --> 00:46:31,460
First time. 

961
00:46:34,510 --> 00:46:34,650
Right? 

962
00:46:35,610 --> 00:46:38,250
We'll ignore how we're doing order preserving dictionary encoding

963
00:46:38,260 --> 00:46:39,170
until next class. 

964
00:46:39,510 --> 00:46:40,440
But basically, what we're doing again,

965
00:46:40,450 --> 00:46:47,520
we're trying to get bit level paralysation where we're trying to scan lots

966
00:46:47,530 --> 00:46:48,300
of data, 

967
00:46:48,310 --> 00:46:53,070
a a lot of attributes at a bit by bit level, 

968
00:46:54,740 --> 00:46:58,170
and be able to identify quickly whether something's going to match

969
00:46:58,180 --> 00:46:58,850
our predicate or not. 

970
00:47:01,090 --> 00:47:04,210
So this idea first came from university of wisconsin, 

971
00:47:04,220 --> 00:47:07,220
and it was implemented in a olap. 

972
00:47:07,470 --> 00:47:08,990
A better lab engine called quick step. 

973
00:47:09,310 --> 00:47:11,880
Think of like duct db before duct db which isn't called quick step, 

974
00:47:11,890 --> 00:47:14,520
but it didn't have sql, didn't have a parser, didn't have a query planar.

975
00:47:15,170 --> 00:47:17,560
It was like rockdb, but for analytics,

976
00:47:17,770 --> 00:47:19,430
but without boarding sequel, 

977
00:47:21,000 --> 00:47:22,100
they rolled us at it was constant, 

978
00:47:22,110 --> 00:47:23,940
became an incubator project for apache, 

979
00:47:23,950 --> 00:47:24,980
but then in 2016, 

980
00:47:24,990 --> 00:47:27,300
but then it got killed off in 2018. 

981
00:47:27,310 --> 00:47:28,300
I forget what happened. 

982
00:47:29,070 --> 00:47:30,420
He went up and he did a startup. 

983
00:47:32,350 --> 00:47:35,630
So this is invented by jeannox, patel.

984
00:47:36,000 --> 00:47:38,020
Again, he's probably along with the germans,

985
00:47:38,320 --> 00:47:39,470
thomas norman and germany. 

986
00:47:39,960 --> 00:47:42,490
Probably one of the best data systems researchers in the world. 

987
00:47:45,060 --> 00:47:46,540
He's really cutthroat too. 

988
00:47:46,550 --> 00:47:48,540
Like every time I showed this picture, 

989
00:47:48,550 --> 00:47:51,060
I always say like he would tell me stories about him growing up in india

990
00:47:51,070 --> 00:47:55,210
where he had to like getting on the bus just go to school every day it was

991
00:47:55,220 --> 00:47:56,990
really but he's a super mild mannered guy. 

992
00:47:57,000 --> 00:47:58,590
It was insane when he was telling me anyway. 

993
00:48:00,460 --> 00:48:00,740
All right. 

994
00:48:00,750 --> 00:48:02,450
So there'd be two ways to do this. 

995
00:48:02,460 --> 00:48:02,770
And again, 

996
00:48:04,850 --> 00:48:06,240
this was in the sketch paper you guys read. 

997
00:48:06,250 --> 00:48:09,090
I think they allude to and talk about bit weaving, 

998
00:48:09,100 --> 00:48:10,410
but only the vertical approach. 

999
00:48:11,080 --> 00:48:12,510
So we'll talk about both of them, 

1000
00:48:12,960 --> 00:48:15,590
and you'll see why the vertical one is going to be superior. 

1001
00:48:15,980 --> 00:48:17,880
The other thing I also forgot to point out, too,

1002
00:48:17,890 --> 00:48:22,310
is this paper came out in 2013. 

1003
00:48:24,560 --> 00:48:26,530
The time, cindy,

1004
00:48:26,540 --> 00:48:27,570
at least on x 86, 

1005
00:48:27,980 --> 00:48:29,660
didn't have scattering gather operations. 

1006
00:48:30,040 --> 00:48:36,820
They're going to be able to get all the parallelism with vectorization

1007
00:48:37,130 --> 00:48:38,960
in some cases without cindy entirely. 

1008
00:48:39,350 --> 00:48:40,760
But also if they are using cindy, 

1009
00:48:40,770 --> 00:48:42,200
they don't require scattering gather. 

1010
00:48:43,620 --> 00:48:44,890
Because again, now we have it,

1011
00:48:44,900 --> 00:48:48,400
but back then there are the modern versions that do it, 

1012
00:48:48,410 --> 00:48:50,300
take advantage of it, but it doesn't require it.

1013
00:48:50,950 --> 00:48:52,020
We'll cover, scatter, gather,

1014
00:48:52,030 --> 00:48:54,450
and stevie stuff in more detail later in the semester. 

1015
00:48:56,810 --> 00:48:58,120
Again, we're doing this.

1016
00:49:00,330 --> 00:49:01,970
When we talk about the horizontal approach first, 

1017
00:49:02,530 --> 00:49:04,910
it's gonna be row based, but again, think of it.

1018
00:49:04,920 --> 00:49:06,810
It's it's for a single column, 

1019
00:49:06,940 --> 00:49:12,980
even though we're storing the bits within a value in a row oriented manner. 

1020
00:49:13,470 --> 00:49:15,030
It's assumed that it's on a single column, 

1021
00:49:15,040 --> 00:49:17,710
and the column will be stored in a column in our fashion, 

1022
00:49:17,720 --> 00:49:19,600
the attributes we stored in the column in our fashion. 

1023
00:49:21,800 --> 00:49:21,990
Again, 

1024
00:49:22,120 --> 00:49:25,350
it's a clever way to allow us to get better parallelism through vectors asian, 

1025
00:49:25,360 --> 00:49:26,430
because we're organizing bits. 

1026
00:49:28,330 --> 00:49:28,580
Right? 

1027
00:49:28,590 --> 00:49:31,350
So say we have our tuples here. 

1028
00:49:33,470 --> 00:49:35,620
This is just the binary form or whatever the storing. 

1029
00:49:35,790 --> 00:49:39,300
Here's the actual the numeric value that's being represented by all of these. 

1030
00:49:40,300 --> 00:49:42,370
We're going to break it up into segments. 

1031
00:49:43,860 --> 00:49:45,960
Think of these again, just like roe groups, as we had before.

1032
00:49:47,020 --> 00:49:49,130
For each segment, we're going to store.

1033
00:49:51,550 --> 00:49:53,970
We would know how many values we need to store. 

1034
00:49:54,740 --> 00:49:57,090
We're going to represent them in sort of this, 

1035
00:49:57,830 --> 00:50:04,140
a in a weaved manner where we were stored in the continuous memory

1036
00:50:04,150 --> 00:50:05,700
will be have gaps where we

1037
00:50:05,710 --> 00:50:08,060
would come back around and get the next batch of tuples. 

1038
00:50:08,820 --> 00:50:09,910
So let me show you what I mean by this. 

1039
00:50:10,760 --> 00:50:11,980
So think of this, 

1040
00:50:11,990 --> 00:50:15,090
as like this is the start of memory and goes here from here to here. 

1041
00:50:15,810 --> 00:50:19,080
We're going to represent these 3 bit values in 4 bits. 

1042
00:50:19,640 --> 00:50:21,100
You actually have the actual value. 

1043
00:50:21,430 --> 00:50:23,760
The first one is 0010 is a one here. 

1044
00:50:24,080 --> 00:50:26,150
But there's always going to be a delimiter that's in front of it. 

1045
00:50:26,730 --> 00:50:29,130
That's going to be aa padding bit that's always set to zero. 

1046
00:50:29,900 --> 00:50:31,800
And we're going to use this to then, 

1047
00:50:32,740 --> 00:50:35,580
when you start doing the arithmetic to figure out whether something that

1048
00:50:35,590 --> 00:50:36,300
matches the predicate, 

1049
00:50:36,630 --> 00:50:39,350
they're going to use this to store whether the triple matches

1050
00:50:39,360 --> 00:50:41,170
or not in the output. 

1051
00:50:41,180 --> 00:50:42,530
You see what I mean in a second. 

1052
00:50:42,540 --> 00:50:44,370
And the other thing to point out also, too,

1053
00:50:44,380 --> 00:50:48,700
is that the way it works is we go from top to bottom t this is t zero, 

1054
00:50:48,710 --> 00:50:50,260
t one, t two, t three.

1055
00:50:50,840 --> 00:50:53,960
Then we loop back around and then append t four to where t one is the same

1056
00:50:53,970 --> 00:50:55,100
with t five and so forth. 

1057
00:50:55,110 --> 00:50:55,290
Here. 

1058
00:50:56,840 --> 00:50:58,030
We'll do this when we start, 

1059
00:50:58,270 --> 00:51:02,110
because we need to get convert out or extract out the matching tubes

1060
00:51:02,120 --> 00:51:04,390
into a selection vector to know what all sets match. 

1061
00:51:04,820 --> 00:51:07,520
If we organize it in this way, it'll pop out nicely.

1062
00:51:07,860 --> 00:51:09,180
You'll see what I mean in a second. 

1063
00:51:11,320 --> 00:51:13,830
Then the same thing for the second segment, we know there's two values.

1064
00:51:13,840 --> 00:51:16,770
We only have to store222 poles, 

1065
00:51:17,090 --> 00:51:18,270
and there's the pen in one after another. 

1066
00:51:20,470 --> 00:51:22,920
I'm showing this with 3 bit values, 

1067
00:51:23,170 --> 00:51:24,400
with shortest 4 bits. 

1068
00:51:25,030 --> 00:51:26,660
We'll say this is a process of word. 

1069
00:51:28,300 --> 00:51:29,810
It's just 8 bits on x 86. 

1070
00:51:29,820 --> 00:51:30,530
I process a word. 

1071
00:51:30,540 --> 00:51:32,370
I think it's 16 bits, because that's what it was.

1072
00:51:32,960 --> 00:51:35,240
Back in the 80s, I think arm is 32 bits.

1073
00:51:36,320 --> 00:51:38,810
It doesn't matter to, for simplicity, we're using this.

1074
00:51:43,730 --> 00:51:43,900
Here. 

1075
00:51:43,910 --> 00:51:45,520
Here's the little emitter that's always set to zero. 

1076
00:51:46,940 --> 00:51:48,440
Let's see how we might use these to match a predicate. 

1077
00:51:49,600 --> 00:51:51,880
Select start from table where value is less than five. 

1078
00:51:52,860 --> 00:51:57,140
We're just going to deal with just two tuples like t one and t four I that I

1079
00:51:57,150 --> 00:51:58,660
showed before going back or t zero, 

1080
00:51:58,670 --> 00:51:58,980
t four. 

1081
00:51:58,990 --> 00:52:00,420
We're going to deal with this first one here. 

1082
00:52:02,230 --> 00:52:06,100
The x is going to be the actual triple organized in the bit, 

1083
00:52:06,110 --> 00:52:07,820
weaving horizontal format. 

1084
00:52:08,360 --> 00:52:10,450
Then this y vector here, 

1085
00:52:10,690 --> 00:52:13,970
that's just going to be the constant that we're comparing five. 

1086
00:52:16,290 --> 00:52:19,000
Then there'll be this mass to just zero with a bunch of ones. 

1087
00:52:19,130 --> 00:52:24,050
We're going to use that when we do our when we do our arithmetic to figure

1088
00:52:24,060 --> 00:52:24,970
out whether this thing, 

1089
00:52:24,980 --> 00:52:28,590
whether the value is less than the concept we're looking for. 

1090
00:52:30,160 --> 00:52:32,220
The way it works is that we have this formula here. 

1091
00:52:32,740 --> 00:52:35,390
We'll have a different formula per based on what the predicate is. 

1092
00:52:35,520 --> 00:52:39,120
This is for less than you take the x core of the x and the mask, 

1093
00:52:39,620 --> 00:52:42,250
add y to it and then end it with a negation of the mask. 

1094
00:52:43,430 --> 00:52:44,100
Lo and behold, 

1095
00:52:44,110 --> 00:52:46,890
what you get as the output is a selection vector that tells you

1096
00:52:49,430 --> 00:52:50,910
whether the 2 point actually matches. 

1097
00:52:51,830 --> 00:52:53,540
Now, you see in the padding here,

1098
00:52:53,850 --> 00:52:57,080
that's where you put, whether the predicate actually was satisfied or not.

1099
00:52:57,980 --> 00:52:58,920
You ignore the zeros. 

1100
00:52:58,930 --> 00:53:00,000
The padding here says one, 

1101
00:53:00,010 --> 00:53:01,910
because t zero is what? 

1102
00:53:01,920 --> 00:53:04,190
One and one is less than five, so that matches.

1103
00:53:05,030 --> 00:53:10,060
And then the second t four is 65 is less than six. 

1104
00:53:10,850 --> 00:53:11,220
That's false. 

1105
00:53:11,670 --> 00:53:12,410
That's set to zero. 

1106
00:53:15,820 --> 00:53:18,000
What's really cool about this is that it only requires three instructions

1107
00:53:18,010 --> 00:53:19,040
to evaluate a single word. 

1108
00:53:19,050 --> 00:53:20,970
It works with any word size. 

1109
00:53:20,980 --> 00:53:22,530
As I said, these are apite.

1110
00:53:22,540 --> 00:53:23,370
Words can go larger. 

1111
00:53:23,380 --> 00:53:24,010
It doesn't matter. 

1112
00:53:24,700 --> 00:53:26,820
And all these different formulas here, 

1113
00:53:26,830 --> 00:53:28,860
they're all defined in the paper. 

1114
00:53:28,870 --> 00:53:30,360
They actually didn't invent these. 

1115
00:53:31,660 --> 00:53:31,980
Actually, 

1116
00:53:31,990 --> 00:53:34,940
leslie lamport event of these back in the 1970s to do this kind

1117
00:53:34,950 --> 00:53:36,340
of these bit wise comparisons. 

1118
00:53:37,250 --> 00:53:40,170
So again, nothing i'm showing here is actually, SIMD,

1119
00:53:41,210 --> 00:53:43,090
this is just doing this mask here. 

1120
00:53:43,380 --> 00:53:44,870
You can use regular 60instructions, 

1121
00:53:46,250 --> 00:53:50,660
but you're still getting the parallelism because you pack things to fit a

1122
00:53:50,670 --> 00:53:51,500
in a single word. 

1123
00:53:53,260 --> 00:53:53,670
Right? 

1124
00:53:55,390 --> 00:53:55,980
This is pretty cool. 

1125
00:53:55,990 --> 00:53:57,580
Again, the1970s, SIMD,

1126
00:53:57,590 --> 00:54:01,160
I think SIMD existed theoretically with flints taxonomy. 

1127
00:54:01,430 --> 00:54:03,500
But I don't think any cpu back in the 70s supported this, 

1128
00:54:03,760 --> 00:54:06,520
at least not to the extent they do now. 

1129
00:54:08,610 --> 00:54:10,800
So again, 

1130
00:54:10,810 --> 00:54:13,640
i'm showing you 8 bit words if you had 64 bit words. 

1131
00:54:13,940 --> 00:54:16,050
Now you can pack in 16 free byte values, 

1132
00:54:16,060 --> 00:54:20,470
and you can do this simple arithmetic to compute the comparison very quickly. 

1133
00:54:25,740 --> 00:54:26,170
The question is, 

1134
00:54:26,180 --> 00:54:28,410
why do they arrange vertically instead of horizontally next slide? 

1135
00:54:28,420 --> 00:54:30,800
Because we have to at this point here, 

1136
00:54:30,810 --> 00:54:32,760
we just have a one and a zero to say this match. 

1137
00:54:32,770 --> 00:54:35,120
We've got to convert this back into what is the offset of the tuple. 

1138
00:54:35,130 --> 00:54:37,300
And then it's raising horizontally will solve the problem. 

1139
00:54:40,640 --> 00:54:42,560
You have one less bit to represent all of your data. 

1140
00:54:42,570 --> 00:54:43,560
Say this does this mean? 

1141
00:54:43,570 --> 00:54:45,560
You have one less bit to represent all your data, 

1142
00:54:47,570 --> 00:54:50,570
but normally, the americans does the quest 32bits.

1143
00:54:50,580 --> 00:54:54,470
Now, it can't be put here because this would only be the 31bit.

1144
00:54:54,730 --> 00:54:56,390
If you had random third cube, 

1145
00:54:56,400 --> 00:55:01,590
it ended as wouldn't this actually not work? 

1146
00:55:04,110 --> 00:55:05,820
You'd have to know something about the value domain to say, 

1147
00:55:06,110 --> 00:55:07,180
what's the minimum you can do? 

1148
00:55:07,760 --> 00:55:07,770
Right? 

1149
00:55:08,070 --> 00:55:11,250
This is why they're assuming they're dictionary encoded in order preserving. 

1150
00:55:12,130 --> 00:55:13,480
So again, these are a bunch of,

1151
00:55:14,280 --> 00:55:15,920
say, the state codes in the united states.

1152
00:55:16,350 --> 00:55:17,840
You could represent that in these bits. 

1153
00:55:19,770 --> 00:55:21,920
Let me plow ahead and get through to answer her question. 

1154
00:55:23,610 --> 00:55:28,220
Yeah, these all are bit weaved packed.

1155
00:55:29,120 --> 00:55:29,630
I don't say packs. 

1156
00:55:29,640 --> 00:55:30,790
That means that's compression scheme, 

1157
00:55:30,800 --> 00:55:32,950
but these are bit maps. 

1158
00:55:34,020 --> 00:55:37,250
We can just do all the same comparison that I showed in the last slide. 

1159
00:55:37,260 --> 00:55:39,260
And we have a bunch of vectors like this. 

1160
00:55:39,610 --> 00:55:40,630
Again, in our delimiter,

1161
00:55:40,640 --> 00:55:42,590
we have set 0to1 based on whether it matched. 

1162
00:55:43,740 --> 00:55:45,530
But now we need to convert this back into, 

1163
00:55:46,460 --> 00:55:48,210
again, whether what offset matched.

1164
00:55:49,210 --> 00:55:52,990
You just do simple bit shifting to slide things over based on where you are

1165
00:55:55,950 --> 00:55:56,740
in the list. 

1166
00:55:57,710 --> 00:56:00,250
Then now when you combine these together, 

1167
00:56:00,570 --> 00:56:03,830
then you end up with a bit map that says what tuple actually matched. 

1168
00:56:05,070 --> 00:56:08,750
That's why you have to shift this t what t four was over here. 

1169
00:56:09,370 --> 00:56:10,770
When I shifted it over by one, 

1170
00:56:11,030 --> 00:56:19,170
when I push it up, it lands nicely where t four should be.

1171
00:56:20,640 --> 00:56:21,340
Is it more efficient? 

1172
00:56:21,870 --> 00:56:25,120
A statement isn't the one we saw before more I the higher co encoding

1173
00:56:25,130 --> 00:56:28,160
or a bit slicing? 

1174
00:56:29,940 --> 00:56:39,340
Is that more efficient for facing the word and the thing to do in the operation? 

1175
00:56:39,850 --> 00:56:43,340
But not just do you have to face the way? 

1176
00:56:44,280 --> 00:56:44,570
Is it? 

1177
00:56:47,310 --> 00:56:52,510
The storage of the bit slicing approach is potentially more efficient. 

1178
00:56:52,520 --> 00:56:54,660
I because our modeling is obviously, 

1179
00:56:56,760 --> 00:56:57,800
yes, I want to reduce.

1180
00:57:00,110 --> 00:57:06,240
Yes, but this bit is not another bit,

1181
00:57:06,250 --> 00:57:07,400
but we're going to face the data as well. 

1182
00:57:08,830 --> 00:57:10,430
Even if the data is a little bit small, 

1183
00:57:10,440 --> 00:57:14,500
and it doesn't matter whether you're catching the 32 bits of the data as well. 

1184
00:57:15,130 --> 00:57:17,850
Whereas in bit splicing, you're only fetching 1 bit for every 32bit.

1185
00:57:17,860 --> 00:57:19,030
You understand? 

1186
00:57:21,020 --> 00:57:22,740
Ii had to look at the original paper. 

1187
00:57:22,750 --> 00:57:25,800
I forget whether think it might have compared against bit slicing, 

1188
00:57:25,810 --> 00:57:26,840
and this is way faster. 

1189
00:57:28,080 --> 00:57:28,620
This is faster. 

1190
00:57:28,630 --> 00:57:28,940
Yes. 

1191
00:57:29,740 --> 00:57:31,530
Actually, the vertical one, we'll see next.

1192
00:57:31,540 --> 00:57:32,450
That's actually even faster. 

1193
00:57:34,000 --> 00:57:36,550
That answers your question like why you shift it over and you pop it up. 

1194
00:57:36,560 --> 00:57:38,350
And then that's why they do they arrange it that way. 

1195
00:57:40,780 --> 00:57:42,170
This is called the selection vector. 

1196
00:57:42,500 --> 00:57:48,370
This basically is a bit math that says that is the value for the does

1197
00:57:48,620 --> 00:57:49,490
the tube of this offset? 

1198
00:57:49,500 --> 00:57:50,770
Did this match our predicate or not? 

1199
00:57:51,540 --> 00:57:52,660
But again, it's just a bit map.

1200
00:57:52,670 --> 00:57:55,140
We've got to convert that back actually to numerical offsets. 

1201
00:57:56,340 --> 00:57:56,710
Right? 

1202
00:57:58,120 --> 00:57:59,000
There's two ways to do this. 

1203
00:57:59,010 --> 00:58:02,720
One is the new real simple thing would just be take our selection vector

1204
00:58:03,370 --> 00:58:04,770
and just write a for loop and say, 

1205
00:58:04,940 --> 00:58:06,130
if the bit set to one, 

1206
00:58:06,530 --> 00:58:08,360
then I know this is the two ball at this offset. 

1207
00:58:08,490 --> 00:58:11,350
I then I need to consider it'll work. 

1208
00:58:14,000 --> 00:58:19,250
So the better approach is to do a precomputed positions table where you

1209
00:58:19,260 --> 00:58:21,490
recognize the size of my bit map. 

1210
00:58:21,500 --> 00:58:22,330
Isn't that big? 

1211
00:58:23,080 --> 00:58:25,140
In this case here, it's 8bits.

1212
00:58:25,810 --> 00:58:28,950
That means that there's only two to the eight possible combinations

1213
00:58:28,960 --> 00:58:32,950
of offsets that could be in this represented by this bit map. 

1214
00:58:33,810 --> 00:58:38,750
I can just pre compute that as a giant array where the offset

1215
00:58:38,760 --> 00:58:42,310
of the integer of the integer representation of the bits corresponds

1216
00:58:42,320 --> 00:58:43,270
to an offset position table. 

1217
00:58:43,820 --> 00:58:46,210
Then I have a pre computed vector of the two plus that match. 

1218
00:58:47,720 --> 00:58:48,670
This is150. 

1219
00:58:48,680 --> 00:58:50,270
I jump to one position, 150.

1220
00:58:50,600 --> 00:58:52,130
And there's my offsets. 

1221
00:58:53,690 --> 00:58:56,610
And I can keep that this positions table, 

1222
00:58:57,850 --> 00:58:58,920
it's in a low megs. 

1223
00:59:00,010 --> 00:59:02,440
If that I keep this in ll two or l one, 

1224
00:59:04,960 --> 00:59:10,050
this technique was amended by vector wise in the late2000s. 

1225
00:59:10,060 --> 00:59:13,050
And then we end up using this in our own system for our research

1226
00:59:13,060 --> 00:59:14,610
on vectors and execution stuff. 

1227
00:59:15,220 --> 00:59:16,890
Paper papers will cover a later semester. 

1228
00:59:18,780 --> 00:59:18,820
All right. 

1229
00:59:18,910 --> 00:59:19,660
This is the horizontal. 

1230
00:59:28,740 --> 00:59:28,790
Sorry, 

1231
00:59:31,460 --> 00:59:33,000
that's already in the register, right?

1232
00:59:34,210 --> 00:59:36,570
But iii got to go examine the bits. 

1233
00:59:38,360 --> 00:59:42,230
There isn't an instruction that says convert these bits into the integers. 

1234
00:59:44,720 --> 00:59:46,380
You just pre compute this little small table. 

1235
00:59:46,730 --> 00:59:47,320
This is way faster. 

1236
00:59:53,330 --> 00:59:54,760
This is the horizontal representation. 

1237
00:59:56,620 --> 00:59:59,640
I don't, I don't know whether they actually implemented this in quick step.

1238
01:00:00,060 --> 01:00:01,410
No, the system I know, actually does this.

1239
01:00:01,730 --> 01:00:04,500
I just find this like super cool and super clever like the things you can do. 

1240
01:00:05,380 --> 01:00:07,970
The vertical one is the one that showed up in the sketch paper. 

1241
01:00:07,980 --> 01:00:10,840
You guys read same thing as before. 

1242
01:00:10,850 --> 01:00:14,520
We're going to split our tuples up into segments or row groups. 

1243
01:00:15,210 --> 01:00:18,770
But now what we're going to do is going to journey bit slices

1244
01:00:19,370 --> 01:00:23,470
for all the positions and store those continuously in memory. 

1245
01:00:24,430 --> 01:00:24,670
Right? 

1246
01:00:25,160 --> 01:00:28,150
Take all on the first column, on the second column, and the third column.

1247
01:00:28,610 --> 01:00:31,090
And each segment will store those bits. 

1248
01:00:33,750 --> 01:00:38,330
In this case here, even though the second segment only has 2 tuples,

1249
01:00:38,610 --> 01:00:43,180
we had to store the entire bit map with much useless data, 

1250
01:00:43,190 --> 01:00:46,150
because we had to make sure that everything fits in up. 

1251
01:00:46,450 --> 01:00:47,410
In our process of words, 

1252
01:00:50,920 --> 01:00:51,670
what can you do with this? 

1253
01:00:55,090 --> 01:00:58,330
When we want to start doing evaluating two balls, we can use.

1254
01:00:58,340 --> 01:00:58,930
Now, SIMD,

1255
01:00:59,610 --> 01:01:01,610
to compare tuples, i'm sorry,

1256
01:01:01,620 --> 01:01:07,870
to compare bits on a bit by bit basis within the value that we're looking for. 

1257
01:01:09,070 --> 01:01:11,860
Right along in sydney registers without having go to back

1258
01:01:11,870 --> 01:01:15,530
to regular cpu registers and compute our predicates very efficiently. 

1259
01:01:16,550 --> 01:01:19,300
Say we have now select star from table with value equals two. 

1260
01:01:19,310 --> 01:01:21,460
I'm showing over the inequality predicate, 

1261
01:01:22,100 --> 01:01:24,580
just for simplicity, but you can use inequalities as well.

1262
01:01:25,650 --> 01:01:28,440
For this one, we're going to represent it in binary form, 010.

1263
01:01:29,600 --> 01:01:32,230
We want to do an evaluation of the first bit, 

1264
01:01:32,940 --> 01:01:36,960
find all the matches within our first bit map vector. 

1265
01:01:38,110 --> 01:01:38,940
In our first slice, 

1266
01:01:40,480 --> 01:01:42,630
we throw out this empty zero mass thing. 

1267
01:01:42,930 --> 01:01:45,250
We do our sympathy compare to see whether it's true. 

1268
01:01:45,260 --> 01:01:46,650
And then we get back, 

1269
01:01:47,330 --> 01:01:49,020
we get better a selected vector like this. 

1270
01:01:50,040 --> 01:01:50,240
Right? 

1271
01:01:51,440 --> 01:01:53,200
Then we have to evaluate the second one, 

1272
01:01:53,210 --> 01:01:57,100
but we only want to compare the ones where we know that actually match

1273
01:01:57,110 --> 01:01:57,780
the first one. 

1274
01:01:58,150 --> 01:01:59,580
It would be these positions here. 

1275
01:02:00,090 --> 01:02:02,970
So we'd bring in another mask to do that comparison. 

1276
01:02:03,420 --> 01:02:06,980
And then this would then produce out a in this case, they're all zeros.

1277
01:02:08,070 --> 01:02:10,940
We just use that pop count thing before when we look at the vector and say, 

1278
01:02:11,210 --> 01:02:12,210
how many ones are there? 

1279
01:02:12,470 --> 01:02:14,740
As soon as the vector comes out with all zeros, 

1280
01:02:15,600 --> 01:02:17,390
we know we're done and we do early pruning. 

1281
01:02:18,650 --> 01:02:18,730
Right? 

1282
01:02:18,740 --> 01:02:20,490
So this is basically bit slicing, 

1283
01:02:20,500 --> 01:02:23,860
but a way to do it with SIMD efficiently. 

1284
01:02:27,300 --> 01:02:27,700
Right? 

1285
01:02:29,620 --> 01:02:30,490
In the case, 

1286
01:02:32,420 --> 01:02:34,570
in the horizontal coding, we can't do early pruning,

1287
01:02:35,430 --> 01:02:38,090
because we had to always look at the whole tuples, 

1288
01:02:38,100 --> 01:02:41,420
the whole bits within the attribute or the value in this case here, 

1289
01:02:41,430 --> 01:02:42,540
because we slice it up, 

1290
01:02:42,890 --> 01:02:44,860
we can do early stops. 

1291
01:02:46,250 --> 01:02:47,790
Is this different from this place? 

1292
01:02:48,860 --> 01:02:51,510
It's just showing you how to do it with SIMD. 

1293
01:02:57,410 --> 01:02:57,940
It's a statement. 

1294
01:02:57,950 --> 01:03:00,660
You're doing this exact same operation as bit slicing. 

1295
01:03:01,610 --> 01:03:02,210
Yes. 

1296
01:03:02,640 --> 01:03:06,210
But like the visual bit slicing paper doesn't say how to do this. 

1297
01:03:06,220 --> 01:03:12,040
The difference because it was from the90s. 

1298
01:03:13,060 --> 01:03:13,160
Right? 

1299
01:03:13,490 --> 01:03:19,360
In this case here, we're able to examine 8 tuples with two instructions.

1300
01:03:20,300 --> 01:03:20,900
That's insane. 

1301
01:03:21,550 --> 01:03:21,990
It's amazing. 

1302
01:03:24,090 --> 01:03:28,240
Again, if you had to do a sequential scan with a for loop and evaluate tuples one by one,

1303
01:03:28,710 --> 01:03:29,890
and there were attributes on the column. 

1304
01:03:30,220 --> 01:03:32,380
You're not going to get down to it in two instructions. 

1305
01:03:32,390 --> 01:03:33,100
It's not entirely true. 

1306
01:03:33,110 --> 01:03:34,220
I it's two SIMD instructions, 

1307
01:03:34,230 --> 01:03:36,260
but there's instructions to get things in and out of the registers. 

1308
01:03:36,270 --> 01:03:37,020
You pay that cost. 

1309
01:03:37,720 --> 01:03:38,720
But we're ignoring that for now, 

1310
01:03:42,190 --> 01:03:42,260
right? 

1311
01:03:42,270 --> 01:03:42,620
That's a lot. 

1312
01:03:42,870 --> 01:03:44,020
Where have we gone so far? 

1313
01:03:44,030 --> 01:03:44,940
We've done zone maps bit. 

1314
01:03:44,950 --> 01:03:46,660
Map index is bit slicing, bit weaving.

1315
01:03:47,290 --> 01:03:49,630
Then we'll finish up quickly and talk about column imprints

1316
01:03:50,160 --> 01:03:56,420
and column sketches all the bit map schemes. 

1317
01:03:56,430 --> 01:04:02,100
I showed you so far about storing exact representations of the data

1318
01:04:02,110 --> 01:04:02,820
within the columns. 

1319
01:04:03,710 --> 01:04:04,310
Meaning, again,

1320
01:04:04,320 --> 01:04:10,720
I think the simplest case of the basic bit map index with the quality coding, 

1321
01:04:11,120 --> 01:04:12,690
there's a one whether, in a bit map,

1322
01:04:12,700 --> 01:04:14,450
whether the value is equivalent or not. 

1323
01:04:16,120 --> 01:04:18,160
Now, the range encoding, you're kind of grouping things together.

1324
01:04:18,390 --> 01:04:20,800
As I said, that's what basically sketching is trying to do.

1325
01:04:21,100 --> 01:04:22,030
But in a different way. 

1326
01:04:22,720 --> 01:04:25,590
But in general, it's ignoring region coding.

1327
01:04:25,600 --> 01:04:27,200
If I could check a bit map, 

1328
01:04:27,730 --> 01:04:30,470
and it says something it matches, I know it matches.

1329
01:04:30,480 --> 01:04:34,070
And i'll have to go double check on the actual tuple and make sure I

1330
01:04:34,080 --> 01:04:34,910
don't have false positives. 

1331
01:04:36,200 --> 01:04:38,500
But in some cases, 

1332
01:04:38,980 --> 01:04:44,310
if we give up some of this accuracy to support faster evaluation

1333
01:04:44,320 --> 01:04:47,020
and maybe more compact bit maps, 

1334
01:04:47,770 --> 01:04:50,040
then that actually might be a big win for the common case. 

1335
01:04:51,100 --> 01:04:53,480
Again, there's extreme examples as we talked about.

1336
01:04:53,790 --> 01:04:55,710
Those may not work perfectly for this, 

1337
01:04:56,120 --> 01:04:57,310
but most data doesn't look like that. 

1338
01:04:57,850 --> 01:04:57,960
Right? 

1339
01:04:57,970 --> 01:04:59,280
Most data is highly skewed. 

1340
01:05:01,050 --> 01:05:01,400
Again, 

1341
01:05:01,730 --> 01:05:04,650
we may have to check original data in some cases to make

1342
01:05:04,660 --> 01:05:05,700
sure there's no false positives. 

1343
01:05:05,710 --> 01:05:07,590
And the case of the range encoding example, 

1344
01:05:07,910 --> 01:05:09,020
I always had to go check. 

1345
01:05:10,100 --> 01:05:11,590
But in the sketch check, in some cases,

1346
01:05:11,600 --> 01:05:13,630
I know there will be some times where I don't have to check. 

1347
01:05:16,610 --> 01:05:22,900
Column imprints is a precursor to the column sketches. 

1348
01:05:23,400 --> 01:05:25,800
The basic idea is that we're going to have a bit map that says

1349
01:05:26,160 --> 01:05:27,520
whether something could exist. 

1350
01:05:28,540 --> 01:05:31,860
It's basically collapsing down the bit slices into a single bit map

1351
01:05:31,870 --> 01:05:32,860
instead of multiple slices. 

1352
01:05:34,630 --> 01:05:37,220
Same original data is184. 

1353
01:05:38,180 --> 01:05:40,050
So i've had bit indexes like this. 

1354
01:05:41,370 --> 01:05:44,600
You would see that like for the different possible values that I have, 

1355
01:05:45,320 --> 01:05:45,940
there's a bunch of ones. 

1356
01:05:45,950 --> 01:05:48,460
There's a bunch of columns or a bunch of bit maps that are zeros. 

1357
01:05:49,970 --> 01:05:51,630
So instead, if I distort imprint,

1358
01:05:51,640 --> 01:05:55,330
I just collapse us all down by ordering everything. 

1359
01:05:56,510 --> 01:06:00,400
Then I end up with a single bit map called an imprint. 

1360
01:06:01,400 --> 01:06:04,550
I can use that to then determine whether quickly whether something exists

1361
01:06:04,560 --> 01:06:04,750
or not. 

1362
01:06:05,090 --> 01:06:07,860
And again, if I get a one here or one over here,

1363
01:06:08,160 --> 01:06:11,240
I got to go check the original tuples and see whether it's

1364
01:06:11,250 --> 01:06:12,000
actually a match or not. 

1365
01:06:12,490 --> 01:06:13,910
But if i'm landing maybe in here, 

1366
01:06:13,920 --> 01:06:14,990
I knew this is a zero. 

1367
01:06:15,390 --> 01:06:16,290
I don't have to do anything. 

1368
01:06:19,360 --> 01:06:20,250
What's that? 

1369
01:06:21,080 --> 01:06:21,540
It's like a balloon. 

1370
01:06:21,670 --> 01:06:23,220
It's the same as it's like a bloom filter. 

1371
01:06:27,060 --> 01:06:30,030
A without, I guess,

1372
01:06:30,040 --> 01:06:34,160
a any sort of false positive rate guarantees or things like that. 

1373
01:06:35,420 --> 01:06:37,780
We'll cover bloom filters much more detail when we talk about hash joins, 

1374
01:06:37,790 --> 01:06:38,940
but basically the same idea. 

1375
01:06:44,610 --> 01:06:44,990
Good. 

1376
01:06:46,730 --> 01:06:49,880
Looking at digesting glen column and print slides. 

1377
01:06:49,890 --> 01:06:50,520
Yeah. 

1378
01:06:51,980 --> 01:06:56,210
This was done and this was done in mooney to be again in the early 2000s tense. 

1379
01:06:57,510 --> 01:07:00,780
The column sketch paper you guys read is a variation, as I said,

1380
01:07:00,790 --> 01:07:02,580
a bit map range, encoded bit maps,

1381
01:07:03,210 --> 01:07:06,650
where the idea is that we want to maintain aa sketch, 

1382
01:07:07,370 --> 01:07:07,960
which is, 

1383
01:07:08,750 --> 01:07:10,940
this is like a probabilistic data structure, kind of like a bloomboat.

1384
01:07:11,110 --> 01:07:12,420
There's county sketches and things like that. 

1385
01:07:12,780 --> 01:07:15,390
But it's an approximation of what data exists. 

1386
01:07:17,510 --> 01:07:22,540
And it's gonna be the bit will be set the true to determine that there is something, 

1387
01:07:23,790 --> 01:07:25,020
it could be something in this range. 

1388
01:07:26,060 --> 01:07:31,170
The idea is that we want to convert our are sort of base values

1389
01:07:31,180 --> 01:07:32,850
into these smaller codes. 

1390
01:07:33,590 --> 01:07:37,140
And then we would use those codes to figure out how much data we

1391
01:07:37,150 --> 01:07:37,940
actually have to look at. 

1392
01:07:39,820 --> 01:07:39,890
Now, 

1393
01:07:39,900 --> 01:07:42,090
there's going to be a trade off between the distribution of the values

1394
01:07:42,100 --> 01:07:43,010
and the compactness. 

1395
01:07:43,370 --> 01:07:46,110
If our codes are the same size of the original data, 

1396
01:07:46,520 --> 01:07:48,260
then they're going to be very precise. 

1397
01:07:48,610 --> 01:07:48,900
Of course, 

1398
01:07:48,910 --> 01:07:53,830
that means that we're not going to a throw anything away and waste space

1399
01:07:53,840 --> 01:07:55,550
for maintaining using the values twice. 

1400
01:07:56,620 --> 01:08:01,790
They will have a special case for the most frequent values to avoid false positives, 

1401
01:08:01,800 --> 01:08:02,470
because otherwise, 

1402
01:08:02,480 --> 01:08:03,830
if something appears all the time, 

1403
01:08:05,660 --> 01:08:08,150
and they get sort of grouped together, you may have to go,

1404
01:08:08,240 --> 01:08:10,350
we check to see whether the thing actually exists or not. 

1405
01:08:10,950 --> 01:08:11,900
But if you have a special case, 

1406
01:08:11,910 --> 01:08:15,260
I know this frequent value appears most 99% of the time. 

1407
01:08:15,560 --> 01:08:16,880
Here is the exact value for it. 

1408
01:08:17,330 --> 01:08:21,850
I can just do that look ups and avoid having to go read things I don't need. 

1409
01:08:24,340 --> 01:08:26,010
Here's a high level overview how it works. 

1410
01:08:26,230 --> 01:08:27,480
Say we have our original data, 

1411
01:08:27,820 --> 01:08:29,700
and we're storing this as aa bit values. 

1412
01:08:31,870 --> 01:08:34,380
What they're going to do is basically take a pass over the data. 

1413
01:08:34,390 --> 01:08:34,700
Again. 

1414
01:08:34,710 --> 01:08:35,260
We can do this. 

1415
01:08:35,750 --> 01:08:37,520
If we're we're creating an immutable file, 

1416
01:08:37,530 --> 01:08:40,180
we've got to read everything in and encode it and write it out. 

1417
01:08:40,610 --> 01:08:45,160
So they can build a histogram where they want the height of the data

1418
01:08:45,170 --> 01:08:48,450
represented within a range of the histogram within a bucket to be

1419
01:08:48,460 --> 01:08:49,010
equivalent. 

1420
01:08:50,740 --> 01:08:54,640
You want the you don't have one range, 

1421
01:08:54,650 --> 01:08:55,680
have a lot of skew. 

1422
01:08:56,430 --> 01:09:00,020
And therefore, you're doing much of look ups on it for useless data.

1423
01:09:01,310 --> 01:09:03,660
It's then based on this histogram, again,

1424
01:09:03,790 --> 01:09:07,620
the paper sort of describes how you'd actually want to build it. 

1425
01:09:08,180 --> 01:09:09,580
You maintain this compression map. 

1426
01:09:10,490 --> 01:09:13,110
The idea is that you would say within this, 

1427
01:09:13,200 --> 01:09:14,550
the values over here, 

1428
01:09:14,950 --> 01:09:17,350
it's less than a greater, less than or equal to this.

1429
01:09:17,930 --> 01:09:21,920
You get mapped to some smaller dictionary code here. 

1430
01:09:23,550 --> 01:09:28,050
Then now, when you want to return this column as in the sketch form,

1431
01:09:28,060 --> 01:09:31,450
you want to store these in these codes here. 

1432
01:09:31,460 --> 01:09:32,650
Instead of actually original values, 

1433
01:09:33,360 --> 01:09:35,230
say the original values were 8 bits. 

1434
01:09:35,770 --> 01:09:37,840
I can store it now down into 2 bits. 

1435
01:09:40,530 --> 01:09:41,800
Now, when a query comes along,

1436
01:09:41,810 --> 01:09:43,240
like select star from table, 

1437
01:09:43,250 --> 01:09:44,920
where value is less than 90, 

1438
01:09:44,930 --> 01:09:52,230
I do a look up in my map and figure out what's the highest

1439
01:09:52,710 --> 01:09:56,510
the smallest value that will be covered by the thing i'm looking for. 

1440
01:09:56,890 --> 01:09:59,240
In this case here, 90 is greater than 60.

1441
01:09:59,600 --> 01:10:03,570
I got to go to the next 11. 30 2 is greater than 90. 

1442
01:10:03,580 --> 01:10:04,810
So I can stop here. 

1443
01:10:05,450 --> 01:10:11,880
I know I need to consider the values that are encoded by these code values here. 

1444
01:10:13,230 --> 01:10:17,220
You basically convert you use the mapping function to convert 90 into 01. 

1445
01:10:18,030 --> 01:10:22,770
And then I go do a look up in my ii know these are the offsets that

1446
01:10:22,780 --> 01:10:26,540
are matching for me in my sketch column. 

1447
01:10:27,540 --> 01:10:29,340
But because 132, 

1448
01:10:29,590 --> 01:10:31,450
132is greater than 90. 

1449
01:10:32,520 --> 01:10:35,540
We don't know whether there might be a value like 91, 

1450
01:10:35,550 --> 01:10:38,690
92 that doesn't satisfy our predicate, 

1451
01:10:39,000 --> 01:10:43,940
but is still within the scope that's managed by this code value here. 

1452
01:10:44,820 --> 01:10:46,370
For zero ones, 

1453
01:10:46,650 --> 01:10:50,860
we have to go back to the original data and see whether we have

1454
01:10:50,870 --> 01:10:52,010
a false positive or not. 

1455
01:10:52,020 --> 01:10:53,560
So in this case here, 

1456
01:10:53,570 --> 01:10:55,280
but 01.2, 

1457
01:10:56,220 --> 01:11:00,300
the code 01 points to 81~140. 81will satisfy our predicate. 

1458
01:11:00,670 --> 01:11:01,590
140does not. 

1459
01:11:03,540 --> 01:11:05,160
There is tactical call error here, 

1460
01:11:05,170 --> 01:11:09,960
because 132 means that like 01 is in the range of 60~132, 

1461
01:11:11,820 --> 01:11:13,810
01can't map to 140, 

1462
01:11:13,820 --> 01:11:16,410
140should map to 10, isn't it?

1463
01:11:16,460 --> 01:11:17,210
As a typo? 

1464
01:11:17,870 --> 01:11:18,400
That's my fault. 

1465
01:11:19,830 --> 01:11:20,390
Did I screw this up? 

1466
01:11:20,560 --> 01:11:20,790
Apply? 

1467
01:11:20,800 --> 01:11:21,630
I might have typed that wrong. 

1468
01:11:21,640 --> 01:11:22,310
That's my fault. 

1469
01:11:24,170 --> 01:11:26,790
So i'd say this is hundred 20, 

1470
01:11:27,080 --> 01:11:34,990
the type that's the type of in direction that we have to go out of. 

1471
01:11:35,900 --> 01:11:37,360
So it's, david,

1472
01:11:37,530 --> 01:11:39,600
doesn't this have the same problem as before? 

1473
01:11:40,210 --> 01:11:40,840
Where? 

1474
01:11:41,520 --> 01:11:45,340
Or in direction in terms of traversing the data structure itself

1475
01:11:45,350 --> 01:11:46,340
or actually doing this? 

1476
01:11:46,350 --> 01:11:47,500
Second, look up over here.

1477
01:11:47,510 --> 01:11:47,820
Second, 

1478
01:11:51,660 --> 01:11:54,170
you had one for me that you were going to now. 

1479
01:11:55,880 --> 01:11:57,740
Or do you put up all of them together and do it? 

1480
01:11:58,920 --> 01:11:59,120
Yeah. 

1481
01:11:59,130 --> 01:12:01,080
So his statement is, 

1482
01:12:01,960 --> 01:12:03,570
when I talked about the hierarchical encoding, 

1483
01:12:03,580 --> 01:12:07,260
I talked about how like i've got to go up and down the tree to go find

1484
01:12:07,270 --> 01:12:10,500
all the bits that was just trying to find the matches from the bits, 

1485
01:12:10,510 --> 01:12:12,540
like when they were set to one, 

1486
01:12:12,790 --> 01:12:16,780
then I go look out and get the actual two balls themselves in the table. 

1487
01:12:18,690 --> 01:12:19,780
In this case here, 

1488
01:12:20,070 --> 01:12:22,260
I can scan through this column quickly, 

1489
01:12:23,260 --> 01:12:26,600
find all the positions where it's either 0001. 

1490
01:12:27,830 --> 01:12:29,900
Then for the ones that are 01, 

1491
01:12:29,910 --> 01:12:32,020
it depends on what the query actually needs. 

1492
01:12:32,620 --> 01:12:35,060
I then do my batch look up to go get the actual look at the exam

1493
01:12:35,070 --> 01:12:36,380
and the actual the data itself. 

1494
01:12:37,520 --> 01:12:38,670
You would scan this thing first, 

1495
01:12:38,680 --> 01:12:39,750
get all the offsets. 

1496
01:12:41,590 --> 01:12:42,430
You could use ap community table. 

1497
01:12:42,440 --> 01:12:43,710
If you want it to, it doesn't matter.

1498
01:12:44,010 --> 01:12:48,470
Get all the offsets where this that where it satisfies my code, 

1499
01:12:48,480 --> 01:12:50,540
where it's less than or equal to 01. 

1500
01:12:50,550 --> 01:12:54,760
I've got to maybe get the data anyway, because in this case here,

1501
01:12:54,770 --> 01:12:55,680
it's like star. 

1502
01:12:56,210 --> 01:12:57,710
I've got to go get all the values anyway. 

1503
01:12:58,450 --> 01:13:01,290
Then I do an extra check there before I shut up to the next operator

1504
01:13:01,300 --> 01:13:01,930
and the query plan. 

1505
01:13:06,450 --> 01:13:08,320
Other than the type of the abbey picked out. 

1506
01:13:08,330 --> 01:13:08,760
Thank you. 

1507
01:13:09,850 --> 01:13:10,680
Any questions about this? 

1508
01:13:12,860 --> 01:13:17,530
Isn't the main optimization that if it isn't on that boundary, but the 01,

1509
01:13:18,000 --> 01:13:19,720
is it 00 you're going to have to do? 

1510
01:13:19,730 --> 01:13:25,590
The his statement is one of the organizations you would get is

1511
01:13:27,940 --> 01:13:30,390
I only have to check for 01 to avoid false positives. 

1512
01:13:30,690 --> 01:13:32,040
And in case it's 00, 

1513
01:13:32,390 --> 01:13:34,890
I don't have to go check it again, 

1514
01:13:34,900 --> 01:13:38,730
like depends on what the query is. 

1515
01:13:39,120 --> 01:13:41,090
If it's a count, then I go check these,

1516
01:13:41,350 --> 01:13:42,840
but I count it all together. 

1517
01:13:43,160 --> 01:13:46,630
If I need to do like a join where I do need, maybe the column,

1518
01:13:46,980 --> 01:13:47,910
I had to go fetch it anyway, 

1519
01:13:47,920 --> 01:13:51,250
but I don't have to do the comparison because I know it satisfies my predicate. 

1520
01:13:55,940 --> 01:13:56,520
All right, cool.

1521
01:13:57,170 --> 01:13:57,800
And on time, 

1522
01:14:01,100 --> 01:14:02,090
those amounts, as I said,

1523
01:14:02,100 --> 01:14:04,610
are the most widely used method to accelerate sequential scans. 

1524
01:14:05,220 --> 01:14:07,690
It's pretty much what everyone will give you. 

1525
01:14:07,700 --> 01:14:08,610
I think parquet actually, 

1526
01:14:08,970 --> 01:14:11,900
I know my work and this is what parquet the only thing parquet has. 

1527
01:14:13,360 --> 01:14:19,220
Bit map indexes are more common in the sort of the enterprise row stores honestly. 

1528
01:14:19,660 --> 01:14:20,980
Postcards lessons themselves enterprise, 

1529
01:14:20,990 --> 01:14:23,430
but I like the oracles and sql servers, 

1530
01:14:23,440 --> 01:14:24,510
things that cost a lot of money. 

1531
01:14:25,720 --> 01:14:25,990
Because again, 

1532
01:14:26,000 --> 01:14:31,260
they were trying to get the benefits of sort of columnar processing or data

1533
01:14:31,570 --> 01:14:34,760
without having to to have a whole columnist or engine, 

1534
01:14:34,770 --> 01:14:36,200
which they all eventually built anyway. 

1535
01:14:36,860 --> 01:14:37,330
As I said, 

1536
01:14:37,340 --> 01:14:40,130
we're completely ignoring multi dimensional indexes and vertical indexes. 

1537
01:14:41,520 --> 01:14:42,890
I don't think they teach a 26 anymore, 

1538
01:14:42,900 --> 01:14:45,010
but that whole class was about this kind of stuff. 

1539
01:14:45,290 --> 01:14:46,940
Kd trees are trees and so forth. 

1540
01:14:47,390 --> 01:14:48,720
We can ignore that geo spatial things. 

1541
01:14:48,730 --> 01:14:49,410
We ignore that. 

1542
01:14:49,690 --> 01:14:55,020
And then our verte index is just like a mapping from a word or an n graham

1543
01:14:55,030 --> 01:14:57,750
within a text field to the extra tuples that match. 

1544
01:15:01,300 --> 01:15:01,850
Next class. 

1545
01:15:01,860 --> 01:15:03,250
We'll talk about data compression. 

1546
01:15:03,700 --> 01:15:05,290
We'll spend most of the time talking about tuples. 

1547
01:15:05,690 --> 01:15:09,490
We'll also talk about how we actually compress these bit maps

1548
01:15:09,500 --> 01:15:11,530
as well beyond the things we talked about before. 

1549
01:15:14,060 --> 01:15:16,930
So quickly, I want to talk about project one, which is out,

1550
01:15:17,350 --> 01:15:19,790
thank you for win for updating everyone at piazza. 

1551
01:15:20,600 --> 01:15:23,180
So for the first project, 

1552
01:15:24,090 --> 01:15:28,450
you guys will be writing a foreign data wrapper for postcards to access, 

1553
01:15:29,960 --> 01:15:34,530
to access the columnar data stored in a file format that we invented. 

1554
01:15:34,980 --> 01:15:36,860
The reason why we didn't choose parquet or work

1555
01:15:36,870 --> 01:15:42,240
because there's existing libraries to access them, 

1556
01:15:42,250 --> 01:15:45,500
and they basically care of all the complexity for you. 

1557
01:15:46,510 --> 01:15:49,440
And it's way more complicated, also bring it in these other libraries.

1558
01:15:49,450 --> 01:15:53,230
So we just gave you something really simple and hear about that. 

1559
01:15:53,510 --> 01:15:55,900
A foreign data rapper is basically a way to have if you're

1560
01:15:55,910 --> 01:15:56,700
from your sequel light, 

1561
01:15:56,710 --> 01:15:57,700
it's got a virtual table. 

1562
01:15:58,040 --> 01:15:59,970
It's a way to basically sometimes called a connector. 

1563
01:16:00,340 --> 01:16:04,960
But it's basically it's a way to have lincoln a shared object to postgres

1564
01:16:05,650 --> 01:16:07,560
that allows you override

1565
01:16:08,980 --> 01:16:13,450
scan operators on a table to go to whatever that shared object wants to support, 

1566
01:16:13,460 --> 01:16:16,130
rather than always go into their internal base tables. 

1567
01:16:17,770 --> 01:16:20,000
There's foreign data wrappers to read csv files, 

1568
01:16:20,620 --> 01:16:21,960
foreign data wrapper to read. 

1569
01:16:23,120 --> 01:16:25,150
We connect to another database system, right?

1570
01:16:25,700 --> 01:16:27,080
Postcards is actually really extensible. 

1571
01:16:27,570 --> 01:16:28,360
It's very amazing. 

1572
01:16:28,370 --> 01:16:29,120
It's an amazing, 

1573
01:16:29,910 --> 01:16:31,430
it's amazing data system, but it's also very extensible,

1574
01:16:31,440 --> 01:16:32,230
which is pretty cool. 

1575
01:16:33,500 --> 01:16:37,700
So there's actually commercial products that build foreign data wrappers

1576
01:16:38,610 --> 01:16:39,160
for postcards, 

1577
01:16:39,170 --> 01:16:41,360
like time scale is makes heavy use of this. 

1578
01:16:42,230 --> 01:16:42,540
But again, 

1579
01:16:43,710 --> 01:16:45,780
from the sql perspective, it looks like a regular table,

1580
01:16:45,790 --> 01:16:47,140
even though it's going to go through your code. 

1581
01:16:48,630 --> 01:16:51,370
The goal of this is get you familiar with how to extend progress, 

1582
01:16:51,380 --> 01:16:52,890
because you could use that for project three, 

1583
01:16:53,670 --> 01:16:56,510
but also to actually implement the things that we're talking

1584
01:16:56,520 --> 01:16:59,960
about in class today in the last couple classes of how she had a scan

1585
01:16:59,970 --> 01:17:00,840
through kilometer data. 

1586
01:17:02,040 --> 01:17:04,820
The file format that when developed pretty basic, 

1587
01:17:04,830 --> 01:17:05,860
we're not doing any compression. 

1588
01:17:06,210 --> 01:17:08,640
It's just storing things in columnar data and columnar format. 

1589
01:17:10,220 --> 01:17:10,460
Sorry, 

1590
01:17:10,470 --> 01:17:13,090
the footer has jason metadata about where to find the offsets

1591
01:17:13,260 --> 01:17:14,410
for different types of data. 

1592
01:17:15,680 --> 01:17:16,830
I bumped the deadline up. 

1593
01:17:16,840 --> 01:17:17,190
I don't know. 

1594
01:17:17,200 --> 01:17:19,310
I think it was due the 16th or something. 

1595
01:17:19,320 --> 01:17:21,310
And now it's due to the 26th, which is a sunday.

1596
01:17:21,770 --> 01:17:24,460
And all the information is on the project website now. 

1597
01:17:25,120 --> 01:17:26,390
And then i'll post on piazza, 

1598
01:17:26,840 --> 01:17:27,950
a spreadsheet. 

1599
01:17:28,810 --> 01:17:34,490
So I I already know everyone's andrew idi i'm gonna fill out a form to get

1600
01:17:34,780 --> 01:17:36,490
aws credits. 

1601
01:17:36,880 --> 01:17:37,870
Then i'll send everyone an email. 

1602
01:17:37,880 --> 01:17:41,830
If you're enrolled in the class code for like $100 from amazon, 

1603
01:17:41,840 --> 01:17:43,230
you can use that for development. 

1604
01:17:43,240 --> 01:17:43,590
If you want. 

1605
01:17:44,730 --> 01:17:45,920
You use it for bit coin mining. 

1606
01:17:45,930 --> 01:17:46,550
If you want. 

1607
01:17:47,120 --> 01:17:48,510
Go ahead, I don't care.

1608
01:17:50,430 --> 01:17:53,990
So anyway, like we you can make it work on your laptop,

1609
01:17:54,000 --> 01:17:54,870
make it work on ec two. 

1610
01:17:54,880 --> 01:17:58,100
An end of the day has to compile and work on aws sorry, 

1611
01:17:58,110 --> 01:17:58,820
i'm on grade school. 

1612
01:17:58,830 --> 01:18:00,460
This is how you submit this. 

1613
01:18:02,720 --> 01:18:04,870
Again, we've already provided you a basic file format.

1614
01:18:06,750 --> 01:18:08,940
There's instructions on what it actually looks like. 

1615
01:18:09,390 --> 01:18:10,540
It's not super sophisticated. 

1616
01:18:10,550 --> 01:18:11,900
It's not doing any compression. 

1617
01:18:12,590 --> 01:18:16,780
It's literally just storing data in binary form and in common and common arrangement. 

1618
01:18:17,740 --> 01:18:21,130
So the first step you want to do is just write a parse over this file, 

1619
01:18:21,660 --> 01:18:24,150
just to scan image of columns and try to stitch two balls back together. 

1620
01:18:24,520 --> 01:18:26,510
When you process when you read the metadata, 

1621
01:18:26,520 --> 01:18:27,710
I understand what the offsets are. 

1622
01:18:27,940 --> 01:18:30,180
Know how to put tuples back together. 

1623
01:18:30,790 --> 01:18:36,200
Then you want to integrate this in postgres in using the foreign data wrapper, 

1624
01:18:36,210 --> 01:18:40,200
api I think when we provide them some basic skeleton code already, right?

1625
01:18:40,930 --> 01:18:41,950
There's some basic skeleton code, 

1626
01:18:41,960 --> 01:18:43,190
some functions you've got to fill out, 

1627
01:18:43,420 --> 01:18:46,090
and then instructions on the website how to actually compile and then link

1628
01:18:46,100 --> 01:18:47,250
it into postgres. 

1629
01:18:48,820 --> 01:18:53,020
And then the last step is then you want to actually support predicate evaluation. 

1630
01:18:53,430 --> 01:18:54,570
So you can have postgres, 

1631
01:18:54,580 --> 01:18:55,810
take the where clause. 

1632
01:18:56,170 --> 01:18:58,450
We're not trying to support all everything you can possibly do in a where clause, 

1633
01:18:58,930 --> 01:19:03,310
some basic comparison operators that'll get passed to you in what the post

1634
01:19:03,320 --> 01:19:03,790
got struck. 

1635
01:19:04,300 --> 01:19:07,360
And you want to then apply the predicate as you scan the data. 

1636
01:19:12,270 --> 01:19:13,180
You have to join us. 

1637
01:19:16,560 --> 01:19:17,150
It's hard. 

1638
01:19:17,810 --> 01:19:20,740
It was surprisingly difficult enough just to do the scan. 

1639
01:19:20,920 --> 01:19:21,460
As a foreign driver. 

1640
01:19:21,470 --> 01:19:25,450
I suppose I say process is extensible. 

1641
01:19:25,460 --> 01:19:25,810
That's great. 

1642
01:19:25,820 --> 01:19:26,970
I never said it's easy to extend. 

1643
01:19:26,980 --> 01:19:28,170
You can't extend it, right?

1644
01:19:33,080 --> 01:19:35,470
The way to get started, as I said, start with the this,

1645
01:19:35,710 --> 01:19:37,400
Writing the parcel code without any post graphs. 

1646
01:19:37,410 --> 01:19:40,230
You can just do that locally. 

1647
01:19:41,560 --> 01:19:42,600
You should do this in c plus, 

1648
01:19:42,610 --> 01:19:47,240
and we're giving you skeleton code that uses pgxs if you want to do rust, 

1649
01:19:47,760 --> 01:19:48,430
talk to chi. 

1650
01:19:49,190 --> 01:19:52,190
He's gonna tell you go away, but like you looked into it, right?

1651
01:19:52,200 --> 01:19:52,910
Or Wayne looked at it. 

1652
01:19:53,240 --> 01:19:54,590
We don't think you can do it. 

1653
01:19:55,510 --> 01:20:00,810
But you he can do it because because he's all right. 

1654
01:20:01,650 --> 01:20:02,520
Offering that out. 

1655
01:20:02,530 --> 01:20:02,880
All right? 

1656
01:20:02,890 --> 01:20:06,160
So just like the interclass, 

1657
01:20:08,060 --> 01:20:09,650
make sure you all your button locally, 

1658
01:20:12,010 --> 01:20:13,320
you write your own local tests, 

1659
01:20:13,690 --> 01:20:14,860
don't use gray scope to do this, 

1660
01:20:15,570 --> 01:20:16,440
because it's going to be slow. 

1661
01:20:18,480 --> 01:20:21,350
Don't change any of the files that we don't other than the ones that use

1662
01:20:22,240 --> 01:20:23,860
the on grade scope, 

1663
01:20:23,870 --> 01:20:27,060
because those will get wiped one compile and link it on grade scope. 

1664
01:20:27,580 --> 01:20:29,300
Make sure you fork our version of progress, 

1665
01:20:29,310 --> 01:20:32,220
because it already has the scaffolding code that win has set up. 

1666
01:20:32,590 --> 01:20:36,610
And it's all sort of set up for you to like compile from the command line

1667
01:20:36,620 --> 01:20:39,800
or vs code or whatever sea line he's using. 

1668
01:20:40,340 --> 01:20:40,600
That's in there. 

1669
01:20:40,610 --> 01:20:41,860
And then it'll be able to link it in. 

1670
01:20:41,870 --> 01:20:43,540
You don't have to figure that out yourself, which is not trivial.

1671
01:20:44,050 --> 01:20:47,020
And then please post questions on piazza or come to piazo office hours. 

1672
01:20:47,410 --> 01:20:47,770
Ok. 

1673
01:20:49,110 --> 01:20:51,060
There are example, 

1674
01:20:51,500 --> 01:20:55,240
farm data rappers that are in the write up that Wayne is linked to. 

1675
01:20:56,850 --> 01:20:58,650
There's the parquet one, 

1676
01:20:58,660 --> 01:20:59,810
and then it's the slightest one. 

1677
01:21:03,700 --> 01:21:06,170
Some of the scaffolding code will look very similar. 

1678
01:21:07,090 --> 01:21:08,320
It won't be entirely useful to you, 

1679
01:21:08,330 --> 01:21:11,630
but you can just look at it as a reference, but avoid copying wholesale.

1680
01:21:12,140 --> 01:21:13,660
Without attribution from existing cute, 

1681
01:21:13,670 --> 01:21:15,720
you may find and also, too,

1682
01:21:15,730 --> 01:21:18,480
because there's always random people on the internet that see the videos

1683
01:21:18,490 --> 01:21:19,640
and try to implement themselves, 

1684
01:21:20,010 --> 01:21:20,870
avoid their stuff. 

1685
01:21:21,930 --> 01:21:22,770
It's usually crap. 

1686
01:21:23,050 --> 01:21:23,810
I'm not going to lie, 

1687
01:21:25,740 --> 01:21:26,090
right? 

1688
01:21:26,100 --> 01:21:26,890
Any questions? 

1689
01:21:27,700 --> 01:21:29,890
Ha ha, that's my favorite.

1690
01:21:29,900 --> 01:21:30,850
All that are. 

1691
01:21:33,200 --> 01:21:33,670
What is it? 

1692
01:21:34,560 --> 01:21:36,070
It's the SP cricket, 

1693
01:21:36,080 --> 01:21:41,150
idesi make a mess unless I could do it like a deal, 

1694
01:21:41,290 --> 01:21:44,720
ice cube with the g to the e to the t it comes. 

1695
01:21:44,730 --> 01:21:46,000
Do I play the game? 

1696
01:21:46,010 --> 01:21:46,920
Where's no roof? 

1697
01:21:47,250 --> 01:21:48,080
He's on the custody. 

1698
01:21:48,090 --> 01:21:49,600
I'm a focus on drink fruit, 

1699
01:21:49,610 --> 01:21:51,840
put the bus a cap on the ice road, 

1700
01:21:51,850 --> 01:21:55,440
bush week on the go with a blow to the eyes come. 

1701
01:21:56,130 --> 01:21:56,560
Indeed, 

1702
01:21:56,570 --> 01:22:01,840
that's me rolling with 5th what's up park and south central g and thank us

1703
01:22:01,850 --> 01:22:02,920
when I party. 

1704
01:22:03,260 --> 01:22:05,890
By the 12 pack case on the 46 pack, 

1705
01:22:05,900 --> 01:22:07,890
48 gets the real Paris. 

1706
01:22:08,170 --> 01:22:10,920
I drink fruit, but you are drinking Bob 12,

1707
01:22:10,930 --> 01:22:13,040
but they say bill makes you fat, 

1708
01:22:13,050 --> 01:22:14,520
but saying eyes is straight, 

1709
01:22:14,530 --> 01:22:16,120
so it really don't matter. 
